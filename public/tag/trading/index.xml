<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Trading | KidQuant</title>
    <link>http://localhost:4321/tag/trading/</link>
      <atom:link href="http://localhost:4321/tag/trading/index.xml" rel="self" type="application/rss+xml" />
    <description>Trading</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 02 Apr 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:4321/media/icon_huf195f9a67bfe19c82d5f0ba6a67922fa_2030_512x512_fill_lanczos_center_3.png</url>
      <title>Trading</title>
      <link>http://localhost:4321/tag/trading/</link>
    </image>
    
    <item>
      <title>Is DJT a Meme Stock?</title>
      <link>http://localhost:4321/post/2024-04-02-djt-meme-stock/</link>
      <pubDate>Tue, 02 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2024-04-02-djt-meme-stock/</guid>
      <description>&lt;p&gt;Donald Trump&amp;rsquo;s knock-off Twitter has officially become public for under a week, and the entire country didn&amp;rsquo;t take long to create an investment thesis about it. Just in case you fell asleep, Trump got himself banned from Twitter for&amp;hellip; reasons [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;To combat the bias and censorship that was prevalent at the time, Trump decided to launch his social media network called Truth Social, which was created by T Media Tech LLC in 2021, owned by Trump Media &amp;amp; Technology Group Corp. Since then, the company was acquired via SPAC (Special Purpose Acquisition Vehicle) by Digital World Acquisition Corp.&lt;/p&gt;
&lt;p&gt;You can find the stock on the NASDAQ exchange under the ticker DTJ, which I&amp;rsquo;m sure is just a coincidence. It&amp;rsquo;s currently trading at $~49$ per share, down from it&amp;rsquo;s peak of $~66$ per share. Trump is currently the majority shareholder, owning $~57%$ of the available shares.&lt;/p&gt;
&lt;p&gt;So it begs the question: &lt;strong&gt;Is DJT, a company owned by the former President of the United States, a meme stock?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Probably.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/Meme-DJT/stonks.png&#34; alt=&#34;Stonks&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;At least according to the analysis I conducted three years ago is concerned. Recall that I&amp;rsquo;ve created an algorithm that tracks the most frequently mentioned assets on the infamous subreddit: Wall Street Bets [&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;]. Three years ago, owning shares of AMC, GME, BB, and some crypto was cool. Today, owning shares of NVDA, AAPL, SPY, RDDT (obviously), and DJT is cool, so it&amp;rsquo;s a mixed bag for the overall &amp;ldquo;memeness.&amp;rdquo; Some of the assets that generate a lot of buzz on the subreddit are definitely at risk. On the other hand, some of the assets are more established, less volatile, positive cash flow generating assets.&lt;/p&gt;
&lt;p&gt;So what does this all mean? What is going on? Did the &amp;ldquo;Retards&amp;rdquo; over at WSB learn from their diamond hands fiasco and decide to adopt a more mature investment philosophy? Or they chose to do what all retail traders do: talk about exciting companies.&lt;/p&gt;
&lt;p&gt;One thing is for sure: We don&amp;rsquo;t need WSB to figure out whether DJT is a sound investment because DJT—like everything related to Donald John Trump—stems, and will forever stem, from whether or not you hate Donald Trump. It&amp;rsquo;s impossible to have a neutral opinion about him, even things about him that are objectively true. (such as, he lost the election; handled it poorly AND people were consistently trying to sabotage his administration)&lt;/p&gt;
&lt;p&gt;If you love Trump, DJT is the greatest thing ever, it&amp;rsquo;s going to the moon, and everyone loves the stock.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/Meme-DJT/diamond.png&#34; alt=&#34;Diamond Handsl&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;If you hate Trump, DJT is a clear scam; a pump and dump (it&amp;rsquo;s not a pump and dump) and it&amp;rsquo;s going to zero, just like his other businesses.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/Meme-DJT/zero.png&#34; alt=&#34;zero&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Like everything, the truth is probably close to the middle but definitely much closer to zero than the moon. Right now, DJT is receiving a wave of bad press, mostly related to lackluster earnings performance and the uncertainty of the firm&amp;rsquo;s future viability. As expected, the stock took a hit based on that news.&lt;/p&gt;
&lt;p&gt;Most outsiders typically don&amp;rsquo;t understand that every asset class has a behavioral aspect (the stock skyrocketed upon news of the successful merger; it tanked upon news of poor performance). If what we are experiencing with DJT is indeed behavioral, then this period of volatility will go away. However, as it stands right now, the market expects the stock to go lower simply because it generates huge losses from its operating activities, and there is no evidence of a viable user base on the platform.&lt;/p&gt;
&lt;p&gt;Again, I&amp;rsquo;m not saying that the stock is worthless, BUT it&amp;rsquo;s also not worth $7 billion dollars.&lt;/p&gt;
&lt;h3 id=&#34;will-i-analyze-djt-similiar-to-other-meme-stocks&#34;&gt;Will I Analyze DJT Similiar To Other Meme Stocks?&lt;/h3&gt;
&lt;p&gt;Other meme stocks, such as GME and AMC, were different from DJT because these companies are more established. GME and AMC have been around for over two decades, while DJT has just arrived on the scene. Also, DJT is a SPAC, so the asset inherently has a negative connotation attached to it. We would need more data about the company to determine whether it can deliver excess returns to investors. So, I won&amp;rsquo;t be conducting any research relating to DJT, and even if I wanted to, it would primarily be conducted utilizing quantitative methods.&lt;/p&gt;
&lt;p&gt;Since I&amp;rsquo;ve already stated that it&amp;rsquo;s fair value is closer to zero than to the moon, does this imply that I know it&amp;rsquo;s fair value? Not exactly. Anyone who has visited my page can probably guess that I subscribe to &lt;strong&gt;Efficient Market Hypothesis&lt;/strong&gt;, which is an investment theory that states all relevant information (maybe past and present) is reflected in the current share price. Also, since stock prices fluctuate randomly, their predictability is no better than flipping a coin. As such, it&amp;rsquo;s almost impossible to consistently outperform the markets in the long run.&lt;/p&gt;
&lt;p&gt;So, just because DJT was up for three consecutive days last week doesn&amp;rsquo;t mean it&amp;rsquo;ll be up on the fourth day; it also doesn&amp;rsquo;t mean it will be down on the fourth day, either. Even though there is a solid upward bias, it&amp;rsquo;s essential to understand that asset prices are random, and it&amp;rsquo;s necessary to build a framework that can capture said randomness (quant finance, for starters). Don&amp;rsquo;t bet on it going to the moon, but don&amp;rsquo;t bet on it going to zero for the foreseeable future.&lt;/p&gt;
&lt;p&gt;Regardless, the only reason to pick a dog in this fight is to vindicate your confirmation biases. If DJT succeeds, it will be another example of Trump&amp;rsquo;s business prowess and a triumphant story of beating the odds against him, thus proving he deserves another shot at the presidency. If DJT fails, it will, once again, be another one of his failed business ventures, proving that he doesn&amp;rsquo;t know what he&amp;rsquo;s doing and he doesn&amp;rsquo;t care about delivering value to anyone; he just cares about milking his supporters.&lt;/p&gt;
&lt;p&gt;As always, pick your own adventure.&lt;/p&gt;
&lt;p&gt;[&lt;a name=&#34;1&#34;&gt;1&lt;/a&gt;] &lt;a href=&#34;https://www.npr.org/2021/01/08/954760928/twitter-bans-president-trump-citing-risk-of-further-incitement-of-violence#:~:text=Twitter%20Permanently%20Suspends%20Trump%2C%20Citing,Further%20Incitement%20Of%20Violence%27%20%3A%20NPR&amp;amp;text=Close%20Navigation%20Menu-,Twitter%20Permanently%20Suspends%20Trump%2C%20Citing%20%27Risk%20Of%20Further%20Incitement%20Of,that%20violated%20the%20company%27s%20rules.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NPR| Twitter Permanently Suspends Trump, Citing &amp;lsquo;Risk Of Further Incitement of Violence&amp;rsquo;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a name=&#34;2&#34;&gt;2&lt;/a&gt;] &lt;a href=&#34;https://kidquant.com/post/2021-05-24-wsb-favorite-stock/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KidQuant | r/WallStreetBets Most Talked About Stonks&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Revisitng Delta</title>
      <link>http://localhost:4321/post/2023-03-28-revisiting-delta/</link>
      <pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2023-03-28-revisiting-delta/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve recently been tasked with working more with more fixed-income derivatives, so I&amp;rsquo;m writing this more as a refresher for myself, but I&amp;rsquo;ve always guaged how well I know something based on how well I&amp;rsquo;m able to teach others.&lt;/p&gt;
&lt;p&gt;Recall that the Black-Scholes price of a European-style call option is&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{aligned}
C &amp;amp; = S\Phi(d_1)-Ke^{-rT}\Phi(d_2),\
d_1 &amp;amp; = \frac{1}{\sigma\sqrt{T}}[log(S/K)+[r+(1/2)\sigma^2]T],\
d_2 &amp;amp; = d_1-\sigma\sqrt{T},
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\Phi(x)$ is the cumulative distribution function (CDF) of the standard normal distribution:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{aligned}
\Phi(x):=\frac{1}{\sqrt{\pi}}\int^{x}_{-\infty}e^{-z^2/2}dz
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Since $\Phi(x)$ is the CDF of the standard normal distribution, its derivation $\Phi^\prime(x)$ is the probability density function(PDF) of the standard normal distribution. I&amp;rsquo;ll denote this with $\phi(x)$ or&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\phi(x):=\Phi^{\prime}(x).
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Delta ($\Delta$)&lt;/em&gt; captures the sensitivity, or more precisely the instanteneous rate of change, of the option price to the spot price. The Black-Scholes delta for a call option is&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\Delta(C)=\frac{\partial C}{\partial S}=\Phi(d_1)
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\Phi(x)$ and $d_1$ are defined in Equations 1 and 2.&lt;/p&gt;
&lt;p&gt;To understand delta, let&amp;rsquo;s start with an example. Suppose we have a stock with a price of $S=100$, a call option with a price of $C=10$, and a delta between the two of 0.7. (The prices do not matter here but will be in later examples.) Now imagine that we sell calls for $3000$ shares of this stock and want to hedge our resulting short position. We can use delta to compute the appropriate size of our hedge. We should put $0.7\times 3000=2100$ shares of the underlying stock. Why? This is just a direct application of Equation 4.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s replace the differentials (e.g., $\partial C$) with small moves in the asset (e.g., $dC$). This is a fine thing to do because Equation 8 says that $\Delta$ is the linear appropriate of how the Black-Scholes prices of our calls will change. We can write this as&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{aligned}
dC=\Delta\times dS.
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Now let $w$ denote the number of shares of the stock assoicated with the calls we sold (here $w=3000$). Then by equation 5, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
dC\times w = \Delta \times dS \times w
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Our proposed hedge above is $\Delta \times w = 2100$, and this works because if the stock price changes by $dS=1$, then the call moves by $dC=0.7$, and we will make $2,100$ on our hedge and lose $0.7\times 3000=21000$ on our calls. In practice, this calculation is slightly more tedious because options are typically sold with a contract multiplier indicating the number of shares per contract, but this is just a matter of bookkeeping.&lt;/p&gt;
&lt;p&gt;Thus, while the options we have sold have a delta of $0.7$, our actual portfolio position has a zero delta. We say that our portfolio is &lt;em&gt;delta neutral&lt;/em&gt;, which means that if the spot changes a little, the value of our options position does not change significantly.&lt;/p&gt;
&lt;h3 id=&#34;dollar-delta&#34;&gt;Dollar Delta&lt;/h3&gt;
&lt;p&gt;In the previous example, the raw values of the stock and call did not matter, and we had no sense of our total notional exposure. Thus, in practice, &lt;em&gt;mathematical delta&lt;/em&gt; (Equation 4) is often converted to &lt;em&gt;dollar delta&lt;/em&gt;, sometimes called notional delta, because this quantity tells us how much notional exposure we have to the underlying. A dollar delta is just delta times the spot price or&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
$\Delta=\Delta S
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;For example, our notional exposure in the previous example was the number of shares times dollar delta or&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{aligned}
$\Delta\times w &amp;amp; = \Delta\times S\times w  \
&amp;amp; = 0.7\times$ 100\times 3000 \
&amp;amp; = $ 210,000. \
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;To hedge, we should buy $210,000$ of stock, which is obviously the same as buying $2,100$ shares of stock at $S=100$.&lt;/p&gt;
&lt;p&gt;One benefit of considering a notional amount is that we can multiply the dollar delta by a percent change in the stock. In contrast, since delta represents a dollar change in stock move, per dollar change in option move, we cannot directly multiply mathematical delta by a relative move such as a return or percent return. To see this, consider the returns in our running example. Let $S_1$ denote the stock price after moving $dS$. Then, the return on $S$ is&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
R_S=\frac{s_1-S}{S}=\frac{101-100}{100}=0.01
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;while the return on C to a new price $C_1$ is&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
R_C=\frac{C_1-C}{C}=\frac{\Delta dS}{C}=\frac{0.7}{10}-0.07
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;And clearly&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\Delta R_S=0.7\times 0.01=0.007\neq 0.07=R_C
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;So, multiplying the mathematical delta by a relative move is nonsensical. Given Equation 8, we must operate on it in a way that makes sense.&lt;/p&gt;
&lt;p&gt;However, we can multiply dollar delta by a relative move since&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{aligned}
$\Delta\times R_S &amp;amp;= \Delta\times S\times\bigg(\frac{S_1-S}{S}\bigg)\
&amp;amp;= \Delta\times dS \
&amp;amp;= dC.
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;The dollar delta is nice because it tells us our notional exposure to the underlying and because we can multiply it by a relative move in the underlying to get a dollar change in the option.&lt;/p&gt;
&lt;h3 id=&#34;portfolio-delta&#34;&gt;Portfolio Delta&lt;/h3&gt;
&lt;p&gt;Imagine we had a portfolio of $n$ options. We can represent the value of our portfolio, $V_p$, as a weighted sum of these $n$ assets,&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
V_p=\sum_{i=1}^nw_i V_i,
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where the portfolio weights represent the quantity of each option, and where $V_i$ is the value of the $i$-th call or put. By the linearity of differentiation, we can efficiently compute the delta of this portfolio.&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{aligned}
\Delta(V_p) &amp;amp;= \frac{\partial V_p}{\partial S} \
&amp;amp;= \sum_{i=1}^{n}w_i\frac{\partial V_i}{\partial S} \
&amp;amp;= \sum_{i=1}^{n}w_i\Delta(V_i)
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Thus, the delta of a portfolio can be computed by adding up the deltas of the individual options.&lt;/p&gt;
&lt;p&gt;For example, recall our short position in call options from the example above. Now imagine that we additionally have a long position in call options worth 2000 shares of the same underlying, where the delta of each option is $0.6$. The delta of our portfolio in shares of stock is&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{aligned}
-3000(0.7)+2000(0.6) &amp;amp;= -2100+1200\
&amp;amp;=-900.
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Thus, our portfolio has a dollar delta of—$90,000$, which we can make delta-neutral by buying $90,000$ of stock. This makes sense. Before, we had a notional hedge of $210,000$. Now, given our long position in calls that behave roughly like half a stock, we have a notional hedge that is roughly half that.&lt;/p&gt;
&lt;h3 id=&#34;delta-hedging&#34;&gt;Delta Hedging&lt;/h3&gt;
&lt;p&gt;In our leading example, we hedged our delta from our option position, a short position in calls. Now that we understand the dollar delta, let&amp;rsquo;s think about a more general way to formulate the problem. Imagine a portfolio with a $\delta_{\Pi}$ delta. And imagine that another asset has a delta of $\Delta_i$. Then, to be delta neutral, we want to buy $w_i$ of this asset such that&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
w_i+\Delta_{\Pi}=0
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Solving for $w_i$, we get&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
w_i=-\frac{\Delta_{\Pi}}{\Delta_i}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;But what asset should we use here? We could find an option with a delta of $\Delta_i$, but a more straightforward thing would be to buy the underlying asset, which has a delta of one $(\Delta_i=1)$. So, our hedge in shares or dollars should be proportional to $w_i=\Delta_{\Pi}$, which is what we see in Equation 10. We can easily consider this regarding shares or dollars by multiplying both sides of Equation 21 as desired.&lt;/p&gt;
&lt;h3 id=&#34;functional-form&#34;&gt;Functional Form&lt;/h3&gt;
&lt;p&gt;Now that we understand how delta is used let&amp;rsquo;s dig into the precise definition of the Black-Scholes delta (Equation 8). This is an interesting result, but what it means may take time to be noticeable. First, note that since $\Phi(x)$ is the CDF of symmetric distribution, we can immediately guess that the delta&amp;rsquo;s shape is roughly a sigmoid function, which it is. This also makes sense if we visualize the slope of the tangent line changing as we move along the Black-Scholes price curve.&lt;/p&gt;
&lt;p&gt;The delta for a call must initially be nearly zero when $S \ll K$, increase as $S$ approaches and passes $K$, and finally flatten off to almost one when $S\gg K$.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/Revisitng-Delta/Delta_Put.png&#34; alt=&#34;BS as a function of spot&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Notice that an at-the-money (ATM) option has a delta of slightly greater than 0.5. This is because while $\Phi(x)$ is symmetric with respect to its input $d_1$, the variable $d_1$ is a logarithmic function of $S$. Confusingly, some people will claim that the delta of an ATM option is 0.5. For example, Investopedia says&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An ATM option as a delta of $\pm 0.50$, positive if it is a call, negative for a put.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But this is false. Just consider the full definition of delta for a call when $S=K$.&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\Delta(C)=\Phi\bigg(\frac{[r+(1/2)\sigma^2]\sqrt{T}}{\sigma}\bigg).
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;since $\phi(x)$ is the cdf of the standard normal distribution, we can see that $\delta(c)=0.5$ only when $t=0$, assuming interest rates and volatility are nonzero. However, it only makes sense to talk about the delta of an atm option at expiry. We&amp;rsquo;ll discuss this when we discuss delta sensitivities to other parameters.&lt;/p&gt;
&lt;p&gt;In most realistic scenarios (positive interest rates, nonzero volatility, and nonzero time expiry), the term $d_1$ is greater than zero when $S=K$, and thus the Black-Scholes delta of an ATM option is slightly greater than $0.5$.&lt;/p&gt;
&lt;p&gt;A deep-in-the-money (ITM) option has a delta close to one, meaning it behaves like the spot. A deep out-of-the-money (OTM) option has a delta close to zero, meaning it is insensitive to the underlying price changes. Many of these points should hold for any option pricing model. However, it does not make sense to have an option pricing model with zero delta for an ITM option. So, the coherence of Black-Scholes Greeks with reality justifies this model.&lt;/p&gt;
&lt;p&gt;It makes sense that the delta for a call option is in the $[0,1]$ since $\Delta$ is the slope of the relationship between the spot and option. Using put-call parity, we can immediately derive the delta for a put:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{aligned}
\Delta(P) &amp;amp;= \frac{\partial}{\partial S}P \
&amp;amp;= \frac{\partial}{\partial S}(C+Ke^{-rT}-S) \
&amp;amp;= \Delta(C)-1.
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Since $\Delta(C)$ ranges in $[0,1]$, clearly $\Delta(P)$ is in the range [-1,0]. And this makes sense, given the Black-Scholes price curve for a put (figure 2). So the delta for a call and put have the same shape for all input parameters but are shifted by one.&lt;/p&gt;
&lt;h3 id=&#34;moneyness&#34;&gt;Moneyness&lt;/h3&gt;
&lt;p&gt;At this point, we have enough understanding of delta to investigate a common interpretation of it: that it captures an option&amp;rsquo;s &lt;em&gt;moneyness&lt;/em&gt; and how likely it is that the option ends ITM.&lt;/p&gt;
&lt;p&gt;To understand this, we need to understand $\Phi(d_1)$ and $\Phi(d_2)$ from the Black-Scholes equation. $\Phi(d_2)$ has a clean interpretation. It is the probability that the option ends ITM in a risk-neutral world. If this is unclear, interpret $\Phi(d_2)$ by mapping it onto the appropriate term in the binomial options-pricing formula, a discrete-time analog of Black-Scholes.&lt;/p&gt;
&lt;p&gt;But since the moneyness interpretation of $\Phi(d_2)$ is accurate, it is only true of $\Phi(d_1)$ to the extent that $d_1\approx d_2$. How are these terms different? They differ by the volatility of the spot:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
d_2=d_1-\sigma\sqrt{T}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;This reasoning suggests that the moneyness interpretation of delta is approximately accurate when volatility or time to expiry is low but is less accurate when volatility or time to expiry is high. This is precisely the relationship we see when we compare $\Phi(d_1)$ and $\Phi(d_2)$ for varying volatilities.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/Revisitng-Delta/Delta_Impact_IV.png&#34; alt=&#34;Delta Volatility Sensitivity&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;delta-sensitivity&#34;&gt;Delta Sensitivity&lt;/h3&gt;
&lt;p&gt;As we have seen already, the precise shape of the delta changes based on the other Black-Scholes inputs. Just as we can understand Black-Scholes by fixing all parameters but one, we can understand a single Greek, such as delta, by fixing all other parameters.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s look at how delta changes while holding all other parameters fixed as the time to expiry $T$ decreases. I like plotting this with $T$ decreasing since the graph can be read left to right as physical time progresses. We can see that all options, regardless of money, have more similar deltas when the expiration date is far enough out. Then, as expiration approaches, ITM, ATM, and OTM options cover the delta of $1$, $0.5$, and $0$, respectively, for calls and $0$, $-0.5$, and $-1$, respectively, for puts.&lt;/p&gt;
&lt;p&gt;Another way to see this is to visualize how the Black-Scholes price curve (for calls) changes as a function of time. In the left subplot of Figure 2, I plotted the Black-Scholes price curve for a call as the time to expiry $T$ decreases to zero. As we can see, when $T$ is large, the option&amp;rsquo;s delta is roughly 0.5 over an extensive range of stock values.&lt;/p&gt;
&lt;h3 id=&#34;changing-delta&#34;&gt;Changing Delta&lt;/h3&gt;
&lt;p&gt;Finally, it is essential to remember that delta is a derivative and is, therefore, the slope of the tangent line to the Black-Scholes price curve at a particular spot price! When the spot changes in value from $S$ to $S_1$, our original estimate of the delta is no longer correct. It should be a new value $\Delta_1$, representing the slope of the line tangent to the Black-Scholes price curve at $S_1$.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/Revisitng-Delta/call_delta.jpeg&#34; alt=&#34;cal wrt &#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Hedging an options position by trading the underlying requires continuously rebalancing the hedged position due to this continuously accumulating &lt;em&gt;hedging error&lt;/em&gt;. One way to visualize this ever-changing error is by visualizing the gap between the linear equation $\Delta S_1+b$ and $\Delta_1 S_1+b_1$, where $b$ and $b_1$ and the $y$-intercepts their respective delta. What would increase this gap between the two points? More curvature to the Black-Scholes price curve.&lt;/p&gt;
&lt;p&gt;This, in order to account for this hedging error, we could add a term that accounts for the curvature of the Black-Schole&amp;rsquo;s model a particular point. To do this, we use a second-order Greek called gamma, which we will get into in another post.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Daniel Sevcovic, Beata Stehlikova, Karol Mikula (2011). &amp;ldquo;Analytical and numerical methods for pricing financial derivatives&amp;rdquo;. Nova Science Pub Inc; UK.&lt;/p&gt;
&lt;p&gt;[2] Wilmott Paul (1994), &amp;ldquo;Option pricing - Mathematical models and computation&amp;rdquo;. Oxford Financial Press.&lt;/p&gt;
&lt;p&gt;[3] Steven Shreve (2005), &amp;ldquo;Stochastic calculus for finance&amp;rdquo;. Spring Finance&lt;/p&gt;
&lt;p&gt;[4] Paul Wilmott (2006), &lt;em&gt;Paul Wilmott on quantitiative finance&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;[5] Broadie, Glasserman, Kou. &lt;em&gt;Connecting Discrete and Continuous Path-Dependent Options&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>r/WallStreetBets Most Talked About Stonks</title>
      <link>http://localhost:4321/post/2021-05-24-wsb-favorite-stock/</link>
      <pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2021-05-24-wsb-favorite-stock/</guid>
      <description>&lt;p&gt;One of the most bizarre David vs. Goliath scenarios in modern finance, by far, belongs to the retail investors from Reddit&amp;rsquo;s r/WallStreetBets versus Hedge Fund space. The story became such a fad that it garnered the attention of Treasury Secretary Janet Yellen and the SEC. With the democratization of finance, everyone can invest in markets once thought unobtainable to the common man.&lt;/p&gt;
&lt;h2 id=&#34;rwallstreetbets-vs-wall-street-hedge-funds&#34;&gt;r/WallStreetBets vs. Wall Street Hedge Funds&lt;/h2&gt;
&lt;p&gt;The mania revolves around the most shorted stocks, shorted by hedge funds that hoped to make a killing when those stocks collapse. For anyone unfamiliar with how short selling works, the art of short selling involves a hypothesis that an asset price will fall within a given time frame. The process consists of an investor borrowing a stock, selling the stock, and then buying the stock back to return to the lender (hopefully at a lower price than what the investor initially purchased).&lt;/p&gt;
&lt;p&gt;Where do hedge funds fit into all of this? Well, a bunch of hedge funds decided to get into the business of shorting equities with the highest short-interest. (The number of shares that have been sold short but have not yet been covered or closed out) At one point, the short interest of GameStop shares was over 140% of the float. If institutional investors wanted to close out their positions, they would need to buy those shares. But who is going to sell them those shares?&lt;/p&gt;
&lt;p&gt;The folks over at r/WallStreetBets have discovered that stocks with small float are the easiest to manipulate if enough people got together. They also figured out that stocks that were massively shorted and didn&amp;rsquo;t have many sellers left could be driven up to the point where investors would panic-buy to cover their short position. That panic buying would trigger a massive surge in GME&amp;rsquo;s price, which could wipe out those hated Hedge Funds.&lt;/p&gt;
&lt;p&gt;Since then, stocks such as GME, along with plenty of others, have been dubbed what we in the business call &amp;ldquo;Meme Stocks.&amp;rdquo; A meme stock is basically the equity belonging to any publicly traded company where the stock has increased in value, not because of the company&amp;rsquo;s performance fundamentals, but purely based on the attention the stock is receiving on social media (namely r/WallStreetBets). I&amp;rsquo;m not entirely sure which stocks are considered &amp;ldquo;memes&amp;rdquo; (there is actually an index which tracks these types of equities), but I know that this list would comprise of companies such as AMC Entertainment Holdings (AMC), Bed Bath and Beyond (BBBY), and Blackberry (BB) [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Now that the mania has passed, GME sits at around $180 per share. I once remember a single day of trading, GME opened at 191; surged to a high of 255; crashed to a low of 173, and still managed to close at 188 per share. Totally normal behavior for a stock. Since then, the price has stabilized at around the 150-180 price range.&lt;/p&gt;
&lt;p&gt;What does this mean for GME? Does this imply that r/WallStreetBets have moved on to the next oversold stock?&lt;/p&gt;
&lt;h2 id=&#34;getting-the-information-from-reddit&#34;&gt;Getting the Information from Reddit&lt;/h2&gt;
&lt;p&gt;I began by navigating to r/WallStreetBets. My original plan was to build this similiar to the news media webscraper algorithm for a data science project that I&amp;rsquo;ve done [&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;]. One of the issues with using that algorithm is being able to infinitely scrolling reddit and scraping the new information.&lt;/p&gt;
&lt;p&gt;Rather than using pagination to reveal additional results, Reddits makes an async call to fetch additional posts as you scroll down the page. This means that when we make our requests, we&amp;rsquo;ll be able to scrape data from the ~20 or so posts but won&amp;rsquo;t access additional data by requesting the next page.&lt;/p&gt;
&lt;p&gt;However, there is a workaround that I&amp;rsquo;ve found to access the data I need: &lt;strong&gt;PRAW&lt;/strong&gt;. PRAW stands for Python Reddit API Wrapper. PRAW is an api that allows you to easily read and write data to the website. To use it, you need to create the app credientials via Reddit and retrieve the &lt;code&gt;client_id&lt;/code&gt; and &lt;code&gt;client_secret&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Also, we&amp;rsquo;re going to need a list of all publicly traded US stocks and their ticker symbols, so the script will know which strings (or text) to match. The NASDAQ website allows you to download a list from the major US exchanges, such as the New York Stock Exchange, American Stock Exchange (I didn&amp;rsquo;t realize this was still a thing), and of course, NASDAQ [&lt;a href=&#34;#4&#34;&gt;4&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;The NASDAQ screener covers micro and nano-cap equities (the stock of companies with less than 300 million dollars in market capitalization); it covers the equities of companies of different regions; it even covers American Depository Receipts (or ADRs for short).&lt;/p&gt;
&lt;h2 id=&#34;scraping-reddit-via-praw&#34;&gt;Scraping Reddit via PRAW&lt;/h2&gt;
&lt;p&gt;Getting the information from via involves 3 easy steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting content from r/WallStreetBets&lt;/li&gt;
&lt;li&gt;Analyzing word frequencies&lt;/li&gt;
&lt;li&gt;Inner joining word frequencies with stock tickers&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;praw&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;requests&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;reddit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;praw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Reddit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;client_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;YOUR CLIENT ID&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;client_secret&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;YOUR CLIENT SECRET&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;user_agent&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;PRAW is actually easy to work with. PRAW stands for Python Reddit API Wrapper, which allows you to easily read and write data to the website. You start out by configuring an instance of Reddit, which requires the &lt;code&gt;client_id&lt;/code&gt; and &lt;code&gt;client_secret&lt;/code&gt; we&amp;rsquo;ll you&amp;rsquo;ll set up your authorized applications [&lt;a href=&#34;#3&#34;&gt;3&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;The API also requires something called a &amp;ldquo;User Agent.&amp;rdquo; A user agent is a line of text that helps servers&amp;rsquo; identify who is requesting information from the website hosted on their server. Any popular user agent will do; however, you can find out your user agent by simply asking Google. The script shows precisely what yours should look like.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;post&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reddit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subreddit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;wallstreetbets&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;limit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;title&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;post&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;text&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;post&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;selftext&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The dataframe will end up looking like the following code block. In the first column, we have the title of the post; in the second, we have the text that encompasses the post.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/WSB-Favorite-Stocks/01.PNG&#34; alt=&#34;Dataframe&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The query only returned 388 rows of information, although we requested at most 1,000 post. This is because reddit has removed queries based on time and only allows reddit users to browse based on the most recent post. 388 post, while limiting, is also more than enough to extract the information that we need.&lt;/p&gt;
&lt;p&gt;Now we need to analyze the data, which is the most complex part of the script. We&amp;rsquo;ll need to use some regular expressions, which will allow python to check a specific sequence of characters to match or find a set of strings, in our case, tickers or the company names. Python has a native package for this, know as &lt;code&gt;re&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We will loop through our data frame and utilize a key/value pair while iterating in this process. The key will be the words we are looking for in the post, while the value will be the frequency of the number of times certain words (in this case, tickers or companies) appear in each post. While we are doing this, we will also disregard the use of commonly-used terms, or filler words, in each of the posts. For those familiar with the Natural Language Processing techniques we used for analyzing media bias, the method is somewhat similar. [&lt;a href=&#34;#5&#34;&gt;5&lt;/a&gt;]&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;regex&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;compile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;[^a-zA-Z ]&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;word_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iterrows&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# titles&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;title_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# content&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;content_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# combine&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content_words&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;GO&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;ARE&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;ON&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;IT&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;ALL&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;NEXT&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;PUMP&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;AT&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;NOW&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;FOR&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;TD&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;CEO&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;AM&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;K&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;BIG&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;BY&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;LOVE&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;CAN&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;BE&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;SO&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;OUT&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;STAY&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;OR&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;NEW&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;RH&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;EDIT&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;ONE&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;ANY&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;pass&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;word_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;word_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;word_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word_dict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()))&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rename&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Term&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Frequency&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the final step we will simply create a frequency table that counts the number of times a certain stock has appeared in a recently created reddit post. For this you should be using the csv you have saved from the NASDAQ stock screener website. The results are listed in the following data frame.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;stonks_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;merge&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ticker_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Term&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;stonks_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stonks_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sort_values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;by&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Frequency&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ascending&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ignore_index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;stonks_df&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/WSB-Favorite-Stocks/02.PNG&#34; alt=&#34;Result&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s interesting to see Macy&amp;rsquo;s on this list, considering that the retailer is practically on its deathbed (although, I did write about how clothing accessories and apparel have experienced the largest sales growth post-lockdown, so maybe not) [&lt;a href=&#34;#6&#34;&gt;6&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Macy&amp;rsquo;s doesn&amp;rsquo;t appear to involve the same factors that sparked the AMC/GME buying frenzy &amp;ndash; vigorously pumping up stocks for companies that are relatively undervalued by the market &amp;ndash; doesn&amp;rsquo;t necessarily apply to all stocks that have piqued the interests of r/WallStreetBets. Of course, you can&amp;rsquo;t be considered &amp;ldquo;day-traders&amp;rdquo; if there is a rhyme or reason as to why you make certain trades by not others.&lt;/p&gt;
&lt;p&gt;As a &amp;ldquo;sophisticated investor,&amp;rdquo; you may be wondering why I would be interested in the chatter of a retail trader platform such as r\WallStreetBets? It&amp;rsquo;s true; our markets and investment philosophies do not converge. However, this information is vital for a much large project that I plan on conducting within a year or so.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t really have a dog in this fight (although, in full disclosure, I am bearish on GME). Still, this information serves as documentation, either for those who seek vindication for being right (on an investment strategy or stock pick) or those who seek to scrub away their awful track records.&lt;/p&gt;
&lt;p&gt;[&lt;a name=&#34;1&#34;&gt;1&lt;/a&gt;] &lt;a href=&#34;https://apnews.com/article/meme-stocks-buzz-fund-wall-street-c1b1086d9b4de2e69a9c790c97dd10e6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Associated Press | &amp;lsquo;Meme stocks&amp;rsquo; go mainstream: There&amp;rsquo;s now a fund for that&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a name=&#34;2&#34;&gt;2&lt;/a&gt;] &lt;a href=&#34;https://kidquant.com/project/analyzing-news-articles-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kidquant | Analyzing News Articles With Python&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a name=&#34;3&#34;&gt;3&lt;/a&gt;] &lt;a href=&#34;https://www.reddit.com/prefs/apps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reddit | Develop your Reddit Application&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a name=&#34;4&#34;&gt;4&lt;/a&gt;] &lt;a href=&#34;https://www.nasdaq.com/market-activity/stocks/screener&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASDAQ | Stock Screener&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a name=&#34;5&#34;&gt;5&lt;/a&gt;] &lt;a href=&#34;https://www.nasdaq.com/market-activity/stocks/screener&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KidQuant | Analyzing New Articles With Python&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a name=&#34;6&#34;&gt;6&lt;/a&gt;] &lt;a href=&#34;https://kidquant.com/post/2020-08-15-covid-impact-retail-sales/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KidQuant | Analyzing the Impact of COVID-19 on Retail&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pairs Trading Strategies in Python</title>
      <link>http://localhost:4321/project/pairs-trading-strategies-in-python/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/project/pairs-trading-strategies-in-python/</guid>
      <description>&lt;h2 id=&#34;pairs-trading-strategies-using-python&#34;&gt;Pairs Trading Strategies Using Python&lt;/h2&gt;
&lt;p&gt;When it comes to making money in the stock market, there are a myriad of different ways to make money. And it seems that in the finance community, everywhere you go, people are telling you that you should learn Python. After all, Python is a popular programming language which can be used in all types of fields, including data science. There are a large number of packages that can help you meet your goals, and many companies use Python for development of data-centric applications and scientific computation, which is associated with the financial world.&lt;/p&gt;
&lt;p&gt;Most of all Python can help us utilize many different trading strategies that (without it) would by very difficult to analyze by hand or with spreadsheets. One of the trading strategies we will talk about is referred to as &lt;strong&gt;Pairs Trading.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;pairs-trading&#34;&gt;Pairs Trading&lt;/h3&gt;
&lt;p&gt;Pairs trading is a form of &lt;em&gt;mean-reversion&lt;/em&gt; that has a distinct advantage of always being hedged against market movements. It is generally a high alpha strategy when backed up by some rigorous statistics. The stratey is based on mathematical analysis.&lt;/p&gt;
&lt;p&gt;The prinicple is as follows. Let&amp;rsquo;s say you have a pair of securities X and Y that have some underlying economic link. An example might be two companies that manufacture the same product, or two companies in one supply chain. If we can model this economic link with a mathematical model, we can make trades on it.&lt;/p&gt;
&lt;p&gt;In order to understand pairs trading, we need to understand three mathematical concepts: &lt;strong&gt;Stationarity, Integration, and Cointegration.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This will assume everyone knows the basics of &lt;a href=&#34;http://mathworld.wolfram.com/HypothesisTesting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hypothesis testing&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;statsmodels&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;statsmodels.api&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sm&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;statsmodels.tsa.stattools&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;coint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adfuller&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;seaborn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;whitegrid&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;stationaritynon-stationarity&#34;&gt;Stationarity/Non-Stationarity&lt;/h3&gt;
&lt;p&gt;Stationarity is the most commonly untested assumption in time series analysis. We generally assume that data is stationary when the parameters of the data generating process do not change over time. Else consider two series: A and B. Series A will generate a stationary time series with fixed parameters, while B will change over time.&lt;/p&gt;
&lt;p&gt;We will create a function that creates a z-score for probability density function. The probability density for a Gaussian distribution is:&lt;/p&gt;
&lt;p&gt;$$ p(x) = \frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}$$&lt;/p&gt;
&lt;p&gt;where $\mu$ is the mean and  $\sigma$ the standard deviation. The square of the standard deviation, $\sigma^{2}$, is the variance. The empircal rule dictates that 66% of the data should be somewhere between $x+\sigma$ and $x-\sigma$, which implies that the function &lt;code&gt;numpy.random.normal&lt;/code&gt; is more likely to return samples lying close to the mean, rather than those far away.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;generate_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From there, we can create two plots that exhibit a stationary and non-stationary time series. The left time series will be stationary, whereas the right will be non-stationary.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/statonarity_comparison.png&#34; alt=&#34;stationarity comparison&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;why-stationarity-is-important&#34;&gt;Why Stationarity is Important&lt;/h3&gt;
&lt;p&gt;Many statistical tests require that the data being tested are stationary. Using certain statistics on a non-stationary data set may lead to garbage results. As an example, let&amp;rsquo;s take an average through our non-stationary $B$.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/average_timeseries_seriesB.png&#34; alt=&#34;Averge time series&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The computed mean will show that the mean of all data points, but won&amp;rsquo;t be useful for any forecasting of the future state. It&amp;rsquo;s meaningless when compared with any specific time, as it&amp;rsquo;s a collection of different states at different times mashed together. This is just a simple and clear example of why non-stationarity can distort the analysis, much more subtle problems can arise in practice.&lt;/p&gt;
&lt;h4 id=&#34;augmented-dickey-fuller&#34;&gt;Augmented Dickey Fuller&lt;/h4&gt;
&lt;p&gt;In order to test for stationarity, we need to test for something called a &lt;em&gt;unit root&lt;/em&gt;. Autoregressive unit root test is based on the following hypothesis test:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_{0} &amp;amp; : \phi =\ 1\ \implies y_{t} \sim I(0) \ | \ (unit \ root) \
H_{1} &amp;amp; : |\phi| &amp;lt;\ 1\ \implies y_{t} \sim I(0) \ | \ (stationary)  \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s referred to as a unit root tet because under the null hypothesis, the autoregressive polynominal of $\mathcal{z}_{t},\ \phi (\mathcal{z})=\ (1-\phi \mathcal{z}) \ = 0$, has a root equal to unity.&lt;/p&gt;
&lt;p&gt;$y_{t}$ is trend stationary under the null hypothesis. If $y_{t}$is then first differenced, it becomes:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\Delta y_{t} &amp;amp; = \delta\ + \Delta\mathcal{z}&lt;em&gt;{t} \
\Delta \mathcal{z} &amp;amp; = \phi\Delta\mathcal{z}&lt;/em&gt;{t-1}\ +\ \varepsilon_{t}\ -\ \varepsilon_{t-1} \
\end{aligned}
.$$&lt;/p&gt;
&lt;p&gt;The test statistic is&lt;/p&gt;
&lt;p&gt;$$ t_{\phi=1}=\frac{\hat{\phi}-1}{SE(\hat{\phi})}$$&lt;/p&gt;
&lt;p&gt;$\hat{\phi}$ is the least square estimate, and SE($\hat{\phi}$) is the usual standard error estimate. The test is a one-sided left tail test. If {$y_{t}$} is stationary, then it can be shown that&lt;/p&gt;
&lt;p&gt;$$\sqrt{T}(\hat{\phi}-\phi)\xrightarrow[\text{}]{\text{d}}N(0,(1-\phi^{2}))$$&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;$$\hat{\phi}\overset{\text{A}}{\sim}N\bigg(\phi,\frac{1}{T}(1-\phi^{2}) \bigg)$$&lt;/p&gt;
&lt;p&gt;and it follows that $t_{\phi=1}\overset{\text{A}}{\sim}N(0,1).$ However, under the null hypothesis of non-stationarity, the above result gives&lt;/p&gt;
&lt;p&gt;$$
\hat{\phi}\overset{\text{A}}{\sim} N(0,1)
$$&lt;/p&gt;
&lt;p&gt;The following function will allow us to check for stationarity using the Augmented Dickey-Fuller (ADF) test.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;stationarity_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cutoff&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# H_0 in adfuller is unit root exists (non-stationary)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# We must observe significant p-value to convince ourselves that the series is stationary&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adfuller&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cutoff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;p-value = &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; The series &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; is likely stationary.&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;p-value = &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;stationarity_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;stationarity_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;p-value = 4.0811223061569216e-17 The series A is likely stationary.&lt;br&gt;
p-value = 0.7317208279589542 The series B is likely non-stationary.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;cointegration&#34;&gt;Cointegration&lt;/h3&gt;
&lt;p&gt;The correlations between financial quantities are notoriously unstable. Nevertheless, correlations are regularly used in almost all multivariate financial problems. An alternative statistical measure to correlation is cointegration. This is probably a more robust measure of linkage between two financial quantities, but as yet there is little derivatives theory based on this concept.&lt;/p&gt;
&lt;p&gt;Two stocks may be perfectly correlated over short timescales, yet diverge in the long run, with one growing and the other decaying. Conversely, two stocks may follow each other, never being more than a certain distance apart, but with any correlation, positive, negative, or varying. If we are delta hedging, then maybe the short timescale correlation matters, but not if we are holding stocks for a long time in an unhedged portfolio.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve constructed an example of two cointegrated series. We&amp;rsquo;ll plot the difference between the two now so we can see how this looks.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/cointegration_spread.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;testing-for-cointegration&#34;&gt;Testing for Cointegration&lt;/h4&gt;
&lt;p&gt;The steps in the cointegration test procdure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Test for a unit root in each component series $y_{t}$ individually, using the univariate unit root tests, says ADF, PP test.&lt;/li&gt;
&lt;li&gt;If the unit root cannot be rejected, then the next step is to test cointegration among the components, i.e., to test whether $\alpha Y_{t}$ is I(0).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we find that the time series as a unit root, then we move on to the cointegration process. There are three main methods for testing for cointegration: Johansen, Engle-Granger, and Phillips-Ouliaris. We will primarily use the Engle-Granger test.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider the regression model for $y_{t}$:&lt;/p&gt;
&lt;p&gt;$$y_{1t} = \delta D_{t} + \phi_{1t}y_{2t} + \phi_{m-1} y_{mt} + \varepsilon_{t} $$&lt;/p&gt;
&lt;p&gt;$D_{t}$ is the deterministic term. From there, we can test whether $\varepsilon_{t}$ is $I(1)$ or $I(0)$. The hypothesis test is as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_{0} &amp;amp; :  \varepsilon_{t} \sim I(1) \implies y_{t} \ (no \ cointegration)  \
H_{1} &amp;amp; : \varepsilon_{t} \sim I(0) \implies y_{t} \ (cointegration)  \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If  the time series is cointegrated, then $y_{t}$ is cointegrated with a &lt;em&gt;normalized cointegration vector&lt;/em&gt; $\alpha = (1, \phi_{1}, \ldots,\phi_{m-1}).$&lt;/p&gt;
&lt;p&gt;We also use residuals $\varepsilon_{t}$ for unit root test.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_{0} &amp;amp; :  \lambda = 0 \ (Unit \ Root)  \
H_{1} &amp;amp; : \lambda &amp;lt; 1 \ (Stationary)  \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;This hypothesis test is for the model:&lt;/p&gt;
&lt;p&gt;$$\Delta\varepsilon_{t}=\lambda\varepsilon_{t-1}+\sum^{p-1}&lt;em&gt;{j=1}\varphi\Delta\varepsilon&lt;/em&gt;{t-j}+\alpha_{t}$$&lt;/p&gt;
&lt;p&gt;The test statistic for the following equation:&lt;/p&gt;
&lt;p&gt;$$t_{\lambda}=\frac{\hat{\lambda}}{s_{\hat{\lambda}}} $$&lt;/p&gt;
&lt;p&gt;Now that you understand what it means for these time series to be cointegrated, we can test for it and measure it using python:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;coint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;correlation-vs-cointegration&#34;&gt;Correlation vs. Cointegration&lt;/h4&gt;
&lt;p&gt;Correlation and cointegration, while theoretically similar, are anything but similar. To demonstrate this, we can look at examples of these time series that are correlated, but not cointegrated.&lt;/p&gt;
&lt;p&gt;A simple example is two series that just diverge.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/diverge.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Next, we can print the correlation coefficient, $r$, and the cointegration test.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Correlation: &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_diverging&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Y_diverging&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;coint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_diverging&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Y_diverging&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Cointegration test p-value: &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;Correlation: 0.9918846224870514&lt;br&gt;
Cointegration test p-value: 0.915621777573125&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As we can see, there is a very strong (nearly perfect) correlation between series X and Y. However, our p-value for the cointegration test yields a result of 0.7092, which means there is no cointegration between time series X and Y.&lt;/p&gt;
&lt;p&gt;Another example of this case is a normally distributed series and a sqaure wave.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/squared_wave.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Although the correlation is incredibly low, the p-value shows that these time series are cointegrated.&lt;/p&gt;
&lt;h3 id=&#34;data-science-in-trading&#34;&gt;Data Science in Trading&lt;/h3&gt;
&lt;p&gt;Before we begin, I’ll first define a function that makes it easy to find cointegrated security pairs using the concepts we’ve already covered.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;find_cointegrated_pairs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;score_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pvalue_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pairs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;S1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;S2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;coint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;S1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;S2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;score_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;pvalue_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.05&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;pairs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;score_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pairs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We are looking through a set of tech companies to see if any of them are cointegrated. We&amp;rsquo;ll start by defining the list of securities we want to look through. Then we&amp;rsquo;ll get the pricing data for each security from the year 2013 - 2018.&lt;/p&gt;
&lt;p&gt;As mentioned before, we have formulated an economic hypothesis that there is some sort of link between a subset of securities within the tech sector, and we want to test whether there are any cointegrated pairs. This incurs significantly less multiple comparisons bias than searching through hundreds of securities and slightly more than forming a hypothesis for an individual test.&lt;/p&gt;
&lt;p&gt;We have decided to analyze the following technology stocks: AAPL, ADBE, SYMC, EBAY, MSFT, QCOM, HPQ, JNPR, AMD, and IBM.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve also decided to include the S&amp;amp;P 500 in our analysis, just in case there was equity that was cointegrated with the entire market.&lt;/p&gt;
&lt;p&gt;After extracting the historical prices, we will create a heatmap that will show the p-values of the cointegrated test between each pair of stocks. The heatmap will mask all pairs with a p-value greater than 0.05.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/heatmap.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Our algorithm listed two pairs that are cointegrated: AAPL/EBAY, and ABDE/MSFT. We will analyze the ABDE/MSFT equity pair.&lt;/p&gt;
&lt;h4 id=&#34;calculating-the-spread&#34;&gt;Calculating the Spread&lt;/h4&gt;
&lt;p&gt;Now we can plot the spread of these time series. To actually calculate the spread, we use a linear regression to get the coefficient for the linear combination to construct between our two securities, as mentioned with the Engle-Granger method before.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/spread.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Alternatively, we can examine the ration between the time series.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/ratio.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Regardless of whether or not we use the spread approach or the ratio approach, we can see that our first plot pair ADBE/SYMC tends to move around the mean. We now need to standardize this ratio because the absolute ratio might not be the most ideal way of analyzing this trend. For this, we need to use z-scores.&lt;/p&gt;
&lt;p&gt;A z-score is the number of standard deviations a data point is from the mean. More importantly, the number of standard deviations above or below the population mean is from the raw score. The z-score is calculated by the following:&lt;/p&gt;
&lt;p&gt;$$\mathcal{z}&lt;em&gt;{i}=\frac{x&lt;/em&gt;{i}-\bar{x}}{s} $$&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/standard_deviation.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;By setting two other lines placed at the z-score of 1 and -1, we can clearly see that for the most part, any big divergences from the mean eventually converge back. This is precisely what we want for a pairs trading strategy.&lt;/p&gt;
&lt;h3 id=&#34;trading-signals&#34;&gt;Trading Signals&lt;/h3&gt;
&lt;p&gt;When conducting any type of trading strategy, it&amp;rsquo;s always important to clearly define and delineate at what point you will actually make a trade. As in, what is the best indicator that I need to buy or sell a particular stock?&lt;/p&gt;
&lt;h4 id=&#34;setup-rules&#34;&gt;Setup rules&lt;/h4&gt;
&lt;p&gt;We&amp;rsquo;re going to use the ratio time series that we&amp;rsquo;ve created to see if it tells us whether to buy or sell a particular moment in time. We&amp;rsquo;ll start off by creating a prediction variable $Y$. If the ratio is positive, it will signal a &amp;ldquo;buy,&amp;rdquo; otherwise, it will signal a sell. The prediction model is as follows:&lt;/p&gt;
&lt;p&gt;$$Y_{t} = sign(Ratio_{t+1}-Ratio_{t}) $$&lt;/p&gt;
&lt;p&gt;What&amp;rsquo;s great about pair trading signals is that we don&amp;rsquo;t need to know absolutes about where the prices will go, all we need to know is where it&amp;rsquo;s heading: up or down.&lt;/p&gt;
&lt;h4 id=&#34;train-test-split&#34;&gt;Train Test Split&lt;/h4&gt;
&lt;p&gt;When training and testing a model, it&amp;rsquo;s common to have splits of 70/30 or 80/20. We only used a time series of 252 points (which is the number of trading days in a year). Before training and splitting the data, we will add more data points in each time series.&lt;/p&gt;
&lt;h4 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h4&gt;
&lt;p&gt;We need to find out what features are actually important in determining the direction of the ratio moves. Knowing that the ratios always eventually revert back to the mean, maybe the moving averages and metrics related to the mean will be important.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try using these features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;60 day Moving Average of Ratio&lt;/li&gt;
&lt;li&gt;5 day Moving Average of Ratio&lt;/li&gt;
&lt;li&gt;60 day Standard Deviation&lt;/li&gt;
&lt;li&gt;z score&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/rolling_mean.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;creating-a-model&#34;&gt;Creating a Model&lt;/h4&gt;
&lt;p&gt;A standard normal distribution has a mean of 0 and a standard deviation 1. Looking at the plot, it&amp;rsquo;s pretty clear that if the time series moves 1 standard deviation beyond the mean, it tends to revert back towards the mean. Using these models, we can create the following trading signals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Buy(1) whenever the z-score is below -1, meaning we expect the ratio to increase.&lt;/li&gt;
&lt;li&gt;Sell(-1) whenever the z-score is above 1, meaning we expect the ratio to decrease.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;training-optimizing&#34;&gt;Training Optimizing&lt;/h4&gt;
&lt;p&gt;We can use our model on actual data&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/Trading_Signals.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;areas-of-improvement-and-further-steps&#34;&gt;Areas of Improvement and Further Steps&lt;/h3&gt;
&lt;p&gt;This is by no means a perfect strategy and the implementation of our strategy isn&amp;rsquo;t the best. However, there are several things that can be improved upon.&lt;/p&gt;
&lt;h4 id=&#34;1-using-more-securities-and-more-varied-time-ranges&#34;&gt;1. Using more securities and more varied time ranges&lt;/h4&gt;
&lt;p&gt;For the pairs trading strategy cointegration test, I only used a handful of stocks. Naturally (and in practice) it would be more useful to use clusters within an industry. I only use the time range of only 5 years, which may not be representative of stock market volatility.&lt;/p&gt;
&lt;h4 id=&#34;2-dealing-with-overfitting&#34;&gt;2. Dealing with overfitting&lt;/h4&gt;
&lt;p&gt;Anything related to data analysis and training models has much to do with the problem of overfitting. There are many different ways to deal with overfitting like validation, such as Kalman filters, and other statistical methods.&lt;/p&gt;
&lt;h4 id=&#34;3-adjusting-the-trading-signals&#34;&gt;3. Adjusting the trading signals&lt;/h4&gt;
&lt;p&gt;Our trading algorithm fails to account for stock prices that overlap and cross each other. Considering that the code only calls for a buy or sell given its ratio, it doesn&amp;rsquo;t take into account which stock is actually higher or lower.&lt;/p&gt;
&lt;h4 id=&#34;4-more-advanced-methods&#34;&gt;4. More advanced methods&lt;/h4&gt;
&lt;p&gt;This is just the tip of the iceberg of what you can do with algorithmic pairs trading. It&amp;rsquo;s simple because it only deals with moving averages and ratios. If you want to use more complicated statistics, feel free to do so. Other complex examples include subjects such as the Hurst exponent, half-life mean reversion, and Kalman Filters.&lt;/p&gt;
&lt;h3 id=&#34;github-code&#34;&gt;GitHub Code&lt;/h3&gt;
&lt;p&gt;Click here for the full &lt;a href=&#34;https://github.com/Hedgology/Pairs-Trading-With-Python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github Code&lt;/a&gt; and explainations.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
