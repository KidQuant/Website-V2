<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | KidQuant</title>
    <link>http://localhost:4321/tag/machine-learning/</link>
      <atom:link href="http://localhost:4321/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 04 Mar 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:4321/media/icon_huf195f9a67bfe19c82d5f0ba6a67922fa_2030_512x512_fill_lanczos_center_3.png</url>
      <title>Machine Learning</title>
      <link>http://localhost:4321/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Pairs Trading Strategies in Python</title>
      <link>http://localhost:4321/project/pairs-trading-strategies-in-python/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/project/pairs-trading-strategies-in-python/</guid>
      <description>&lt;h2 id=&#34;pairs-trading-strategies-using-python&#34;&gt;Pairs Trading Strategies Using Python&lt;/h2&gt;
&lt;p&gt;When it comes to making money in the stock market, there are a myriad of different ways to make money. And it seems that in the finance community, everywhere you go, people are telling you that you should learn Python. After all, Python is a popular programming language which can be used in all types of fields, including data science. There are a large number of packages that can help you meet your goals, and many companies use Python for development of data-centric applications and scientific computation, which is associated with the financial world.&lt;/p&gt;
&lt;p&gt;Most of all Python can help us utilize many different trading strategies that (without it) would by very difficult to analyze by hand or with spreadsheets. One of the trading strategies we will talk about is referred to as &lt;strong&gt;Pairs Trading.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;pairs-trading&#34;&gt;Pairs Trading&lt;/h3&gt;
&lt;p&gt;Pairs trading is a form of &lt;em&gt;mean-reversion&lt;/em&gt; that has a distinct advantage of always being hedged against market movements. It is generally a high alpha strategy when backed up by some rigorous statistics. The stratey is based on mathematical analysis.&lt;/p&gt;
&lt;p&gt;The prinicple is as follows. Let&amp;rsquo;s say you have a pair of securities X and Y that have some underlying economic link. An example might be two companies that manufacture the same product, or two companies in one supply chain. If we can model this economic link with a mathematical model, we can make trades on it.&lt;/p&gt;
&lt;p&gt;In order to understand pairs trading, we need to understand three mathematical concepts: &lt;strong&gt;Stationarity, Integration, and Cointegration.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This will assume everyone knows the basics of &lt;a href=&#34;http://mathworld.wolfram.com/HypothesisTesting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hypothesis testing&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;statsmodels&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;statsmodels.api&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sm&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;statsmodels.tsa.stattools&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;coint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adfuller&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;seaborn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;whitegrid&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;stationaritynon-stationarity&#34;&gt;Stationarity/Non-Stationarity&lt;/h3&gt;
&lt;p&gt;Stationarity is the most commonly untested assumption in time series analysis. We generally assume that data is stationary when the parameters of the data generating process do not change over time. Else consider two series: A and B. Series A will generate a stationary time series with fixed parameters, while B will change over time.&lt;/p&gt;
&lt;p&gt;We will create a function that creates a z-score for probability density function. The probability density for a Gaussian distribution is:&lt;/p&gt;
&lt;p&gt;$$ p(x) = \frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}$$&lt;/p&gt;
&lt;p&gt;where $\mu$ is the mean and  $\sigma$ the standard deviation. The square of the standard deviation, $\sigma^{2}$, is the variance. The empircal rule dictates that 66% of the data should be somewhere between $x+\sigma$ and $x-\sigma$, which implies that the function &lt;code&gt;numpy.random.normal&lt;/code&gt; is more likely to return samples lying close to the mean, rather than those far away.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;generate_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From there, we can create two plots that exhibit a stationary and non-stationary time series. The left time series will be stationary, whereas the right will be non-stationary.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/statonarity_comparison.png&#34; alt=&#34;stationarity comparison&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;why-stationarity-is-important&#34;&gt;Why Stationarity is Important&lt;/h3&gt;
&lt;p&gt;Many statistical tests require that the data being tested are stationary. Using certain statistics on a non-stationary data set may lead to garbage results. As an example, let&amp;rsquo;s take an average through our non-stationary $B$.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/average_timeseries_seriesB.png&#34; alt=&#34;Averge time series&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The computed mean will show that the mean of all data points, but won&amp;rsquo;t be useful for any forecasting of the future state. It&amp;rsquo;s meaningless when compared with any specific time, as it&amp;rsquo;s a collection of different states at different times mashed together. This is just a simple and clear example of why non-stationarity can distort the analysis, much more subtle problems can arise in practice.&lt;/p&gt;
&lt;h4 id=&#34;augmented-dickey-fuller&#34;&gt;Augmented Dickey Fuller&lt;/h4&gt;
&lt;p&gt;In order to test for stationarity, we need to test for something called a &lt;em&gt;unit root&lt;/em&gt;. Autoregressive unit root test is based on the following hypothesis test:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_{0} &amp;amp; : \phi =\ 1\ \implies y_{t} \sim I(0) \ | \ (unit \ root) \
H_{1} &amp;amp; : |\phi| &amp;lt;\ 1\ \implies y_{t} \sim I(0) \ | \ (stationary)  \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s referred to as a unit root tet because under the null hypothesis, the autoregressive polynominal of $\mathcal{z}_{t},\ \phi (\mathcal{z})=\ (1-\phi \mathcal{z}) \ = 0$, has a root equal to unity.&lt;/p&gt;
&lt;p&gt;$y_{t}$ is trend stationary under the null hypothesis. If $y_{t}$is then first differenced, it becomes:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\Delta y_{t} &amp;amp; = \delta\ + \Delta\mathcal{z}&lt;em&gt;{t} \
\Delta \mathcal{z} &amp;amp; = \phi\Delta\mathcal{z}&lt;/em&gt;{t-1}\ +\ \varepsilon_{t}\ -\ \varepsilon_{t-1} \
\end{aligned}
.$$&lt;/p&gt;
&lt;p&gt;The test statistic is&lt;/p&gt;
&lt;p&gt;$$ t_{\phi=1}=\frac{\hat{\phi}-1}{SE(\hat{\phi})}$$&lt;/p&gt;
&lt;p&gt;$\hat{\phi}$ is the least square estimate, and SE($\hat{\phi}$) is the usual standard error estimate. The test is a one-sided left tail test. If {$y_{t}$} is stationary, then it can be shown that&lt;/p&gt;
&lt;p&gt;$$\sqrt{T}(\hat{\phi}-\phi)\xrightarrow[\text{}]{\text{d}}N(0,(1-\phi^{2}))$$&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;$$\hat{\phi}\overset{\text{A}}{\sim}N\bigg(\phi,\frac{1}{T}(1-\phi^{2}) \bigg)$$&lt;/p&gt;
&lt;p&gt;and it follows that $t_{\phi=1}\overset{\text{A}}{\sim}N(0,1).$ However, under the null hypothesis of non-stationarity, the above result gives&lt;/p&gt;
&lt;p&gt;$$
\hat{\phi}\overset{\text{A}}{\sim} N(0,1)
$$&lt;/p&gt;
&lt;p&gt;The following function will allow us to check for stationarity using the Augmented Dickey-Fuller (ADF) test.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;stationarity_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cutoff&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# H_0 in adfuller is unit root exists (non-stationary)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# We must observe significant p-value to convince ourselves that the series is stationary&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adfuller&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cutoff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;p-value = &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; The series &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; is likely stationary.&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;p-value = &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;stationarity_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;stationarity_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;p-value = 4.0811223061569216e-17 The series A is likely stationary.&lt;br&gt;
p-value = 0.7317208279589542 The series B is likely non-stationary.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;cointegration&#34;&gt;Cointegration&lt;/h3&gt;
&lt;p&gt;The correlations between financial quantities are notoriously unstable. Nevertheless, correlations are regularly used in almost all multivariate financial problems. An alternative statistical measure to correlation is cointegration. This is probably a more robust measure of linkage between two financial quantities, but as yet there is little derivatives theory based on this concept.&lt;/p&gt;
&lt;p&gt;Two stocks may be perfectly correlated over short timescales, yet diverge in the long run, with one growing and the other decaying. Conversely, two stocks may follow each other, never being more than a certain distance apart, but with any correlation, positive, negative, or varying. If we are delta hedging, then maybe the short timescale correlation matters, but not if we are holding stocks for a long time in an unhedged portfolio.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve constructed an example of two cointegrated series. We&amp;rsquo;ll plot the difference between the two now so we can see how this looks.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/cointegration_spread.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;testing-for-cointegration&#34;&gt;Testing for Cointegration&lt;/h4&gt;
&lt;p&gt;The steps in the cointegration test procdure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Test for a unit root in each component series $y_{t}$ individually, using the univariate unit root tests, says ADF, PP test.&lt;/li&gt;
&lt;li&gt;If the unit root cannot be rejected, then the next step is to test cointegration among the components, i.e., to test whether $\alpha Y_{t}$ is I(0).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we find that the time series as a unit root, then we move on to the cointegration process. There are three main methods for testing for cointegration: Johansen, Engle-Granger, and Phillips-Ouliaris. We will primarily use the Engle-Granger test.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider the regression model for $y_{t}$:&lt;/p&gt;
&lt;p&gt;$$y_{1t} = \delta D_{t} + \phi_{1t}y_{2t} + \phi_{m-1} y_{mt} + \varepsilon_{t} $$&lt;/p&gt;
&lt;p&gt;$D_{t}$ is the deterministic term. From there, we can test whether $\varepsilon_{t}$ is $I(1)$ or $I(0)$. The hypothesis test is as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_{0} &amp;amp; :  \varepsilon_{t} \sim I(1) \implies y_{t} \ (no \ cointegration)  \
H_{1} &amp;amp; : \varepsilon_{t} \sim I(0) \implies y_{t} \ (cointegration)  \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If  the time series is cointegrated, then $y_{t}$ is cointegrated with a &lt;em&gt;normalized cointegration vector&lt;/em&gt; $\alpha = (1, \phi_{1}, \ldots,\phi_{m-1}).$&lt;/p&gt;
&lt;p&gt;We also use residuals $\varepsilon_{t}$ for unit root test.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_{0} &amp;amp; :  \lambda = 0 \ (Unit \ Root)  \
H_{1} &amp;amp; : \lambda &amp;lt; 1 \ (Stationary)  \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;This hypothesis test is for the model:&lt;/p&gt;
&lt;p&gt;$$\Delta\varepsilon_{t}=\lambda\varepsilon_{t-1}+\sum^{p-1}&lt;em&gt;{j=1}\varphi\Delta\varepsilon&lt;/em&gt;{t-j}+\alpha_{t}$$&lt;/p&gt;
&lt;p&gt;The test statistic for the following equation:&lt;/p&gt;
&lt;p&gt;$$t_{\lambda}=\frac{\hat{\lambda}}{s_{\hat{\lambda}}} $$&lt;/p&gt;
&lt;p&gt;Now that you understand what it means for these time series to be cointegrated, we can test for it and measure it using python:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;coint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;correlation-vs-cointegration&#34;&gt;Correlation vs. Cointegration&lt;/h4&gt;
&lt;p&gt;Correlation and cointegration, while theoretically similar, are anything but similar. To demonstrate this, we can look at examples of these time series that are correlated, but not cointegrated.&lt;/p&gt;
&lt;p&gt;A simple example is two series that just diverge.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/diverge.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Next, we can print the correlation coefficient, $r$, and the cointegration test.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Correlation: &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_diverging&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Y_diverging&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;coint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_diverging&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Y_diverging&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Cointegration test p-value: &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;Correlation: 0.9918846224870514&lt;br&gt;
Cointegration test p-value: 0.915621777573125&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As we can see, there is a very strong (nearly perfect) correlation between series X and Y. However, our p-value for the cointegration test yields a result of 0.7092, which means there is no cointegration between time series X and Y.&lt;/p&gt;
&lt;p&gt;Another example of this case is a normally distributed series and a sqaure wave.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/squared_wave.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Although the correlation is incredibly low, the p-value shows that these time series are cointegrated.&lt;/p&gt;
&lt;h3 id=&#34;data-science-in-trading&#34;&gt;Data Science in Trading&lt;/h3&gt;
&lt;p&gt;Before we begin, I’ll first define a function that makes it easy to find cointegrated security pairs using the concepts we’ve already covered.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;find_cointegrated_pairs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;score_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pvalue_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pairs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;S1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;S2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;coint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;S1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;S2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;score_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;pvalue_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.05&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;pairs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;score_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pvalue_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pairs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We are looking through a set of tech companies to see if any of them are cointegrated. We&amp;rsquo;ll start by defining the list of securities we want to look through. Then we&amp;rsquo;ll get the pricing data for each security from the year 2013 - 2018.&lt;/p&gt;
&lt;p&gt;As mentioned before, we have formulated an economic hypothesis that there is some sort of link between a subset of securities within the tech sector, and we want to test whether there are any cointegrated pairs. This incurs significantly less multiple comparisons bias than searching through hundreds of securities and slightly more than forming a hypothesis for an individual test.&lt;/p&gt;
&lt;p&gt;We have decided to analyze the following technology stocks: AAPL, ADBE, SYMC, EBAY, MSFT, QCOM, HPQ, JNPR, AMD, and IBM.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve also decided to include the S&amp;amp;P 500 in our analysis, just in case there was equity that was cointegrated with the entire market.&lt;/p&gt;
&lt;p&gt;After extracting the historical prices, we will create a heatmap that will show the p-values of the cointegrated test between each pair of stocks. The heatmap will mask all pairs with a p-value greater than 0.05.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/heatmap.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Our algorithm listed two pairs that are cointegrated: AAPL/EBAY, and ABDE/MSFT. We will analyze the ABDE/MSFT equity pair.&lt;/p&gt;
&lt;h4 id=&#34;calculating-the-spread&#34;&gt;Calculating the Spread&lt;/h4&gt;
&lt;p&gt;Now we can plot the spread of these time series. To actually calculate the spread, we use a linear regression to get the coefficient for the linear combination to construct between our two securities, as mentioned with the Engle-Granger method before.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/spread.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Alternatively, we can examine the ration between the time series.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/ratio.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Regardless of whether or not we use the spread approach or the ratio approach, we can see that our first plot pair ADBE/SYMC tends to move around the mean. We now need to standardize this ratio because the absolute ratio might not be the most ideal way of analyzing this trend. For this, we need to use z-scores.&lt;/p&gt;
&lt;p&gt;A z-score is the number of standard deviations a data point is from the mean. More importantly, the number of standard deviations above or below the population mean is from the raw score. The z-score is calculated by the following:&lt;/p&gt;
&lt;p&gt;$$\mathcal{z}&lt;em&gt;{i}=\frac{x&lt;/em&gt;{i}-\bar{x}}{s} $$&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/standard_deviation.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;By setting two other lines placed at the z-score of 1 and -1, we can clearly see that for the most part, any big divergences from the mean eventually converge back. This is precisely what we want for a pairs trading strategy.&lt;/p&gt;
&lt;h3 id=&#34;trading-signals&#34;&gt;Trading Signals&lt;/h3&gt;
&lt;p&gt;When conducting any type of trading strategy, it&amp;rsquo;s always important to clearly define and delineate at what point you will actually make a trade. As in, what is the best indicator that I need to buy or sell a particular stock?&lt;/p&gt;
&lt;h4 id=&#34;setup-rules&#34;&gt;Setup rules&lt;/h4&gt;
&lt;p&gt;We&amp;rsquo;re going to use the ratio time series that we&amp;rsquo;ve created to see if it tells us whether to buy or sell a particular moment in time. We&amp;rsquo;ll start off by creating a prediction variable $Y$. If the ratio is positive, it will signal a &amp;ldquo;buy,&amp;rdquo; otherwise, it will signal a sell. The prediction model is as follows:&lt;/p&gt;
&lt;p&gt;$$Y_{t} = sign(Ratio_{t+1}-Ratio_{t}) $$&lt;/p&gt;
&lt;p&gt;What&amp;rsquo;s great about pair trading signals is that we don&amp;rsquo;t need to know absolutes about where the prices will go, all we need to know is where it&amp;rsquo;s heading: up or down.&lt;/p&gt;
&lt;h4 id=&#34;train-test-split&#34;&gt;Train Test Split&lt;/h4&gt;
&lt;p&gt;When training and testing a model, it&amp;rsquo;s common to have splits of 70/30 or 80/20. We only used a time series of 252 points (which is the number of trading days in a year). Before training and splitting the data, we will add more data points in each time series.&lt;/p&gt;
&lt;h4 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h4&gt;
&lt;p&gt;We need to find out what features are actually important in determining the direction of the ratio moves. Knowing that the ratios always eventually revert back to the mean, maybe the moving averages and metrics related to the mean will be important.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try using these features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;60 day Moving Average of Ratio&lt;/li&gt;
&lt;li&gt;5 day Moving Average of Ratio&lt;/li&gt;
&lt;li&gt;60 day Standard Deviation&lt;/li&gt;
&lt;li&gt;z score&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/rolling_mean.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;creating-a-model&#34;&gt;Creating a Model&lt;/h4&gt;
&lt;p&gt;A standard normal distribution has a mean of 0 and a standard deviation 1. Looking at the plot, it&amp;rsquo;s pretty clear that if the time series moves 1 standard deviation beyond the mean, it tends to revert back towards the mean. Using these models, we can create the following trading signals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Buy(1) whenever the z-score is below -1, meaning we expect the ratio to increase.&lt;/li&gt;
&lt;li&gt;Sell(-1) whenever the z-score is above 1, meaning we expect the ratio to decrease.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;training-optimizing&#34;&gt;Training Optimizing&lt;/h4&gt;
&lt;p&gt;We can use our model on actual data&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/Trading_Signals.png&#34; alt=&#34;cointegration spread&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;areas-of-improvement-and-further-steps&#34;&gt;Areas of Improvement and Further Steps&lt;/h3&gt;
&lt;p&gt;This is by no means a perfect strategy and the implementation of our strategy isn&amp;rsquo;t the best. However, there are several things that can be improved upon.&lt;/p&gt;
&lt;h4 id=&#34;1-using-more-securities-and-more-varied-time-ranges&#34;&gt;1. Using more securities and more varied time ranges&lt;/h4&gt;
&lt;p&gt;For the pairs trading strategy cointegration test, I only used a handful of stocks. Naturally (and in practice) it would be more useful to use clusters within an industry. I only use the time range of only 5 years, which may not be representative of stock market volatility.&lt;/p&gt;
&lt;h4 id=&#34;2-dealing-with-overfitting&#34;&gt;2. Dealing with overfitting&lt;/h4&gt;
&lt;p&gt;Anything related to data analysis and training models has much to do with the problem of overfitting. There are many different ways to deal with overfitting like validation, such as Kalman filters, and other statistical methods.&lt;/p&gt;
&lt;h4 id=&#34;3-adjusting-the-trading-signals&#34;&gt;3. Adjusting the trading signals&lt;/h4&gt;
&lt;p&gt;Our trading algorithm fails to account for stock prices that overlap and cross each other. Considering that the code only calls for a buy or sell given its ratio, it doesn&amp;rsquo;t take into account which stock is actually higher or lower.&lt;/p&gt;
&lt;h4 id=&#34;4-more-advanced-methods&#34;&gt;4. More advanced methods&lt;/h4&gt;
&lt;p&gt;This is just the tip of the iceberg of what you can do with algorithmic pairs trading. It&amp;rsquo;s simple because it only deals with moving averages and ratios. If you want to use more complicated statistics, feel free to do so. Other complex examples include subjects such as the Hurst exponent, half-life mean reversion, and Kalman Filters.&lt;/p&gt;
&lt;h3 id=&#34;github-code&#34;&gt;GitHub Code&lt;/h3&gt;
&lt;p&gt;Click here for the full &lt;a href=&#34;https://github.com/Hedgology/Pairs-Trading-With-Python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github Code&lt;/a&gt; and explainations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting the 2018 World Cup Winner Using Machine Learning</title>
      <link>http://localhost:4321/project/predicting-the-2018-fifa-world-cup-winner-using-machine-learning/</link>
      <pubDate>Sun, 03 Jun 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/project/predicting-the-2018-fifa-world-cup-winner-using-machine-learning/</guid>
      <description>&lt;p&gt;With the start of the 2018 FIFA World Cup approaching, soccer fans around the world is trying to figure out who will win the tournament? If you&amp;rsquo;re a techie and a soccer fan, you&amp;rsquo;ll probably want to figure out more quantitiative methods of figuring out the answer to that question.&lt;/p&gt;
&lt;h2 id=&#34;goal&#34;&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The goal of this project is established as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use Machine Learning to predict who will win the 2018 FIFA World Cup.&lt;/li&gt;
&lt;li&gt;Predict the outcome of individual matches for the entire competition.&lt;/li&gt;
&lt;li&gt;Run simulation of the next matches i.e. quarter final, semi finals and finals.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These goals present a unique real-world Machine Learning prediction problem and involve solving various Machine Learning task: data integration, feature modeling, and outcome prediction.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I used two data sets from Kaggle. You can find them &lt;a href=&#34;https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017/data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. We will use results of historical matches since the beginning of the 1930s FIFA Championship for all participating teams.&lt;/p&gt;
&lt;p&gt;We have opted not to use FIFA national team rankings, due to the limitations of FIFA rankings being created in the 90&amp;rsquo;s. Considering this limitation, we will stick to the historical match records.&lt;/p&gt;
&lt;p&gt;First, I&amp;rsquo;m going to conduct some exploratory analysis on the two datasets. Next, I will use a feature engine to select the most relevant feature for my prediction. Afterwards, I will attempt to manipulate the data. Finally, I will choose a Machine Learning model and deploy it on the dataset.&lt;/p&gt;
&lt;h4 id=&#34;first-things-first&#34;&gt;First Thing&amp;rsquo;s First&lt;/h4&gt;
&lt;p&gt;We need to import the necessary libaries and load the datasets into a Dataframe. We will be using the following python libraries.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/981b97e4aa135dad1c59d6e54670e2cb.js&#34;&gt;&lt;/script&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pandas&lt;/strong&gt;: Provides in-memory 2d table objectd referred to as &amp;lsquo;Dataframes.&amp;rsquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Numpy&lt;/strong&gt;: Allows use to conduct fast mathematical computation on arrays and matrices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matplotlib&lt;/strong&gt;: A Python SD plotting library.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seaborn&lt;/strong&gt;: A data visualization library based on matplotlib.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scikit-Learn&lt;/strong&gt;: A machine learning library.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ensure the information is imported by calling  &lt;code&gt;world_cup.head()&lt;/code&gt; and &lt;code&gt;results.head()&lt;/code&gt; for both datasets. The Dataframes should look something like the following:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/world_cup.png&#34; alt=&#34;Word Cup Dataframe&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;exploratory-analysis&#34;&gt;&lt;strong&gt;Exploratory Analysis&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Exploratory analysis and feature engineering is the most time consuming part of any Data Science project.&lt;/p&gt;
&lt;p&gt;After analyzing both datasets, the resulting dataset has information on previous matches. The new (resulting) dataset will be useful for analyzing and predicting future mathces.&lt;/p&gt;
&lt;p&gt;We begin our exporatory anaylsis by added goal differentials and match outcomes to the results Dataframe.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/a2d6b86482784fbf5aa728c9d6ffeddb.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The outcome of the new Dataframe has the new results with the goal differential visible.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/new_results.png&#34; alt=&#34;results&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Now I&amp;rsquo;ll work on a subset of the data; one that includes games played by a participating team. I wanted to analyze the US Mens National Team, but unforunately, they were &lt;a href=&#34;https://www.mlssoccer.com/post/2017/10/10/us-national-team-eliminated-2018-fifa-world-cup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eliminated&lt;/a&gt; by Trinidad &amp;amp; Tobago in the qualifying rounds.&lt;/p&gt;
&lt;p&gt;The Danish Mens National Team (NKVD) was also &lt;a href=&#34;https://www.standard.co.uk/sport/football/worldcup/netherlands-miss-world-cup-2018-holland-fail-to-qualify-a3865221.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eliminated&lt;/a&gt; by The Swedish Mens National Team. Because of this, I have chosen to analyze the German Mens National Team.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/20fd329d0c63f397daee59a5134ac96b.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Analyzing the subset of the data will help us determine which features are more relevant for our analysis.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/german_matches.png&#34; alt=&#34;german matches&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;As you can see, the dataset has matches dating back as early as 1908! However, we&amp;rsquo;re only interested in looking at matches from the World Cup era, which started in 1930. We then create a column for year and pick all the games played 1930.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/e6b5d83c3426a37568393d43cc2f586a.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;We can now visualize the most common match outcome for Germany throughout the years.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/wins_losses.jpg&#34; alt=&#34;wins and losses&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;FIFA has recorded 863 matches from the German Mens National Team since 1930, of which Germany has won over 500 matches with ~175 losses. Slightly more matches has ended in a draw. Getting the win-rate for every country is a useful metric. We could use it to predict the most likely outcome of each match in the tournament.&lt;/p&gt;
&lt;h4 id=&#34;analyzing-teams-participating-in-the-world-cup&#34;&gt;Analyzing Teams Participating in the World Cup&lt;/h4&gt;
&lt;p&gt;We can start off by creating a list of teams participating in the 2018 World Cup. The following teams participating are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Group A&lt;/strong&gt;: Uruguay, Russia, Saudi Arabia, Egypt&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group B&lt;/strong&gt;: Spain, Portugal, Iran, Morocco&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group C&lt;/strong&gt;: France, Denmark, Peru, Australia&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group D&lt;/strong&gt;: Croatia, Argentina, Nigeria, Iceland&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group E&lt;/strong&gt;: Brazil, Switzerland, Serbia, Costa Rica&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group F&lt;/strong&gt;: Sweden, Mexico, South Korea, Germany&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group G&lt;/strong&gt;: Belgium, England, Tunisia, Panama&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group H&lt;/strong&gt;: Columbia, Japan, Senegal, Poland&lt;/li&gt;
&lt;/ul&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/48789b295fb144c5d0d68db0c2b6d9aa.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The following code will show only a dataframe with team wordl cup results starting from 1930, while dropping the duplicates (dropping games before 1930). We will also drop the date, home_score, away_score, tournament, city, country, goal_difference, and match_year columns from the &lt;code&gt;df_teams&lt;/code&gt; dataframe and assign it to a variable named &lt;code&gt;df_teams_1930&lt;/code&gt;. This will help us create a prediction label to simplify and process our model.&lt;/p&gt;
&lt;p&gt;For our prediction labels, we need to create a system to help our algorithm to determine a &amp;ldquo;winning&amp;rdquo; outcome versus a &amp;ldquo;losing&amp;rdquo; outcome. During group play, FIFA rewards 2 points to the winner of the match; 0 points to the loser of the match; and 1 point to both teams if the match results in a draw. We will adopt the same system to our labels dataset.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/05fd41e7891cab7bba73cd7b945ed825.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;From there, the home_team and away_team columns will be converted from categorical variables to continuous inputs, by setting dummy variables using Pandas &lt;code&gt;get_dummies()&lt;/code&gt; function. It replaces categorical columns with quantitative representations, which enable them to be utilized in the Scikit-learn model.&lt;/p&gt;
&lt;p&gt;We we separate the labels and features, the &lt;code&gt;train_test_split&lt;/code&gt; function will split the data into 70 percent training and 30 percetn testing.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/8e57023fabfac0f6cf8c119ad4829f01.js&#34;&gt;&lt;/script&gt;

&lt;h4 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h4&gt;
&lt;p&gt;The Scikit-learn package allows us to use machine learning packages such as Linear Regression. The only problem is that we can&amp;rsquo;t use linear regression on a categorical dependent variable. Instead, in such situations, we should try using algorithms such as Logistic Regression.&lt;/p&gt;
&lt;p&gt;Logistic regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 for success or 0 for failure. In other words, the logistic regression model predicts P(Y=1) as a function of X.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/logistic_regression.jpg&#34; alt=&#34;logistic regression&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Logistic regression is one of the most popular ways to fit models for categorical data, especially for binary response data in Data Modeling. It is the most important (and probably the most used) member of a class of models called generalized linear models. Unlike linear regression, logistic regression can directly predict probabilities (values that are restricted to the (0,1) interval).&lt;/p&gt;
&lt;p&gt;Logistic Regression is used when the dependent variable (target) is categorical. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To predict whether an email is spam (1) or not spam (0)&lt;/li&gt;
&lt;li&gt;Whether a company is a bankrupt (1) or not bankrupt (0)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Logistic regression is generally used where the dependent variable is Binary or Dichotomous. That means the dependent variable can take only two possible values, such as &amp;ldquo;Yes&amp;rdquo; or &amp;ldquo;No,&amp;rdquo; &amp;ldquo;Default&amp;rdquo; or &amp;ldquo;No Default,&amp;rdquo; &amp;ldquo;Living&amp;rdquo; or &amp;ldquo;Dead,&amp;rdquo; etc. Independent factors or variables can be categorical or numerical variables.&lt;/p&gt;
&lt;p&gt;In our case, &lt;em&gt;logistic regression&lt;/em&gt; attempts to predict an outcome (a win or loss) given a set of data points (stats) that likely influence that outcome. The way this works in practice is you feed the algorithm one game at a time, with both the aforementioned &amp;ldquo;set of data&amp;rdquo; and the actual outcome of the match. The model then learns how each piece of data you feed it influences the result of the game positively, negatively, and to what extent.&lt;/p&gt;
&lt;h2 id=&#34;machine-learning-match-prediction&#34;&gt;&lt;strong&gt;Machine Learning: Match Prediction&lt;/strong&gt;&lt;/h2&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/cdbf44fd9204923d62050f3a95ec11f4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Passing our features and labels into our algorithm, we recieved a training set accuracy of 0.571 and a testing set accuracy of 0.564, which isn&amp;rsquo;t the greatest, but we will continue to use these features. At this point, we will create a dataframe that we will deploy out model.&lt;/p&gt;
&lt;p&gt;We will start by loading the FIFA ranking as of April 2018 data set and a dataset containing the fixtures of the group stages of the tournament obtained from &lt;a href=&#34;https://fixturedownload.com/results/fifa-world-cup-2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The team which is positioned higher on the FIFA Ranking will be considered the &amp;ldquo;favorite&amp;rdquo; of the match, and therefore, will be positioned under the &amp;ldquo;home_teams&amp;rdquo; column, since obviously the only home team in this world cup is Russia. We will then add teams to the new prediction dataset based on the ranking position of each team. The next step will be to create dummy variables and then deploy the machine learning model.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/Hedgology/80394a88c5131856cf2405809b9d5cc7.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;We can start with deploying the model to the group matches. The follow shows the match results for the Group of Death, which contains Germany, Mexico, Sweden and South Korea.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Germany and Mexico
&lt;br&gt;Winner: Germany
&lt;br&gt;Probability of Draw:  0.267
&lt;br&gt;Probability of Mexico winning:  0.147&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Sweden and Korea Republic
&lt;br&gt;Winner: Sweden
&lt;br&gt;Probability of Draw:  0.308
&lt;br&gt;Probability of Korea Republic winning:  0.170&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Mexico and Korea Republic
&lt;br&gt;Winner: Mexico
&lt;br&gt;Probability of Draw:  0.295
&lt;br&gt;Probability of Korea Republic winning:  0.218&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Germany and Sweden
&lt;br&gt;Winner: Germany
&lt;br&gt;Probability of Draw:  0.243
&lt;br&gt;Probability of Sweden winning:  0.189&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Mexico and Sweden
&lt;br&gt;Winner: Mexico
&lt;br&gt;Probability of Draw:  0.253
&lt;br&gt;Probability of Sweden winning:  0.317&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Germany and Korea Republic
&lt;br&gt;Winner: Germany
&lt;br&gt;Probability of Draw:  0.277
&lt;br&gt;Probability of Korea Republic winning:  0.119&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Based on this information, it looks like the former champions will be making it out of the Group of Death. Unless the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sports-related_curses&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;World Cup Champion&amp;rsquo;s&lt;/a&gt; curse has anything to say about it, prehaps Mexico and Sweden will both make it out of group stages.&lt;/p&gt;
&lt;h5 id=&#34;knockout-stage-group-of-16&#34;&gt;Knockout Stage (Group of 16)&lt;/h5&gt;
&lt;p&gt;Based on the model, the following matchups are generated.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:4321/post/images/knockout_stage.png&#34; alt=&#34;knock out stage&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Our model has generated the following winners from the knockout stage of the 2018 World Cup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Portugal: 44.1% Probability of Winning&lt;/li&gt;
&lt;li&gt;France: 47.6% Probability of Winning&lt;/li&gt;
&lt;li&gt;Brazil: 70.2% Probability of Winning&lt;/li&gt;
&lt;li&gt;England: 51.3% Probability of Winning&lt;/li&gt;
&lt;li&gt;Spain: 52% Probability of Winning&lt;/li&gt;
&lt;li&gt;Germany: 66.8% Probability of Winning&lt;/li&gt;
&lt;li&gt;Belgium: 50.3% Probability of Winning&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;quarter-finals&#34;&gt;Quarter-Finals&lt;/h5&gt;
&lt;p&gt;Our model predicted a quarter final matches of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Portugal vs. France&lt;/li&gt;
&lt;li&gt;Spain vs. Argentina&lt;/li&gt;
&lt;li&gt;Brazil vs. England&lt;/li&gt;
&lt;li&gt;Germany vs. Belgium&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on the matchups, our algorithm has predicted the winners of their quarter final matches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;France: 42.9% Probabillity of Winning&lt;/li&gt;
&lt;li&gt;Argentina: 52% Probabillity of Winning&lt;/li&gt;
&lt;li&gt;Brazil: 52.5% Probabillity of Winning&lt;/li&gt;
&lt;li&gt;Germany: 58% Probabillity of Winning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This time, the algorithm has less than a 60% certainity for most of our match-ups. This is what we can expect as we reach a higher level of play from our competitors.&lt;/p&gt;
&lt;h5 id=&#34;semi-finals&#34;&gt;Semi-Finals&lt;/h5&gt;
&lt;p&gt;The model predicted the following semi-final matches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Brazil vs. France&lt;/li&gt;
&lt;li&gt;Germany vs. Argentina&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Germany and Argentina are once again matched up with one another, except this time it is in the semi-finals instead of the grand finals of the 2014 world cup. Based on the matchups, our algorithm has predicted the winners of their semi-final matches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;France: 69.3% Probabillity of Winning&lt;/li&gt;
&lt;li&gt;Germany: 52% Probabillity of Winning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our algorithm once again has Germany emerging has the victory of the Germany vs. Argentina match. Our model also estimates that Brazil will end up losing to France. Unforunately, the 5 time World Cup champs will have to miss out on another Final.&lt;/p&gt;
&lt;h5 id=&#34;finals&#34;&gt;Finals&lt;/h5&gt;
&lt;p&gt;Finally, we have the grand finals with France vs. Germany. What outcome does our model give us?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Germany: 52.6% Probabillity of Winning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;According to the model, Germany is likely to win the World Cup, although, it isn&amp;rsquo;t possible for repeat champions to emerge. The last time this occur was during the 1962 in Chile.&lt;/p&gt;
&lt;h2 id=&#34;areas-of-further-researchimprovement&#34;&gt;&lt;strong&gt;Areas of Further Research/Improvement&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Some areas for improvement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For improvements of the datasets, we could use FIFA (the game, not the organization), to assess the quality of each team player&lt;/li&gt;
&lt;li&gt;A confusion maxtrix would be great to analyze&lt;/li&gt;
&lt;li&gt;Using more models together to improve accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;There is so much that can be improved upon here. For now, lets see if we get any lucky.&lt;/p&gt;
&lt;p&gt;The full code can be found &lt;a href=&#34;https://github.com/Hedgology/FIFA-2018-WORLDCUP-PREDICTIONS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
