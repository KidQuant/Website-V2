
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Andre is a New York City based researcher and financier. His research interest includes investments, mobile computing, programming, sports and film. Andre does research in Economics and Finance, particularly in the Fixed Income Markets. His current project is ‘Mind The Gap: Alternative Trading Strategies for Retail Investors’.\n","date":1714348800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1714348800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Andre is a New York City based researcher and financier. His research interest includes investments, mobile computing, programming, sports and film. Andre does research in Economics and Finance, particularly in the Fixed Income Markets.","tags":null,"title":"Andre Sealy","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Hugo Blox Builder’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"http://localhost:4321/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Hugo Blox Builder's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Andre Sealy"],"categories":["Finance","Programming"],"content":"I’ve previously talked about Modern Portfolio Theory (MPT) and diversification in the past, the assumptions underlying MPT, and the construction of an Portfolio Frontier (PF). When most people think of diversifying their portfolio, they think of limiting their downside risk. However, it is possible construct a portfolio that still maximizes return with the least amount of volatility (choosing a portfolio that lies somewhere along the PF).\nNow I’m going to further expand upon why diversification helps to maximize risk-adjusted returns, and express the limits of diversification.\nWhat Makes a Portfolio? We can define a portfolio as a combination of $N$ assets with $N$ portfolio weights that sum to unity:\n$$\\bold{w}=[w_1,\\ldots,w_N],\\quad\\sum^{N}_{n=1}w_n=1.$$\nThe weight, $w_N$, represents the proportion of the $n$th asset in the portfolio. If $M_n$ and $P_n$ are the number and price of the $n$th asset, then $w_n$ is simply the total value of the $n$th asset normalized by the value of the portfolio:\n$$w_n=\\frac{M_nP_n}{M_1P_1+\\ldots+M_nP_N}$$\nWeights can be negative, such we could short sell an asset (profiting from asset price going down). Also, weights could be greater than unity, meaning that we are using leverage. We won’t get into these complicated scenarios and use only the basic assumptions in the summary of our portfolio.\nImagine, for example, that we had an investment account of $\\mathrm{$}10,000$ with $40$ shares of stock $A$ at $\\mathrm{$}150$ per share, $50$ shares of stock $B$ at $\\mathrm{$}20$ per share, and $25$ shares of stock $C$ at $\\mathrm{$}120$ per share. Then our portfolio with weights would be\nAsset Shares Price per share Investments Weight $A$ 40 150 6000 0.6 $B$ 50 20 1000 0.1 $C$ 25 120 3000 0.3 However, the weights need not be just the proportion of a given stock or asset. For example, suppose we were trading on margin, with $\\mathrm{$}8,000$ in our account to support our $\\mathrm{$}10,000$ investment. If we withdrew $\\mathrm{$}2,000$ from our investment account, then our portfolio in dollars would be unchanged, but our portfolio weights would have changed:\nAsset Shares Price per share Investments Weight $A$ 40 150 6000 0.6 $B$ 50 20 1000 0.1 $C$ 25 120 3000 0.3 Margin -2000 The weights change because the normalizer changes from $\\mathrm{$}10,000$ to $\\mathrm{$}8,0000$.\nDefining risk and reward Now that we have defined what a portfolio is, we can outline what we want to do. We can construct a portfolio that maximizes reward and minimizes risk, where “reward” is defined as overal portfolio return and “risk” is defined as the volatility ($\\sigma$ or $\\sigma^2$) of that return.\nDiversification with uncorrelated assets We can derive the mean-variance analysis by calculating the mean (expected return) and the variance on that portfolio. Let $R_n$ denote the return on the $n$th asset in a portfolio. By definition, its mean and variance are\n$$ \\begin{aligned} \\mathbb{E}[\\mathbb{R_n}]\\overset{\\Delta}{=}\u0026amp;\\mu_n,\\ \\mathbb{V}[R_n]=\u0026amp;\\mathbb{E}[(R_n-\\mu_n)^2]\\overset{\\Delta}{=}\\sigma^2_{n}.\\ \\end{aligned} $$\nNow let $R_p$ denote the return on the entire portfolio, this is the quantity we’re interested in. By the linearity of expectation, we have\n$$ \\begin{aligned} R_p\\overset{\\Delta}{=}\u0026amp;w_1R_1+\\dots+w_NR_N,\\ \\Downarrow \u0026amp; \\ \\mathbb{E}[R_p]=\u0026amp;\\mathbb{E}[w_1R_1+\\dots+w_NR_N]\\ =\u0026amp;w_1\\mathbb{E}[R_1]+\\dots+w_N\\mathbb{E}[R_N]\\ =\u0026amp;w_1\\mu_1+\\dots+w_N\\mu_N\\ \\overset{\\Delta}{=}\u0026amp;\\mu_p \\end{aligned} $$\nThe first line of this equation is just an accounting identity. It’s how we would calculate the return on our portfolio given weights $\\bold{w}$ and returns $R_1,\\ldots,R_N$. The variance of our portfolio’s return is\n$$ \\begin{aligned} \\mathbb{V}[R_p]\u0026amp;=\\mathbb{E}[(R_p-\\mu_p)^2]\\ \u0026amp;=\\mathbb{E}\\Big[\\big((w_1R_1+\\dots+w_NR_N)-(w_1\\mu_1+\\dots+w_N\\mu_N)\\big)^2\\Big]\\ \u0026amp;=\\mathbb{E}\\Big[\\big((w_1(R_1-\\mu_1)+\\dots+w_n(R_N-\\mu_N)\\big)^2\\Big]\\ \u0026amp;\\overset{\\Delta}{=}\\sigma^2_p. \\end{aligned} $$\nIf we have $N$ assets in our portfolio, and we square the term in the last line in this equation, we get $N^2$ terms inside this expectation. We can write the variance for a single combination $R_n$ and $R_m$ as:\n$$ \\begin{aligned} \\mathbb{E}[w_nw_m(R_n-\\mu_n)(R_m-\\mu_m)]\u0026amp;=w_nw_m\\mathbb{E}[(R_n-\\mu)(R_m-\\mu)]\\ \u0026amp;=w_nw_m\\text{Cov}[R_n,R_m]\\ \u0026amp;=w_nw_m\\sigma_{nm}\\ \u0026amp;=w_nw_m\\sigma_n\\sigma_m\\rho_{nm}, \\end{aligned} $$\nwhere $\\sigma_{nm}$ and $\\rho_{nm}$ are the covariance and correlation between the $n$th and $m$th assets respectively. This provides some basic definitions from probability. Recard that\n$$ \\rho_{mn}=\\frac{\\text{Cov}[R_n, R_m]}{\\sigma_n\\sigma_m}. $$\nNow here’s the main point: The variance of our portfolio is a function of the covariances between the assets in the portfolio. We can represent this compactly using a covariance matrix:\n$$ \\begin{bmatrix} w_1^2\\sigma_1^2 \u0026amp; \\dots \u0026amp;w_1w_N\\sigma_{1N}\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ w_Nw_1\\sigma_{N1} \u0026amp; \\dots \u0026amp; w^2_N\\sigma^2_N\\ \\end{bmatrix}= $$\nNotice that there are $N$ variance terms (the diagonal …","date":1714348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714348800,"objectID":"d7fe147f7614424b2ee3a05439794d89","permalink":"http://localhost:4321/post/2024-28-2014-does-diversification-always-matter/","publishdate":"2024-04-29T00:00:00Z","relpermalink":"/post/2024-28-2014-does-diversification-always-matter/","section":"post","summary":"They say \"never put your eggs in one basket\" when it comes to investing. This intutition is grounded in the mathematics of modern portfolio theory.","tags":["Quant Finance"],"title":"Does Diversification Always Matter?","type":"post"},{"authors":["Andre Sealy"],"categories":["Finance","Programming"],"content":"Donald Trump’s knock-off Twitter has officially become public for under a week, and the entire country didn’t take long to create an investment thesis about it. Just in case you fell asleep, Trump got himself banned from Twitter for… reasons [1].\nTo combat the bias and censorship that was prevalent at the time, Trump decided to launch his social media network called Truth Social, which was created by T Media Tech LLC in 2021, owned by Trump Media \u0026amp; Technology Group Corp. Since then, the company was acquired via SPAC (Special Purpose Acquisition Vehicle) by Digital World Acquisition Corp.\nYou can find the stock on the NASDAQ exchange under the ticker DTJ, which I’m sure is just a coincidence. It’s currently trading at $~49$ per share, down from it’s peak of $~66$ per share. Trump is currently the majority shareholder, owning $~57%$ of the available shares.\nSo it begs the question: Is DJT, a company owned by the former President of the United States, a meme stock?\nProbably.\nAt least according to the analysis I conducted three years ago is concerned. Recall that I’ve created an algorithm that tracks the most frequently mentioned assets on the infamous subreddit: Wall Street Bets [2]. Three years ago, owning shares of AMC, GME, BB, and some crypto was cool. Today, owning shares of NVDA, AAPL, SPY, RDDT (obviously), and DJT is cool, so it’s a mixed bag for the overall “memeness.” Some of the assets that generate a lot of buzz on the subreddit are definitely at risk. On the other hand, some of the assets are more established, less volatile, positive cash flow generating assets.\nSo what does this all mean? What is going on? Did the “Retards” over at WSB learn from their diamond hands fiasco and decide to adopt a more mature investment philosophy? Or they chose to do what all retail traders do: talk about exciting companies.\nOne thing is for sure: We don’t need WSB to figure out whether DJT is a sound investment because DJT—like everything related to Donald John Trump—stems, and will forever stem, from whether or not you hate Donald Trump. It’s impossible to have a neutral opinion about him, even things about him that are objectively true. (such as, he lost the election; handled it poorly AND people were consistently trying to sabotage his administration)\nIf you love Trump, DJT is the greatest thing ever, it’s going to the moon, and everyone loves the stock.\nIf you hate Trump, DJT is a clear scam; a pump and dump (it’s not a pump and dump) and it’s going to zero, just like his other businesses.\nLike everything, the truth is probably close to the middle but definitely much closer to zero than the moon. Right now, DJT is receiving a wave of bad press, mostly related to lackluster earnings performance and the uncertainty of the firm’s future viability. As expected, the stock took a hit based on that news.\nMost outsiders typically don’t understand that every asset class has a behavioral aspect (the stock skyrocketed upon news of the successful merger; it tanked upon news of poor performance). If what we are experiencing with DJT is indeed behavioral, then this period of volatility will go away. However, as it stands right now, the market expects the stock to go lower simply because it generates huge losses from its operating activities, and there is no evidence of a viable user base on the platform.\nAgain, I’m not saying that the stock is worthless, BUT it’s also not worth $7 billion dollars.\nWill I Analyze DJT Similiar To Other Meme Stocks? Other meme stocks, such as GME and AMC, were different from DJT because these companies are more established. GME and AMC have been around for over two decades, while DJT has just arrived on the scene. Also, DJT is a SPAC, so the asset inherently has a negative connotation attached to it. We would need more data about the company to determine whether it can deliver excess returns to investors. So, I won’t be conducting any research relating to DJT, and even if I wanted to, it would primarily be conducted utilizing quantitative methods.\nSince I’ve already stated that it’s fair value is closer to zero than to the moon, does this imply that I know it’s fair value? Not exactly. Anyone who has visited my page can probably guess that I subscribe to Efficient Market Hypothesis, which is an investment theory that states all relevant information (maybe past and present) is reflected in the current share price. Also, since stock prices fluctuate randomly, their predictability is no better than flipping a coin. As such, it’s almost impossible to consistently outperform the markets in the long run.\nSo, just because DJT was up for three consecutive days last week doesn’t mean it’ll be up on the fourth day; it also doesn’t mean it will be down on the fourth day, either. Even though there is a solid upward bias, it’s essential to understand that asset prices are random, and it’s necessary to build a framework that can capture said randomness (quant finance, for starters). Don’t bet on it going to …","date":1712016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712016000,"objectID":"729eca65c467fc35daa0604b388ece24","permalink":"http://localhost:4321/post/2024-04-02-djt-meme-stock/","publishdate":"2024-04-02T00:00:00Z","relpermalink":"/post/2024-04-02-djt-meme-stock/","section":"post","summary":"Probably.","tags":["Trading","Quant Finance","Trump"],"title":"Is DJT a Meme Stock?","type":"post"},{"authors":["Andre Sealy"],"categories":["Programming","Data Science"],"content":"I tried analyzing cycling accidents around New York City, but it was difficult to draw meaningful conclusions from the data. For example, men die more frequently than women while biking, according to a report on Bicyclist Fatalities and Serious Injuries in New York City.\nHowever, interpreting this data is tricky. For example, do more men die because more men bike or is it because they engage in riskier behavior? I had a different issue with analyzing accident locations (even though the data has geogrpahical location, longitude and latitude). Do more accidents happen at intersections because intersections are more dangerous or because they are easier locations to record? These kinds of questions are subtle and require careful data collection and analysis to answer.\nFurthermore, I found it hard to see how any conclusions would be actionable. I’m not going to start leaving work at 3 PM, even if 5 PM is a truly riskier time to bike.\nSo what can I do? In my mind, the simplest framing is that biking is dangerous because I am sharing the road with cars. To justify this claim, consider these data from DOT’s Bicycle Crash Data Reports. They point a clear picture about the risk of accidents with and without cars. The simplest thing I can do is to plan my route to maximize time in bike lanes, ideally protected bike lanes. And I should be extra careful in moments where I interact with cars.\nEstimating The Risk Now let’s calculate my risk. Let $X$ be a random variable denoting my lifetime. Let’s assume that my only risk of death is via my commute and that dying occurs with probability $p$, where $p$ is the estimated KSI over my lifetime by biking:\n$$ p=\\frac{30}{10,000,000} $$\nAn assumption here is that my risk of a severe injury or death on each ride is independent of all other rides. Often, independence assumptions are obviously false simplifications, but here I think it is reasonable. Without a concerted effort on my part to learn to bike more safely, I suspect that each ride carries roughly the same risk.\nThe $X$ is a geometric random variable,\n$$ X\\sim\\text{geom}(p), $$\nand the probability that I do not make $x$ rides without severe injury or death is given by the cumulative distribution function,\n$$ \\mathbb{P}(X\\le 8000) = 1-\\bigg(1-\\frac{30}{10,000,000}\\bigg)^{8000}\\approx 2.4%. $$\nAs I mentioned, this is conservative because $p$ and $x$ are both high in my mind. I doubt that I’ll bike to work this consistently over twenty years, and I also think (hope?) that NYC will become safer over time, that $p$ will actually decrease, driving biking infrastructure improvements.\nNote that while I’ve framed the calculation in terms of number of years commuting, the units of 8000 is simply “rides”. Thus, this risk is independent of time. If I decided to do 8000 rides in a single year, then I’d have a $2.4%$ chance of dying of biking in that year, according to these assumptions.\nA Sanity Check As a sanity check, let’s find a value $p$ from the literature, and compare it to city’s data. I’d expect the probability of dying by cycling averaged over time and space to be less than the probability of dying by cycling in NYC.\nAccording to research, cycling twenty-eight miles has one micromort of risk. My commute\n","date":1711843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711843200,"objectID":"f265c72e6df1cd60fcdbff17bde385b1","permalink":"http://localhost:4321/post/2024-03-31-biking-accidents/","publishdate":"2024-03-31T00:00:00Z","relpermalink":"/post/2024-03-31-biking-accidents/","section":"post","summary":"I tried analyzing cycling accidents around New York City, but it was difficult to draw meaningful conclusions from the data. For example, men die more frequently than women while biking, according to a report on Bicyclist Fatalities and Serious Injuries in New York City.","tags":["New York City","Biking"],"title":"How Dangerous Is Biking in New York City","type":"post"},{"authors":["Andre Sealy"],"categories":["Finance","Programming"],"content":"I’ve recently been tasked with working more with more fixed-income derivatives, so I’m writing this more as a refresher for myself, but I’ve always guaged how well I know something based on how well I’m able to teach others.\nRecall that the Black-Scholes price of a European-style call option is\n$$ \\begin{equation} \\begin{aligned} C \u0026amp; = S\\Phi(d_1)-Ke^{-rT}\\Phi(d_2),\\ d_1 \u0026amp; = \\frac{1}{\\sigma\\sqrt{T}}[log(S/K)+[r+(1/2)\\sigma^2]T],\\ d_2 \u0026amp; = d_1-\\sigma\\sqrt{T}, \\end{aligned} \\end{equation} $$\nwhere $\\Phi(x)$ is the cumulative distribution function (CDF) of the standard normal distribution:\n$$ \\begin{equation} \\begin{aligned} \\Phi(x):=\\frac{1}{\\sqrt{\\pi}}\\int^{x}_{-\\infty}e^{-z^2/2}dz \\end{aligned} \\end{equation} $$\nSince $\\Phi(x)$ is the CDF of the standard normal distribution, its derivation $\\Phi^\\prime(x)$ is the probability density function(PDF) of the standard normal distribution. I’ll denote this with $\\phi(x)$ or\n$$ \\begin{equation} \\phi(x):=\\Phi^{\\prime}(x). \\end{equation} $$\nDelta ($\\Delta$) captures the sensitivity, or more precisely the instanteneous rate of change, of the option price to the spot price. The Black-Scholes delta for a call option is\n$$ \\begin{equation} \\Delta(C)=\\frac{\\partial C}{\\partial S}=\\Phi(d_1) \\end{equation} $$\nwhere $\\Phi(x)$ and $d_1$ are defined in Equations 1 and 2.\nTo understand delta, let’s start with an example. Suppose we have a stock with a price of $S=100$, a call option with a price of $C=10$, and a delta between the two of 0.7. (The prices do not matter here but will be in later examples.) Now imagine that we sell calls for $3000$ shares of this stock and want to hedge our resulting short position. We can use delta to compute the appropriate size of our hedge. We should put $0.7\\times 3000=2100$ shares of the underlying stock. Why? This is just a direct application of Equation 4.\nFirst, let’s replace the differentials (e.g., $\\partial C$) with small moves in the asset (e.g., $dC$). This is a fine thing to do because Equation 8 says that $\\Delta$ is the linear appropriate of how the Black-Scholes prices of our calls will change. We can write this as\n$$ \\begin{equation} \\begin{aligned} dC=\\Delta\\times dS. \\end{aligned} \\end{equation} $$\nNow let $w$ denote the number of shares of the stock assoicated with the calls we sold (here $w=3000$). Then by equation 5, we have\n$$ \\begin{equation} dC\\times w = \\Delta \\times dS \\times w \\end{equation} $$\nOur proposed hedge above is $\\Delta \\times w = 2100$, and this works because if the stock price changes by $dS=1$, then the call moves by $dC=0.7$, and we will make $2,100$ on our hedge and lose $0.7\\times 3000=21000$ on our calls. In practice, this calculation is slightly more tedious because options are typically sold with a contract multiplier indicating the number of shares per contract, but this is just a matter of bookkeeping.\nThus, while the options we have sold have a delta of $0.7$, our actual portfolio position has a zero delta. We say that our portfolio is delta neutral, which means that if the spot changes a little, the value of our options position does not change significantly.\nDollar Delta In the previous example, the raw values of the stock and call did not matter, and we had no sense of our total notional exposure. Thus, in practice, mathematical delta (Equation 4) is often converted to dollar delta, sometimes called notional delta, because this quantity tells us how much notional exposure we have to the underlying. A dollar delta is just delta times the spot price or\n$$ \\begin{equation} $\\Delta=\\Delta S \\end{equation} $$\nFor example, our notional exposure in the previous example was the number of shares times dollar delta or\n$$ \\begin{equation} \\begin{aligned} $\\Delta\\times w \u0026amp; = \\Delta\\times S\\times w \\ \u0026amp; = 0.7\\times$ 100\\times 3000 \\ \u0026amp; = $ 210,000. \\ \\end{aligned} \\end{equation} $$\nTo hedge, we should buy $210,000$ of stock, which is obviously the same as buying $2,100$ shares of stock at $S=100$.\nOne benefit of considering a notional amount is that we can multiply the dollar delta by a percent change in the stock. In contrast, since delta represents a dollar change in stock move, per dollar change in option move, we cannot directly multiply mathematical delta by a relative move such as a return or percent return. To see this, consider the returns in our running example. Let $S_1$ denote the stock price after moving $dS$. Then, the return on $S$ is\n$$ \\begin{equation} R_S=\\frac{s_1-S}{S}=\\frac{101-100}{100}=0.01 \\end{equation} $$\nwhile the return on C to a new price $C_1$ is\n$$ \\begin{equation} R_C=\\frac{C_1-C}{C}=\\frac{\\Delta dS}{C}=\\frac{0.7}{10}-0.07 \\end{equation} $$\nAnd clearly\n$$ \\begin{equation} \\Delta R_S=0.7\\times 0.01=0.007\\neq 0.07=R_C \\end{equation} $$\nSo, multiplying the mathematical delta by a relative move is nonsensical. Given Equation 8, we must operate on it in a way that makes sense.\nHowever, we can multiply dollar delta by a relative move since\n$$ \\begin{equation} \\begin{aligned} …","date":1679961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679961600,"objectID":"a7b168ea42b9f938c9a0c315c24ec6fd","permalink":"http://localhost:4321/post/2023-03-28-revisiting-delta/","publishdate":"2023-03-28T00:00:00Z","relpermalink":"/post/2023-03-28-revisiting-delta/","section":"post","summary":"I’ve recently been tasked with working more with more fixed-income derivatives, so I’m writing this more as a refresher for myself, but I’ve always guaged how well I know something based on how well I’m able to teach others.","tags":["Trading","Quant Finance"],"title":"Revisitng Delta","type":"post"},{"authors":["Andre Sealy"],"categories":null,"content":"Thanks to the prevalence of COVID-19 in our everyday lives, it’s getting increasingly difficult to return to normal for most people. While I’ve used this additional flexibility to pick up on old hobbies (gaming, music, etc.), others have used theirs to learn about financial markets. Knowing that I work in finance, some of my friends have reached out to me for financial pointers, while others have opted for the convenience of reading r/WallStreetBets.\nYou may be asking, why am I – someone who would be considered a “sophiscated investor” – would even be interested in a platform such as r/WallStreetBets? For those of you who don’t know, I’ve written a piece about the subreddit last year. However, it was never clearly explained the buzz around r/WallStreetBets.\nDue to the pandemic (the financial insecurity and flexibility it brought to millions of people), as well as the stimulus checks provided by the government and the rise of free trading platforms such as Robinhood, a lot of people who would typically not dabble with stocks are having fun with the stock market. They’re also investing in all sorts of zany things like Dogecoin and GameStop. Institutional Investors (the “smart money”) and the veterans in the financial media fail to understand this, and they’re generally condescending and negative towards these new brand of retail investors.\nThey call them idiots for taking risks in cryptocurrencies; they call them fools for believing companies in dying industries with falling revenues are great investments [2]. From this, the term “Dumb Money” was used to describe this new breed of investors; “DOGE/GME to the moon,” they frequently chant, much to the disdain and confusion of legacy investors and their friends in the legacy media [3].\nRecognizing the condescension, these retail investors decide to take their agency back using the self-described label known as Retards. Retards, if you don’t know, is a rearrangement of the word tradeRS. Since they’re not considered legitimate tradeRS in the eyes of the investment community, they’ll just call themselves Retards, which is an anagram designed to reclaim the agency taken from retail investments. Sure, they may be considered “Dumb Money,” but they’re going to make the investment decisions they want to without the influence and manipulation of institutional investors and their friends in the financial media.\nThis is largely the energy behind drama involving r/WallStreetBets and the rest of the investment community.\nAre r/WallStreetBets Stock Picks Even Any Good? It is generally assumed – rightly or wrongly – that if you have a background in finance, you know what you’re talking about. The barriers required to work within the industry seem to justify the claim. The most well-known front-end finance jobs require a bachelor’s degree at an accredited four-year university, along with passing, at minimum, the Securities Industry Essentials Exam (or SIE) and either a FINRA Series 3 or 7 Exam.\nJobs that are more analytically driven, such as Actuaries, may require candidates to have a statistical or mathematical background and pass several SOA (Society of Actuaries) Exams. While Quants (which is my domain) typically don’t require examinations; however, some positions do encourage and require candidates to have at minimum a Masters of Science in a STEM field.\nSo yes, it may be easy to see why people on Wall Street are considered “Smart Money.”\nHowever, this doesn’t mean you need education and fancy certificates to make good investments. Warren Buffett, one of the greatest investors alive, began to invest on his own when he was only 11-years-old. While the man would never even look at any of the companies r/WallStreetBets are investing in, he established a system that allowed him to make sound investments using the resources available at the time; namely, a book published by Benjamin Graham called The Intelligent Investor. Speaking from personal experience, I started learning about finance and economics on my own time before I enrolled in university to pursue it as a career.\nToday, the resources available to help retail investors are potentially endless. Most of what you’ll find on the internet is bunk; however, you can find invaluable information if you know where to look.\nThis project aims to see if the TradeRS at r/WallStreetBets know where to look. Are they seeing things we aren’t seeing or just larping as wall street speculators?\nWhat Are “Meme Stocks”? A meme stock is a stock that has seen an increase in volume not because of how well the company performs, but rather because of hype on social media and online forums. For this reason, these stocks often become overvalued, seeing drastic price increases in just a short amount of time.\nMany of these stocks have not performed well in recent years. Some of these stocks may exist in struggling retail or brick-\u0026amp;-mortor (GME) industries. Other stocks may have once been considered leaders in their respective industries but have …","date":1664928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664928000,"objectID":"69d122401c1b27e53efc69e0c129becf","permalink":"http://localhost:4321/project/wallstreet-bets-stock-analysis/","publishdate":"2022-10-05T00:00:00Z","relpermalink":"/project/wallstreet-bets-stock-analysis/","section":"project","summary":"No, but we can watch some interesting stock pitches and learn some valuable lessons","tags":["Finance","Statistics","Data Science"],"title":"Does a r/WallStreetBets Portfolio Significantly Outperform?","type":"project"},{"authors":["Andre Sealy"],"categories":["Finance","Programming"],"content":"One of the most bizarre David vs. Goliath scenarios in modern finance, by far, belongs to the retail investors from Reddit’s r/WallStreetBets versus Hedge Fund space. The story became such a fad that it garnered the attention of Treasury Secretary Janet Yellen and the SEC. With the democratization of finance, everyone can invest in markets once thought unobtainable to the common man.\nr/WallStreetBets vs. Wall Street Hedge Funds The mania revolves around the most shorted stocks, shorted by hedge funds that hoped to make a killing when those stocks collapse. For anyone unfamiliar with how short selling works, the art of short selling involves a hypothesis that an asset price will fall within a given time frame. The process consists of an investor borrowing a stock, selling the stock, and then buying the stock back to return to the lender (hopefully at a lower price than what the investor initially purchased).\nWhere do hedge funds fit into all of this? Well, a bunch of hedge funds decided to get into the business of shorting equities with the highest short-interest. (The number of shares that have been sold short but have not yet been covered or closed out) At one point, the short interest of GameStop shares was over 140% of the float. If institutional investors wanted to close out their positions, they would need to buy those shares. But who is going to sell them those shares?\nThe folks over at r/WallStreetBets have discovered that stocks with small float are the easiest to manipulate if enough people got together. They also figured out that stocks that were massively shorted and didn’t have many sellers left could be driven up to the point where investors would panic-buy to cover their short position. That panic buying would trigger a massive surge in GME’s price, which could wipe out those hated Hedge Funds.\nSince then, stocks such as GME, along with plenty of others, have been dubbed what we in the business call “Meme Stocks.” A meme stock is basically the equity belonging to any publicly traded company where the stock has increased in value, not because of the company’s performance fundamentals, but purely based on the attention the stock is receiving on social media (namely r/WallStreetBets). I’m not entirely sure which stocks are considered “memes” (there is actually an index which tracks these types of equities), but I know that this list would comprise of companies such as AMC Entertainment Holdings (AMC), Bed Bath and Beyond (BBBY), and Blackberry (BB) [1].\nNow that the mania has passed, GME sits at around $180 per share. I once remember a single day of trading, GME opened at 191; surged to a high of 255; crashed to a low of 173, and still managed to close at 188 per share. Totally normal behavior for a stock. Since then, the price has stabilized at around the 150-180 price range.\nWhat does this mean for GME? Does this imply that r/WallStreetBets have moved on to the next oversold stock?\nGetting the Information from Reddit I began by navigating to r/WallStreetBets. My original plan was to build this similiar to the news media webscraper algorithm for a data science project that I’ve done [2]. One of the issues with using that algorithm is being able to infinitely scrolling reddit and scraping the new information.\nRather than using pagination to reveal additional results, Reddits makes an async call to fetch additional posts as you scroll down the page. This means that when we make our requests, we’ll be able to scrape data from the ~20 or so posts but won’t access additional data by requesting the next page.\nHowever, there is a workaround that I’ve found to access the data I need: PRAW. PRAW stands for Python Reddit API Wrapper. PRAW is an api that allows you to easily read and write data to the website. To use it, you need to create the app credientials via Reddit and retrieve the client_id and client_secret.\nAlso, we’re going to need a list of all publicly traded US stocks and their ticker symbols, so the script will know which strings (or text) to match. The NASDAQ website allows you to download a list from the major US exchanges, such as the New York Stock Exchange, American Stock Exchange (I didn’t realize this was still a thing), and of course, NASDAQ [4].\nThe NASDAQ screener covers micro and nano-cap equities (the stock of companies with less than 300 million dollars in market capitalization); it covers the equities of companies of different regions; it even covers American Depository Receipts (or ADRs for short).\nScraping Reddit via PRAW Getting the information from via involves 3 easy steps:\nGetting content from r/WallStreetBets Analyzing word frequencies Inner joining word frequencies with stock tickers import pandas as pd import praw import re import requests reddit = praw.Reddit( client_id=\u0026#34;YOUR CLIENT ID\u0026#34;, client_secret=\u0026#34;YOUR CLIENT SECRET\u0026#34;, user_agent=\u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\u0026#34; ) PRAW is actually …","date":1622851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622851200,"objectID":"ebd19d254944ffd911dfa5aef2203eb8","permalink":"http://localhost:4321/post/2021-05-24-wsb-favorite-stock/","publishdate":"2021-06-05T00:00:00Z","relpermalink":"/post/2021-05-24-wsb-favorite-stock/","section":"post","summary":"One of the most bizarre David vs. Goliath scenarios in modern finance, by far, belongs to the retail investors from Reddit’s r/WallStreetBets versus Hedge Fund space. The story became such a fad that it garnered the attention of Treasury Secretary Janet Yellen and the SEC.","tags":["Trading","GameStop","Quant Finance"],"title":"r/WallStreetBets Most Talked About Stonks","type":"post"},{"authors":null,"categories":["R"],"content":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart.\n","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"84a876ba789bb7232be8d9ed2487fd98","permalink":"http://localhost:4321/post/2020-12-01-r-rmarkdown/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/post/2020-12-01-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Andre Sealy"],"categories":["Data Science","Economics"],"content":"On this blog, I’ve talked about the Mathematics of Pandemics. I’ve even talked about the Politics of Pandemics. I’m long overdue to discuss the “Economics of Pandemics;” however, theorizing the potential effects of an inadequate response was pretty pointless, as anyone who has lived through the Great Recession would tell you. After all, “ignorance on fire is sometimes better than knowledge on ice,” as the saying goes.\nRegardless of whether or not our economic response to COVID was sufficient, the discussion was always framed in a false dichotomy: protecting the vulnerable versus protecting the economy. If one is a pragmatic, normal-thinking individual, these two are not necessarily mutually exclusive. As early as March 27th, the IGM Chicago Forum surveyed more than 40 professional economists regarding the economic lockdowns in response to the COVID-19 crisis. The vast majority of respondents agree (88%) believe that a comprehensive policy response would involve tolerating a massive contraction in economic activity until the spread has dropped significantly (Question A) [3].\nSo the United States, along with other nations around the world, instituted their lockdowns policy. We initiated our 15 Days to Slow The Spread, which quickly became 30 Days to Slow The Spread. Although most states have very minor COVID restrictions (staying six feet apart, no gatherings greater than 20 people, etc.), other places that were more severely affected are imposing more strict regulations. [4] It just so happens that the places with the strictest COVID regulation are also home to the largest economies in the U.S.\nAlthough most states have partially reopened (except for California), the U.S. economy contracted by a whopping -32.9% in the second quarter.\nIf that sounds like a big number, that’s because it is a big number. The Bureau of Economic Analysis calculates GDP on an annualized basis; this facilitates comparison of growth rates over various periods. In other words, the -32.9% figure is what we would get if the economy sustained its growth (or, in this case, decline) for one year. When used to measure very short-term changes (such as locking down for one or two months), it can be misleading. As such, it’s probably better to use Year-over-year change, which would record a -9.5% decline.\nAn Invisible Threat Ever Lurking Regardless of how economic growth is measured, the catch-22 is unavoidable. COVID is so bad, it warrants locking down the economy to control its spread, but the economy is bad because we are locked down to prevent the spread of COVID. What a paradox…\nOf course, the risk of infection is very real; we can’t expect things to go back to “normal” if the risk of transmission is still high. Some would point out that the economic contraction was only severe because we failed to control the spread of COVID. The only problem with this theory is that the United States had the slowest contraction out of all the nations which successfully controlled the spread.\nThe virus ravaged Italy and Spain, and they have instituted strict lockdowns in return. Other places in Europe, which did not want to become the next Italy, instituted strict lockdowns. (Canada is included, for no other reason than it is our neighbor to the north) Some may argue that these nations made a significant trade-off; they chose to protect lives over the economy, and now the economy will be better off in the long-run.\nExcept now all of these places are relaxing their COVID restrictions and the virus is starting to spread again. Cases are starting to spike in France, while infections are slowly increasing in Germany.\nThe following places are experiencing an uptick in COVID-19 cases:\nSpain, Italy, United Kingdom, Belgium, Neterlands, Switzerland, Greece, Portugal, Denmark, Norway, Japan, Australia What do all of these places have in common? These regions were assumed to be in control of the virus [5]. Now the virus is out of control and these regions are either experiencing a large surge (Japan), or a steady increase in cases (United Kingdom). Whether by pure luck or by a fortunate circumstance, the United States experienced a phenomenon in early June that Europe should have learned in early July: you can’t hide from the virus forever.\nOf course, the lockdowns aren’t supposed to last forever; they just need to be maintained until a vaccine is found [6]. There’s no mention of when we can expect a COVID-19 anti-viral vaccine to be developed; “never” is a very realistic timeline for an effective vaccine (the flu vaccine has a 40-70% effectiveness). Regardless, it’s apparent that lockdown is not a sustainable strategy. For two reasons: 1) the obvious economic ramifications of keeping the economy locked down and 2) locking down just delays the inevitable.\nGranted, we don’t have guidelines on what it means to reopen safely, or knowledge if this is even possible because the world has never been locked down before. Lockdown is a new concept that has never been …","date":1598054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598054400,"objectID":"a9f88c68c7426ffa121a513cc595b145","permalink":"http://localhost:4321/post/2020-08-22-covid-vs-economy/","publishdate":"2020-08-22T00:00:00Z","relpermalink":"/post/2020-08-22-covid-vs-economy/","section":"post","summary":"On this blog, I’ve talked about the Mathematics of Pandemics. I’ve even talked about the Politics of Pandemics. I’m long overdue to discuss the “Economics of Pandemics;” however, theorizing the potential effects of an inadequate response was pretty pointless, as anyone who has lived through the Great Recession would tell you.","tags":["Coronavirus","GDP","Lockdowns"],"title":"COVID Vs. Economy","type":"post"},{"authors":["Andre Sealy"],"categories":["Data Science","Economics"],"content":"It doesn’t take a genius to know that 2020 has been a year for the United States economy. Tens of millions were encouraged or forced to remain indoors in an attempt to slow the spread. Only a few industries/businesses were allowed to stay open, as they were classified as “essential businesses.” The longest expansion in U.S. history came to an end in Q1 2020, which resulted in a -5.0% contraction in economic activity (followed by a -32.9% drop in Q2).\nWithin the same period, retail sales declined -0.87% YoY for Q1 2020, equating to a ~5 billion drop. During the second quarter, sales dropped a whopping 8.16% YoY, resulting in an approximately 12 billion decline in retail sales. (although the quarterly decline in sales is the largest since the Great Recession, the monthly decline is the largest in history)\nHowever, during the last two months, retail sales have recovered to pre-recession levels, and total sales are the highest.\nAs everyone across the country is adjusting to a life with COVID, it’s interesting to see which industries benefit the most, and which industries are the most impacted. The Census Bureau publishes data on Advanced Monthly Sales for Retail and Food Services; statistics are gathered utilizing a stratified random sampling method that tracks approximately 5,500 retail and food service firms. The data is then weighted and benchmarked to represent a complete universe of over three million retail and food service firms. The data goes as far back as 1992.\nWe saw a slight decline in February, which can be seen as a result of seasonality (a decline as a result of holiday shopping MoM). However, during the two months afterward, we’ve experienced some extreme declines in retail sales, most of which occurred in April. Most states haven’t instituted their lockdowns until the end of March or the beginning of April.\nRetail Sales Pre-Lockdown In February 2020, retail sales declined -0.44% MoM. The only industries which experienced growth were general merchandise, sporting goods/hobby stores, pharmacies/drug stores, fuel dealers, beer/liquor stores, electronic retail, and nonstore retailers. Nonstore retailers are essentially any business the primarily sells merchandise online, the industry with the largest monthly growth of 1.2%.\nThe most significant declines include gasoline stations (-2.95%), men’s clothing stores (-2.86%), automotive parts (-2.38%), general clothing stores (-1.8%), and building material/suppliers dealers (-1.63%). Out of 30 NAICS industries, just seven experienced sales growth for the entire month of February.\nRetail Sales Post-Lockdown In April 2020, retail sales declined -14.7%; just two industries are experiencing growth during the lockdown: electronic shopping/mail-order houses and nonstore retailers. To be expected, as non-essential businesses are forced to close down, and people are encouraged to work from home if they are able. Social distancing policies instituted by state and local governments fueled demand for products designed to help people work from home and keep in contact with loved ones, such as computers, webcams, desks, monitors, etc.\nOne would also assume that industries such as grocery and fast food would see some growth during this period, considering these are classified as essential businesses. However, grocery and fast food industries experienced a -13.37% and -34.32% decline, respectively. This decline could be attributed to safety concerns (unable to implement the necessary infrastructure to continue operating safely). It could be due to the decrease in mobility (many of the food and drinking places include restaurants and bars, which people can no longer patronize).\nWhatever the reason, it’s pretty clear that both of these industries took a subtaintial hit. There is a reason why the Paycheck Protection Program (PPP) allowed even large resturant chains to recieve loans, despite the program being designed specifically for small businesses.\nFor the most part, the United States is a service-oriented economy. Fewer people moving around generally implies fewer money-making opportunities for businesses, regardless of industry.\nRetail Sales After Reopening Although “reopening” is a somewhat subjective term at this point, we can say that relative to the initial lockdowns, most states have started to liberalize their economies. Like Florida, most states lifted its lockdown at the beginning of May, while others such as New York and New Jersey waited until the beginning of June. Some states, such as South Dakota, never instituted a lockdown at all. As such, we can spot the growth of the recovery at the exact movement states began to reopen.\nDuring May, Retail sales grew 18.27%, which shouldn’t be surprising to anyone if you shut down the economy for nearly two months. Also, every industry experienced sales growth, which also shouldn’t be surprising. If you’re at rock bottom, the only place left to go is up.\nThe top 5 industries with the most growth in May include shoe …","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597622400,"objectID":"635055bf141eb8da562661b01595fa81","permalink":"http://localhost:4321/post/2020-08-15-covid-impact-retail-sales/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/post/2020-08-15-covid-impact-retail-sales/","section":"post","summary":"It doesn’t take a genius to know that 2020 has been a year for the United States economy. Tens of millions were encouraged or forced to remain indoors in an attempt to slow the spread.","tags":["Retail Sales","COVID19","Coronavirus"],"title":"Analyzing the Impact of COVID-19 on Retail","type":"post"},{"authors":["Andre Sealy"],"categories":["Data Science"],"content":"There’s a familiar proverb often cited in politics, which states something to the effect of, “if you’re not outraged, you’re not paying attention.” I often say, as a response, if politics is functioning normally, the average person is not supposed to be paying attention. Everyone in politics has a habit of engaging in hyperbole; attributing nefarious intent towards their rivals; comparing our daily lives to political dramas like House of Cards or The West Wing. In reality, with the apparent exception of the year 2020, life is very dull. Politics is more similar to shows like VEEP; it’s mostly a joke, which would explain why most Americans don’t take it as seriously as others would like.\nAs such, most Americans don’t have well-formed political identities and don’t want one. They say if “Didn’t Vote” was a candidate for President during the 2016 general election, it would have won in a landslide [1]. When you consider that most people are not very political, an attempt to politicize everything becomes very poisonous. Once upon a time, people engaged in the activities they enjoyed. Now, the daily actions of individuals require them to engage a sort of political calculus. Similar to the “Rational Expectations” implication economist implement in theoretical models. When making decisions, the theory states that individual agents will base their decisions on the best possible information available.\nThe political universe, at least political Twitter, operates under a similar notion, albeit with a faulty premise. The assumption is that everyone is politically active, has access to all news information, and aware of all political/cultural tropes. While it’s flattering to assume that your fellow countryman is as politically aware as the next, the opportunity cost of remaining an informed citizen is very high. You can’t attribute every action to some nefarious political motivation. Sometimes, people are just trying to live their lives.\nWoke Capitalism The late Economist Milton Freidman said it best:\n“The great virtue of a free market system is that it does not care what color people are; it does not care what their religion is; it only cares whether they can produce something you want to buy. It is the most effective system we have discovered to enable people who hate one another to deal with one another and help one another.” - Why Government is the Problem\u0026#34;\nFriedman’s statement was made during a much simpler time. Who knows what he would think of the abomination has morphed into “Woke Capitalism.” First of all, what the hell does the term “woke” mean? It should be noted that I don’t use this term, and I have no desire to use it in the future. If you want to figure out what it means, you just look it up yourself.\n“Woke Capitalism” has a different connotation, depending on who uses it. Woke Capitalism can be referred to as an approach to how Corporations capitalize on popular social movements to increase their bottom line. The very polarizing Gillette commercial against toxic masculinity is one example [2]. Companies/brands changing their logo to incorporate rainbow colors every Pride month is another. It’s often seen as shilling and insincere, but some have argued that companies failing to address these issues imply they value profits over people. Damned if you do; damned if you don’t. There are many other examples, but I’m referring to a different kind of woke capitalism.\nThe type of woke capitalism I’m referring to the reverse of free-market capitalism Freidman lionized. In Woke Capitalism, race, sex, gender, religion, ethnicity, and many other factors determine how goods and services are produced. These factors all determine what goods and services are acceptable to purchase. Instead of working together, despite your differences, you work against one another because of them.\nAs such, these factors will even decide who is allowed to participate in the market. In an open market, resources are allocated toward practical and lucrative ideas. Woke Capitalism doesn’t even allow the market to discuss ideas, at least not contrarian ones.\nThe Marketplace of Ideas People often say, “sunlight is the best disinfectant,” which implies that truly objectionable ideas or “bad speech” should not be censored out principle. Instead, when given the opportunity, people should have a chance to examine these ideas. If they’re as terrible as one claims, people will become aware that others hold these opinions, and object to these ideas.\nThis philiophy can loosely be referred to as The Marketplace of Ideas. The rationale is based on the premise that truth will emerge from the competition of ideas, when people are allowed to engage in public discourse [3]. Early usage of this term are attributed to philiophiers, such as John Milton and John Stuart Mills; I first discovered it listening to Dave Ruben [4].\nRuben is a proponent of the freedom of expression, which should come as no surprise given the type of guest he has had on his show. Ruben …","date":1597449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597449600,"objectID":"bb10538c1421a83ff8b67fc2961e35ab","permalink":"http://localhost:4321/post/2020-08-15-hashtag-activism-redux/","publishdate":"2020-08-15T00:00:00Z","relpermalink":"/post/2020-08-15-hashtag-activism-redux/","section":"post","summary":"We're all Evergreen now...","tags":["Cancel Culture"],"title":"Cancel Culture, Woke Capitalism, and the Market Place of Ideas","type":"post"},{"authors":["Andre Sealy"],"categories":["Data Science"],"content":"I bet you’d never thought we’d find a way to politicize a deadly virus. I suppose that is what happens when your government operates under a system of federalism [1]. It’s almost as if people are just now realizing, when it comes to specific issues, that the state and local governments are more important than the federal government. In some ways, this crisis may have renewed our appreciation for the federalist system of government. In others, it has also magnified the polarization.\nLast month, New York State Governor Andrew Cuomo stated on Twitter that, “We [NY State] didn’t just flatten the curve – we crushed it.” This statement isn’t remotely true, for reasons that I’ve outlined [2]. Of course, I never realized at the time how much this statement would be used as a launching pad for the “blue states vs. red state” mantra. (Even during the early months of the Pandemic, it was “blue state vs. red state” in terms of testing, reopening, and a few other issues)\nAnd why not use a pandemic as an opportunity to advance that agenda, especially since Twitter makes it so easy for certain people to get away with being mildly dishonest. (To a certain degree, Twitter “fact-checks” post on their platform)\nWhat is the evidence does Neera Tanden use to support this claim? Who could say? It is, after all, just a talking point, but is there any basis for this claim?\nStating such a claim would require us to ignore some basic facts, such as 45% of all COVID-19 deaths originates in just four states (New York, New Jersey, Massachusetts, and California). You’d also have to ignore the fact that California, Colorado, Michigan, Illinois, Minnesota, and Nevada are all experiencing a surge in cases, along with most states in the country. (These are all “blue states,” in case you were wondering.)\nThe purpose of this post is not to say that, “Democrats and their partisan-politics are beyond the pale.” It’s certainly not beneath Republicans to engage in the same sort of distasteful rhetoric [3]. The point is, under our highly polarizing circumstances, we have become our source for false information. An Oxford study has found, when given accurate statistics on controversial issues, people tend to misremember those numbers to help align with commonly held beliefs [4].\nFor example, here is a fact that most people don’t know: it was primarily travel from New York City that seeded the wave of U.S. outbreaks for the rest of the country [5].\nFrom the New York Times:\nNew York City’s coronavirus outbreak grew so large by early March that the city became the primary source of new infections in the United States, new research reveals, as thousands of infected people traveled from the city and seeded outbreaks around the country.\nThe research indicates that a wave of infections swept from New York City through much of the country before the city began setting social distancing limits to stop the growth. That helped to fuel outbreaks in Louisiana, Texas, Arizona and as far away as the West Coast.\nResearchers studied the genetic samples of infected individuals from many different parts of the country. They’ve found that the majority of infectives were exposed by people traveling from New York City.\nResearchers were able to discover where the primary vectors originate by looking at the genetic code of the virus. Viruses tied to New York have a distinct genetic signature that links them to outbreaks in Europe. In contrast, the spread from Washington State has a signature that connects directly to China.\nThe following graphic shows the breakdown between the genetic samples that originate from Washington State (yellow bars) or New York City (red bars). It is well known that the earliest confirmed cases were originally found in these two areas.\nAs we can see, the vast majority of the spread is associated with an outbreak in New York City, with very little originating from Washington. This is not the say that the spread of the virus was primarily domestic (the virus was still brought into America, from abroad). However, the idea “the whole country could be seeing their surges” if the earliest of confirmed cases appeared in states like Texas is nonsensical.\nAfter all, New York City alone has ~65 million visitors at any given year, both domestic and international. The largest domestic markets originate in visitors from the NYC Metro Area (New Jersey, Connecticut, Pennsylvania), Massachusetts, Washington, D.C., and California (specifically, Los Angeles) [6]. Considering a large amount of international and domestic travel occurs in New York City, it’s easy to see how NYC was the primary vector for the domestic spread. After all, the spread occurred between residents leaving the city and non-residents who passed through the city.\nWould things be different if the earliest of confirmed cases were found in Texas? It’s hard to say. Texas has its fair share of visitors, as it shares a broader with Mexico. Many of the students who went to Mexico for spring break …","date":1594339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594339200,"objectID":"97b74479be93f73d01800411328f490e","permalink":"http://localhost:4321/post/2020-07-11-the-politics-of-pandemics/","publishdate":"2020-07-10T00:00:00Z","relpermalink":"/post/2020-07-11-the-politics-of-pandemics/","section":"post","summary":"We're all in this together, except when we need to score political points against our political enemies.","tags":["Coronavirus","COVID-19","Neera Tanden"],"title":"The Politics of Pandemics","type":"post"},{"authors":["Andre Sealy"],"categories":["Statistics","Economics"],"content":"You may not believe it, but it’s true. We currently already spend, on aggregate (at the state and local level), more on teachers and education more than we spend on police and law enforecement. But who says we aren’t, and why does this matter?\nI give you Exhibit A:\nHere, we have the anatomy of a viral tweet: an emotional platitude, factually inaccurate, and a rallying cry to address a problem that currently does not exist. To be fair, maybe Benjamin Dixon is trying to tell us what we should NOT be doing. As in, we should NOT get to a point where we are funding Police more than Teachers.\nI think that’s fair, and most people would agree with that. As a thought experiment, I decided to look at economic statistics to analyze the ratio of the levels we’re spending on Police relative to Teachers. So, as usual, I took to Python [1].\nSo the output of our code is the historical Government Current Expenditures at the state and local level of law enforcement. This data goes back to 1959. As of 2018, we currently spend 137 billion dollars on police [2]. (Note: much of these data comes from FRED, although it sourced by the BEA)\nIn 1960, we were spending ~2 billion dollars on police. That’s is a 6,750% increase in 60 years. Faster than the rate of inflation last time I checked. That’s a lot.\nHas spending on Education managed to keep the same pace?\nWell, this plot looks the same as the police expenditures plot, but the y-axis is ten times larger.\nMaybe we should place both time series on the same plot so we can see what’s really going on.\nIt certainly doesn’t look like we’re spending more on Police than we are on Teachers. Not anywhere close. Based on what I’m looking at, what we spend on law enforcement is significantly dwarfed by the amount we spend on education (as it should be).\nSo why does this narrative persist? Why do people believe that we’re not spending enough on education? I give you Exhibit B:\nThere is this interesting perception among the #defundthepolice proponents is that Americans should be spending more on social services (including education), and the only reason we can’t do that is because so much of current government expenditures is already allocated towards law enforcement.\nAs you can see from the meme above, the police take up a large portion of the budget. The reduction in funding for police would naturally be offset by other government programs, such as Housing, Education, Health Care, reinvestments, etc.\nSure, a reduction in police spending would increase spending for other things; however, we already spend significantly more on other things relative to the police.\nWe should indeed be spending more on all of the things that would probably reduce crime. However, there is currently nothing preventing us from doing this, especially not the police. If you compare the United States to Europe, we find that most European countries are more generous with welfare and social spending. However, this doesn’t have anything to do with how much they spend on police.\nAs President Obama’s Council of Economic Adviser pointed out, while the United States has more correctional officers and far more prisoners than the average country, it has 35 percent fewer police officers [3].\nThe current problem with our relationship with police in society is that they’re too unaccountable, have too much job security, and have far too little training. In fact, I would argue that that in order for police to recieve more training, they need more funding, not less.\nAs state and local governments take steps towards reducing the level of funding from their law enforcement, we can expect the fewer people employed in these fields. Violent crime is currently at a 50 year low. One could make the argument, given this trend, that we no longer need the amount of police in our society.\nHowever, if one is going to argue that police have no role in maintaining safe streets, they’re also arguing against lots of strong evidence. One of the most robust, most uncomfortable findings in criminology is that putting more officers on the street leads to less violent crime [4][5][6][7][8].\nSources [1] Kid Quant | Replicating the data from this post\n[2] Federal Reserve Economic Database (FRED)\n[3] Obama White House (2016) | Economic Perspectives on Incarceration and the Criminal Justice System\n[4] Klick \u0026amp; Tabarrok (2005) | Using terror alert levels to estimate the effect of police on crime\n[5] Klick, MacDonald, \u0026amp; Grunwald (2015) | The effect of private police on crime: evidence from a geographic regression discontinuity design\n[6] Mello (2018) | More COPS, Less Crime\n[7] MacDonald, Fagan, \u0026amp; Geller (2016) | The effects of local police surges on crime and arrest in New York City\n[8] Rosenfeld (2013) | Focusing police efforts on “hot spots” reduces crime - and can prevent it, too\n","date":1592352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592352000,"objectID":"63472896cfa4ad7aeac0ccf3c24cac66","permalink":"http://localhost:4321/post/2020-06-17-we-fund-teachers-not-police/","publishdate":"2020-06-17T00:00:00Z","relpermalink":"/post/2020-06-17-we-fund-teachers-not-police/","section":"post","summary":"When popular narratives fail to capture the reality.","tags":["Defund The Police","Black Lives Matter","Social Programs"],"title":"We Already Fund Education More Than Law Enforcement","type":"post"},{"authors":["Andre Sealy"],"categories":["Statistics","Data Science"],"content":"I keep reading – and hearing – that New York has successfully “flattened the curve,” and it makes me wonder if people have just decided to go into hibernation the past three months. It’s not outside the realm of possibility; we all have lots of free time on our hands now. However, it’s a sign that our political discourse has become so warped that our politicians can say things that are patently false with very little pushback.\nWhat did I expect? As a politician, he’s not going to say, “New York has been decimated by the virus,” but that’s precisely what happened. I haven’t talked about “flatten the curve” at all on this blog because it assumed that it was well understood by most. I was obviously wrong. How else can a governor of one of the largest states get away from saying, “we crushed” the curve?\nWhat Does “Flatten The Curve” Even Mean? Flatten The Curve is an objective, which refers to the “what if” epidemiological curve. What it means is that instead of the rate of rising being very rapid, perhaps a doubling in cases every 4 - 7 days, we cut the doubling to every 2 - 3 weeks. The purpose of flattening the curve was to prevent an overwhelming of the health care system, so health care professionals can find more effective ways of treating infected patients.\nThe following image shows what a flattened curve should obviously look like. The red-shaded steap curve shows a scenario with preventative measures (social distancing, mask, etc.), while blue-shaded curve shows a scenario with protective measures. The blue-shaded curve is flatter than the red-shaded curve, and that is the scenario that we are trying to implement.\nNotice how I didn’t say any of this would happen, because there is no guarantee that that it should. There are other factors that we may fail to consider and you’ll know you’re at the peak until you’ve reached it.\nSo, what happened? Did we get the blue-shaded flat curve?\nAbout a month or so ago, I did a project on tracking COVID-19 cases globally; part of that analysis involved looking at different U.S. States [1]. Around that time, cases in New York were doubling every 7 days.\nToday New York, the increase in cases is relatively stable, trending at ~1,200 additional cases a day. However, during the early days of the pandemic, the New York average around 10,000-12,000 additional cases day. The state was probably responsible for at least 30 percent of all new coronavirus infections.\nSo, to answer a previously-posed question, no, we did not get the blue-shaded flat curve, synonymous with a gradual increase in new infections. Instead, what we witness was a rapid spike in daily cases. At least, when compared to cases in Washington State, which did experience a large surge in cases but managed to taper off early during the crisis.\nBut that is all in the past. New York is doing relatively better now, even as cases surge in places that were previously managing the virus well. So, what exactly is New York doing that is so special? It’s not as if New York is doing anything spectacular when it comes to testing or tracing.\nThe main thing that happened… was the virus rushed through the most susceptible people in the population.\nIn my post, The Mathematics of Pandemics, I laid out the framework as to why you can prove (mathematically) that the virus always dies out [2]. (susceptible people get exposed; exposure becomes an infection; infectives either recover or die) It doesn’t happen when the disease infects everyone in the population, or when the pathogen mutates into something else.\nThe simple reality is that contact between the remaining infectives and susceptibles become so infrequent that the infectives never get a chance to pass on the disease before being removed from the equation (recovering with immunity or dying). The virus dies due to the lack of infectives, not the lack of susceptibles.\nFor the most part, this has happened (is happening) in New York. Several factors can increase the risk of the susceptibility of COVID-19; however, we know that the disease is more susceptible to individuals over the age of +65-years-old. We can see from the data, the death rate for people age 65 and over is more than ten times higher than for people under 65-years-old [3].\nSo if your objective is to flatten the curve starting March, you would expect to see a steady increase in the number of infections, spread out over months. If you failed to flatten the curve, you saw a huge spike in cases, followed by a massive spike of fatalities, followed by a rapid decrease, as we saw here in New York.\nPutting this in perspective, if the State of New York were a country, it would have the highest death rate in the world. That’s not a novelty.\nWe are noticing a surge in cases in other parts of the country, places that have yet to experience their first wave (Flordia and Arizona). Deaths in these states are relatively small. The death rate from COVID-19 is ~170 per 100,000 people in New York State [4], Arizona has reported …","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591574400,"objectID":"8a33765b28994202472195dda5216d77","permalink":"http://localhost:4321/post/2020-06-08-ny-crushed-by-curve/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/post/2020-06-08-ny-crushed-by-curve/","section":"post","summary":"You keep using that phrase, but it's not clear you understand what it means...","tags":["COVID-19","Coronavirus"],"title":"No, New York Has Not Flattened The Curve","type":"post"},{"authors":["Andre Sealy"],"categories":null,"content":"Never before has America become more polarized than we are today. As such, the news we consume is a function of our polarization. Right-leaning people tend to get their information from Fox News, The Wall Street Journal, National Review, etc. People who lean left tend to get their information from MSNBC, The New York Times, The Huffington Post, etc. There are a handful of neutral publications (Reuters, Politico, Associated Press, etc.). Still, even their alignment falls into question, based on how they report the news.\nIt’s unclear to what extent news plays a role in fueling the partisan divide in our country. There is some evidence to suggest that people, when given accurate information on controversial issues, will choose to disregard this information to fit commonly held beliefs [1]. This perhaps can explain the apparent bias for news outlets. Now, more than ever, people should be allowed to sift through news sources that are free from bias, misinformation, and personal polarization.\nAll Sides News Aggregator According to an old Pew Research poll, most Americans would prefer if news media would present the facts without adding their interpretation of the events [2]. Some may argue that objectivity in the news has never occurred, and therefore, impossible, and in some cases, undesirable. Regardless of whether or not our news should be presented from an objective lens, at the very least, journalists should be honest about their affiliation. In comes All Sides.\nAll Sides is a bipartisan organization that looks at a more balanced approach to news coverage by collecting the top headlines of the day and showcasing the reporting of the news outlet on the left, right, and center. The platform also allows readers the rate the lean of the publication for further analysis [3].\nThe first mode of attack is figuring out how we can go about extracting these stories for our analysis. We can extract all stories from all publications, but what if we want a more targeted focus? One interesting feature of the All Sides website is that we can look for articles based on the topic.\nWe can get articles from pretty much any topic: Criminal Justice, Education, and the Economy. All Sides even collects the news perspectives on the most important topic in the world right now, the Coronavirus. (Yes, our society is so divided right now, we’ve even managed to politize a deadly virus)\nI’m going to use immigration as our topic; I feel it’s pretty easy to understand where both sides (the political left and right) stand on this issue. The first part of the project involves web scraping the information that we are most interested in, such as the headline, date of the story, description, source, the lean/bias of the source, and the link of the story.\nThe results provide a data frame that we can use as a stepping stone to extract relevant information, such as the body of the article and the authors. Sure, we could have extracted all of that information along with the rest. However, web scraping is very tricky, as no two websites have the same HTML structure and layout. This is especially true for news media websites.\nAs such, we relied on a third-party source for the extracting news, with the API known as News-Please.\nNews Please API News-Please is an open source news crawler that extracts structured information from almost any website. You can use it to follow recursively internal hyperlinks and read RSS feeds to extract most recent and old archived articles [4].\nWith this API, we only need to provide the root URL of the article to crawl it completely. The New-Please API combines the power of multiple state-of-the-art libraries and tools, such as scrapy, Newspaper, and readability.\nUsing this API, along with other web crawler techniques, we have randomly extracted 291 articles from multiple sources. The Top 10 articles we extracted are presented in the following data frame.\nAs we can see, Fox News has the largest number of articles in our dataset, with 25 pieces. Reuters is right behind Fox News with 20 articles, followed by the Wall Street Journal (News Section), Washington Times, and The Hill with 20, 17, and 15 articles, respectively.\nI can only speak for myself, but from what I see so far, I believe this alignment is correct for the most part. Fox News tends to lean right, while CNN and New York Times lean more left.\nI think some people would disagree with the alignment of the Wall Street Journal, as the publisher is owned by News Corp, which is the parent company of Fox News. I believe most people would be inclined to agree, as far as community feedback on All Sides is concerned.\nStill, whether or not a news outlet is owned or operated by a particular person has little to do with its overall objectivity and bias. All Sides has conducted an in-depth analysis of major news publications such as The Wall Street Journal and has found that outlet is more aligned to the center than its peers. (Keep in mind, a Center alignment doesn’t mean …","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587513600,"objectID":"d7eb7d6f5f53d3dcfabc850070aba22f","permalink":"http://localhost:4321/project/analyzing-news-articles-python/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/project/analyzing-news-articles-python/","section":"project","summary":"We Analyze the polarity, sentiment, meta-cognition, bias, and many other things.","tags":["Machline Learning","Data Science","Statistics","Media","News"],"title":"Analyzing News Articles With Python","type":"project"},{"authors":["Andre Sealy"],"categories":["Data Science","Programming","Statistics"],"content":"There have been many visualization projects created for the purposes of modeling COVID-19 cases and deaths. The best known are the graphs produced by John Burn-Murdock of the Financial Times. You can view his very create and beautiful COVID visualizations on his Twitter Page.\nBefore COVID, I think people were more used to viewing data in relative terms. Now, log-scale visualizations have become more widespread, primarily due to the way diseases spread during a pandemic.\nIt’s become so popular, programmers developed ways to replicate these charts using Python and R packages. The most notable being tidycovid19 package designed by Joachim Gassen. You can few the package on his GitHub [1].\n# Importing all of the necessary packages library(tidyverse) library(tsibble) library(tidycovid19) # selecting how the data will be formatted updates \u0026lt;- download_merged_data(cached = TRUE) # plotting the countries of interests, countries \u0026lt;- c(\u0026#34;AUS\u0026#34;, \u0026#34;NZL\u0026#34;, \u0026#34;ITA\u0026#34;, \u0026#34;ESP\u0026#34;, \u0026#34;USA\u0026#34;, \u0026#34;GBR\u0026#34;) updates %\u0026gt;% plot_covid19_spread( highlight = countries, type = \u0026#34;confirmed\u0026#34;, edate_cutoff = 40, min_cases = 100 ) However, people have often wondered why we don’t present these numbers on a per capita basis? Burn-Murdock has provided a data-oriented reason why log-scales are used instead of per capita scaling.\nIt isn’t always clear when per capita metrics should (or should not) be used. However, many don’t often realize how per capita figures can distort the story of what is going on in the region.\nFor example, Switzerland has 2,975 cases per capita, while the United States has 1,841 cases per capita [2]. However, cases are currently growing and doubling faster in the U.S. compared to Switzerland [3]. Obviously, Switerzland’s high per capita figure is primarily attributed to its small population size, not its response to containing the virus.\nSimply put: per capita figures makes small countries look big, and vice versa. Not only that, per capita figures only shifts the curves vertically; it would not change the trajectory or slope of the curves. Remember the following:\n$$log(Y_{t}/P_{t})=log(Y_{t})-log(P_{t})$$\nWhere $Y_{t}$ is the number of cases on dat $t$ and $P_{t}$ is the population on day $t$. Population doesn’t change much over a period of time, so per capita figures involves subtracking a constant from the log case data.\nThe same point applies to the differences in testing policies between countries. If Country A is testing more as a percentage of the population relative to Country B, then these differences will show up as a vertical difference between the plots. They won’t; however, show up in the trajectory.\nAlthough these choices are still a bit of a judgment call (not standard among statisticians), we should look at the slope of the lines if we want to explore how countries are responding to the pandemic relative to others.\nWhile the graph doesn’t tell us everything about how the pandemic is overwhelming the different health care systems around the world, it does show how each country is attempting to slow the infection rate and reduce the slope of their curves. It’s easier to compare the trajectory in this way, considering that we have to look to see which country is trending higher.\nSources [1] GitHub | Jaochim Gassen TidyCovid19 Repository\n[2] Twitter | John Burn-Murdock [3] Our World In Data | Global COVID Cases per capita as of April 15th, 2020\n[4] KidQuant | Data Science Project on COVID Tracking (Post from Apr 13, 2020)\n","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586908800,"objectID":"46f1666d2c8f06c6a6e579ef809d7493","permalink":"http://localhost:4321/post/2020-04-15-measuring-covid-progress/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/post/2020-04-15-measuring-covid-progress/","section":"post","summary":"Lying with statistis - Case 1324","tags":["COVID-19","Coronavirus"],"title":"Measuring Pandemic Progress: Log Scale vs. Per Capita","type":"post"},{"authors":["Andre Sealy"],"categories":["Mathematics","Programming"],"content":"So, this is our very first pandemic. At least, it’s the first serious pandemic. I’ve lived through the 2009 H1N1 Pandemic, but I don’t really remember it all that well. All I know is that H1N1 is not nearly as serious as this, because now I’m forced to work from home.\nRight now, COVID-19, a.k.a, the coronavirus is on our doorstep. There are more than 423,000 confirmed cases in the United States; 14,700 confirmed deaths [1]. New York has the highest record death toll; we surpassed Washington State a few weeks ago. It certainly does feel like an apocalyptic scenario is about to occur.\nRegardless, one good thing that can occur out of this situation is that the Coronavirus has rekindled the public’s interest in Mathematics. At least, marginally. Articles are circulating explaining “dangerous mathematics behind the coronavirus,” and how cases could grow exponentially [2].\nThe idea behind exponential growth as it relates to pandemics is that one person who has contracted the virus comes into contact with anyone. So the total number of cases increases to 2. Assume each infected person will come in contact with 1 new person. Then those 2 infected people will infect 2 more people, increasing confirmed cases to 4. Those 4 will eventually become 8, then 8 becomes 16, and so on.\nThis is how COVID-19 could work, but at best, it’s an oversimplification. Everyone online (and by everyone, I mean Twitter) has an opinion on what they believe will happen. Some people are behaving like it’s the end of the world; others think this will be over by Easter.\nI, on the other hand, found that it is more useful to learn about the methodologies epidemiologists are using the model the virus.\nThe SIR Model The SIR model is a simple model developed by Ronald Ross and William Hamar, but greatly expanded by Anderson McKendrick and William Kermack, which simplified the process. Using this model, we assume the population consist of three types of individuals, which are denoted by variables $S$, $I$, and $R$.\n$S$ is the number of susceptibles; who are not infected, but could become infected at a later point. $I$ is the number of infectives; individuals who have the diease and can transmit to others. $R$ is the number of recovered or removed; they cannot become infected or transmit to others. It should be noted that people that have recovered from a virus may have developed a natural immunity over time, or they may have currently been in isolation, or they may have died. The SIR model doesn’t distinguish between these possibilities.\nNew infections will occur as a result of contact between infectives and susceptibles. In the SIR model, the rate at which new infections happen (as a function fo time) is $\\beta$, $S$, $I$, for some positive constant $\\beta$. These interactions occur as a form of “mixing,” and mixing allows us to utilize ordinary differential equations (ODE).\nYou don’t need any knowledge about differential equations. However, derivatives are how mathematicians describe the rate of change of as time goes on.\n$$ \\begin{aligned} \\frac{dS}{dt} \u0026amp; = -\\beta \\cdot S \\cdot I \\ \\frac{dI}{dt} \u0026amp; = \\beta \\cdot I \\cdot \\frac{S}{N} - \\nu \\cdot I \\ \\frac{dR}{dt} \u0026amp; = \\nu \\cdot I, \\end{aligned} $$\nwhere the diease transmission rate $\\beta \u0026gt; 0$, and the recovery rate $\\nu \u0026gt; 0$ (or in other words, the duration of infection $D=1/\\nu$).\nAs you may have noticed, that total population $S+I+R$ is constant because the differential equation $\\frac{dS}{dt}+\\frac{dI}{dt}+\\frac{dR}{dt}=-\\beta IS+(\\beta IS -\\nu I) + \\nu I=0$\nIt’s probably easier to visualize this instead of talking about this subject in the abstract. There are some developers on GitHub that have implemented the SIR model for creating an infectious disease model. One of whom goes by the name of Henri Froese, who is a computer science student [3].\nModel Parameters We want to derive our differential equation, $\\frac{dS}{dt}$, $\\frac{dI}{dt}$, and $\\frac{dR}{dt}$ with the parameters $S$, $I$, $R$, $\\beta$, and $\\nu$. We will also adjust for the population\n# creating a function for the differential equation def deriv(y, t, N, beta, nu): S, I, R, = y dSdt = -beta * S * I / N dIdt = beta * S * I / N - nu * I dRdt = nu * I return dSdt, dIdt, dRdt We’re simulating a pandemic in a town with a population of 1,000 (not a very large but it’s easier to wrap your head around). We will assume that an infected person infects one other person per day ($\\beta = 1$).\nFor our made up virus, we will set the duration of infection for four days. The initial condition starts with one infection, which leaves 999 susceptible people (and no recoveries, so far).\nN = 1000 beta = 1.0 D = 4.0 nu = 1.0 / D S0, I0, R0 = 999, 1, 0 t = np.linspace(0, 49, 50) y0 = S0, I0, R0 ret = odeint(deriv, y0, t, args=(N, beta, nu)) S, I, R = ret.T The solution to a differential equation is any condition that satisfies it, so we are going to use the odeint method to integrate the differential equation. Afterward, we unpack the results for S, I, …","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586304000,"objectID":"1293efabecede8c2061e09b2f43b3f33","permalink":"http://localhost:4321/post/2020-04-08-the-mathematics-of-pandemics/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/post/2020-04-08-the-mathematics-of-pandemics/","section":"post","summary":"How we respond to infectious dieases is probably more important than the diease itself.","tags":["COVID-19","Coronavirus"],"title":"The Mathematics of Pandemics","type":"post"},{"authors":["Andre Sealy"],"categories":null,"content":"Forecasting Gold and Oil has garnered major attention from academics, investors and Government agencies alike. These two products are known for their substantial influence on the global economy. However, what is the relationship between these two assets? What generally happens to Gold prices when Oil takes a plunge? We can use a statistical technique known as Granger Causality to test the relationships of Gold, Oil, and some other variables. We can also use a VAR model to forecast the future Gold \u0026amp; Oil prices.\nExploratory Analysis Let’s load the data and conduct some analysis with visualizations to understand more of the data. Exploratory data analysis is quite extensive in multivariate time series. I will cover some areas here to get insights into the data. However, it is advisable to conduct all statistical tests to ensure our clear understanding of data distribution.\nIn our dataset, we have six variables: Gold, Silver, Oil, US Dollar, Interest, and a Stock Market Index. The stock market index we’ve used was the Dow Jones Industrial Average. We usually use linear regression to determine if there is a correlation between specific variables. Instead, we’re going to determine if any of these variables directly causes movement in the other variables.\nHere, we can see different trends for six of our variables. It would be helpful if we established whether or not our time series followed a normal (or Gaussian) distribution. We will do this based on the test for normality based on the Jarque-Bera test.\nJarque-Bera Test There are actually three test that we could have implemented for testing normality; however, we will focus on just one: the Jarque-Bera Test.\nThe Jarque-Bera goodness-of-fit test determines whether a sample data have the skewness and kurtosis matching a normal distribution.\n$$k_{3}=\\frac{\\sum^{n}{i=1}(x{i}-\\bar{x})^{3}}{ns^{3}} ,,,,,,, k_{4}=\\frac{\\sum^{n}{i=1}(x{i}-\\bar{x})^{4}}{ns^{4}}-3 $$\n$$JB = n \\Bigg(\\frac{(k_{3})^{2}}{6}+\\frac{(k_{4})^{2}}{24}\\Bigg) $$\nwhere $x$ is each observation, $n$ is the sample size, $s$ is the standard deviation, $k_{3}$ is skewness, and $k_{4}$ is kurtosis. The null hypothesis, $H_{0}$, follows that the data is normally distributed; the alternative hypothesis, $H_{A}$, follows that the data is not normally distributed.\nThere is a simple script we can use in python to determine the goodness of fit, the kurtosis, and the skewness.\nfrom scipy import stats stat, p = stats.normaltest(df.Gold) print(\u0026#39;Statistics=%.3f, p=%.3f\u0026#39; % (stat, p)) alpha = 0.05 if p \u0026gt; alpha: print(\u0026#39;Data looks Gaussian (fail to reject H0)\u0026#39;) else: print(\u0026#39;Data does not look Gaussian (reject H0)\u0026#39;) Lets test out the goodness of fit on the Gold variable.\nStatistics=818.075, p=0.000\nData does not look Gaussian (reject H0)\nAs we can see, we recieve a p-value and a test statistic in return. While we don’t have a Chi-Square chart to determine the critical region, we do have a p-value, which is basically zero.\nA p-value less than 0.05 basically means our test statistic falls within the critical region (inside the tail); therefore, we can reject the null hypothesis. The dataset is not normally distributed, which we probably could have assumed by looking at the time series plots from before.\nLets compare kurtosis and skewness for both the Oil and Gold variables. These two distributions can provide us with some intuition about the distribution of our data. A value close to 0 for kurtosis indicates a normal distribution where asymmetrical nature is signified by a value between -0.5 and +0.5 for skewness. The tails are heavier for kurtosis greater than 0 and vice versa. Moderate skewness refers to the value between -1 and -0.5 or 0.5 and 1.\nBased on the normal probability plot, we can see that the dataset is far from normally distributed.\nThe VAR Model Considering that our dataset isn’t normally distributed, we can use this information to build a VAR model. The construction of the model will be based on the following steps:\nTesting for autocorrelation Spliting the time series into Training and Testing data sets Testing for Stationarity Testing for Causation using Granger Causality Conducting a forecast evaluation Auto-Corrlation Autocorrelation refers to how correlated a time series is with its past values as a function of the time lag between them.\nThe following plot shows the autocorrelation for the Gold time series. As we can see, each value is highly correlated with the previous value. We have an autocorrelation of +1, which represents a perfect positive correlation. An increase seen in one time series leads to a proportionate increase in the other time series.\nWe need to apply transformation and neutralize this time series stationary. It measures the linear relationships. Even if the autocorrelation is minuscule, there may still be a nonlinear relationship between the time series and a lagged version of itself.\nSpliting the time series into Training and Testing data sets The VAR model will be fitted on …","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"877b3243092f6bca7408b61b863ebbef","permalink":"http://localhost:4321/project/forecasting-var-granger-causlity/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/project/forecasting-var-granger-causlity/","section":"project","summary":"An exploration of the Black-Scholes framework and examining payoff functions and option Greeks.","tags":["Programming","Finance","Statistics","Mathematics"],"title":"Forecasting Asset Prices Using VAR and Granger Causality","type":"project"},{"authors":["Andre Sealy"],"categories":["Finance","Economics"],"content":"If you were able to catch the Congressional testimony on Facebook’s new project Libra and the Calibra wallet, probably didn’t learn a ton about the latest technology; it’s benefits; it’s detriments, and how it will be integrated into our 21st-century economy [1]. Listing to the Congressional hearing, one might get the impression that Facebook is the cause for every single one of the problems we face in this country.\nAlthough regulators may not care about how the new Libra project will benefit (or affect) our economy, you, the consumer, needs to know what this means for you. Anyone living in China (or visitors to China) has already seen the future. In most major metropolitan Chinese cities, you no longer need to walk around with a wallet. Your cell phone or mobile device allows you to purchase food from street vendors, pay to wash your clothes at a laundry mat, and purchase items in a grocery store. Even the street beggers will not accept your money unless it’s done via QR code.\nNeedless to say, if America does not innovate, its future in financial leadership is not guaranteed. Digital payments are happening worldwide, whether the US government likes it or not, whether you like Facebook or not. Despite the blatant grandstanding from our elected officials on the tech-giant, other concerns with the project are related to privacy, centralization, and the lack of self-sovereignty and are therefore skeptical of libra.\nThese are legitimate concerns; however, they do not really matter in the grand scheme of things. I can tell you first hand that the family running the Chinese restaurant on my street corner doesn’t care about any of those things. However, they do have Facebook, and so do all of their customers. Right now, the restaurant only accepts cash for their services. If they suddenly decide to take payment with nearly zero fees without having to handle cash, there’s no doubt in my mind that they will.\nThe Shift to a Cashless Soceity is Already Mainstream. Countries such as Norway, Sweden, Denmark, and Finland are largely cashless countries; both individuals and retail stores continue to embrace online payment systems. Norway has the lowest cash usage in Europe thanks to companies like Vipps, which is used by two-thirds of the Norwegian population [2] [3].\nThe desire for convenience is also beneficial. Once the average person starts adopting online payments into their life, paper bills and coin-filled pockets only become an annoyance. So now the important question is, “would we rather have a global digital currency controlled by governments or let a company like Facebook bootstrap it?” Is it really bad for crypto-enthusiast? I don’t really think so.\nSins of the Past To say that Facebook had a rough 2018 would be an understatement. In March 2018, it was discovered that 50 million Facebook users had their private data compromised in one of the worst data privacy breaches in history, for what is currently known as the Cambridge Analytica scandal. The data extracted has been used to build psychological profiles to influence elections around the world. Ultimately, CEO Mark Zuckerberg was forced to attend a Congressional hearing regarding the incident.\nAfterwards, in May 2018, it was revealed that Facebook was establishing a blockchain division that would run under the supervision of David Marcus, who had earlier overseen the creation of Facebook Messenger. This happened four months after Zuckerberg stated in his new year resolution post that he wanted Facebook to “go more in-depth and study the positive and negative aspects of cryptocurrencies.\nThen in December 2018, news came out that Facebook was building a cryptocurrency, which would make it easier for them to transfer money on Whatsapp. The initial focus would be in India since they have 400 million Whatsapp users.\nFinally, in June 2019, Facebook formally announced Libra and released its whitepaper [4]. The coin is set to launch in 2020 of next year.\nReaching Out to the Unbanked To understand the real power of cryptocurrencies and why Facebook’s initiative could be a game-changer, we need to understand one of the most significant issues in the financial world. Business Insider reported back in August 2017 that more than 2 billion people around the globe lack access to a bank account [5]. Other interesting information regarding the unbanked population also includes the following:\nMore than 20% of unbanked adults received wages or government transfers in cash. Also, a significant number of people in developing nations pay bills and school fees in cash.\nWomen make up just over half (55%) of the unbanked population.\nThere are ~438 million people without a bank account in Southeast Asia alone.\nIndia, which is the target market for Libra, has ~190 million people without bank accounts, which is the second-largest in the world after China.\nConsidering that there are ~5 billion people with mobile phones and ~2.38 billion Facebook users globally, the company is in a …","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"d7383b07dc9ede4ec5c28a5674c7c8c9","permalink":"http://localhost:4321/post/2019-10-25-libra-is-the-future-of-money/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/post/2019-10-25-libra-is-the-future-of-money/","section":"post","summary":"The post about what Libra is; most of all, what it not.","tags":["Binance","Crypto Currency","Blockchain"],"title":"Libra is the Future of Money","type":"post"},{"authors":["Andre Sealy"],"categories":["Economics"],"content":"There’s a lot of talks these days about an economic slow down that could potentially occur in the United States. If this is true, the evidence of this has yet to appear in the labor market. The BLS posted an unemployment rate of 3.5% for September 2019, down 0.2% from the previous month. The economy also created 136,000 jobs, which is slightly less than what was expected, previous months were revised upwards [1].\nIt was initially assumed that the unemployment couldn’t go much lower, due to something called NAIRU: the Non-Accelerating Inflation Rate of Unemployment. Without getting into deep economic theory, NAIRU (not to be confused with the natural unemployment rate) describes the level at which it would be difficult for the unemployment rate to decline, due to the inability of firms to attract new talent at the prevailing market wage.\nAs such, the only way to attract new workers would be to raise wages, which would force firms to rates prices to offset their cost, which would raise consumer prices overall. Are the historical lows of the unemployment rate evidence of further slack in the labor market?\nWhat does JOLTS tell us? JOLTS, or the Jobs Openings and Labor Turnovers Survey, is another measure of labor utilization, similar to the establishment and population surveys of the BLS [2]. The survey tracks job openings, hires, and total separations (whether voluntary or involuntary) of the labor force. The BLS reports JOLTS with a two-month lag, as opposed to the employment situation, which is reported with a one-month lag.\nJob Openings Total job openings for August 2019 were reported at 7.05 million, which is lower the 7.34 million print during the same time a year ago. This brings the job openings rate to 4.4%.\nHires Total hires were reported at 5.77 million, again lower than the 5.82 million figure posted a year ago. The hires for August is 3.8%.\nSeparations Separations are reported in two different categories: quits and layoffs. The number of quits decreased in August to 3.52 million; however, still the highest level ever. At the same time, the number of layoffs was 1.78 million. Altogether, the number of separations for August is 5.63 million.\nThis brings the quits rate to 2.3%, the layoffs rate to 1.2%, and the total separations rate to 3.7%.\nBeveriedge Curve Conventional economic wisdom tells us that there is an inverse relationship between the job openings rate and the unemployment rate. Why is this; shouldn’t more employment generally lead to fewer job vacancies? This is not necessarily the case. Higher unemployment is usually associated with recessions, and when recessions occur, firms are less likely to expand their workforce; hence, a lower job openings rate. The relationship can be illustrated in what is known as The Beveridge Curve.\nThe Beveridge Curve plots the job openings rate with respect to the unemployment rate [3]. During an expansion, the job openings rate is high, and the unemployment rate is low, which causes the curve to move towards plots upward and to the left. During a contraction, the job openings rate is low, and the unemployment rate is high, which causes a movement along the curve in the opposite direction. Basic Econ 101, pretty much.\nAs we all learn in Econ 101, things become more interesting when we’re dealing with shifts in the curve, which can indicate a structural change in the economy. This shift can be due to industry-based structural mismatching or geographical changes. If we examine the relationship between unemployment and job openings, this appears to be the case.\nThe following Beveridge Curve is one that I have created in python, using data from the Federal Reserve Economic Database (FRED), which was extracted through Quandl [4] [5].\nIt would seem that the curve started to shift around 2010, a couple of months after the end of the Great Recession. However, when the recovery started in 2009, the curve did not retrace the previous pattern from 2000-2009. What does this shift in The Beveridge Curve mean?\nWe can look back at a random period from nearly 20 years ago when the unemployment rate was near 4.0%. The job openings rate hovered around 3.0%. More than a decade later, the job openings rate increased to 4.5% with the same level of unemployment. So, for any level of unemployment (horizontal axis), there appears to be a higher rate of job openings (vertical axis).\nMore than 10 years after the Great Recession, employers seem to be reluctant to fill their job openings. Put it another way, it appears that employers are having a difficult time finding a match when posting job opportunities, whether internally or externally. Following periods of economic decline, it isn’t uncommon to find high levels of unqualified candidates. This mismatch usually lasts about three years, on average, according to some research [6].\nAs mentioned before, the unemployment rate is near historic lows. Who could possibly suffer from this structural shift?\nYour First Big Break After College …","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"4d4ce612c50092877541ce359e4b5831","permalink":"http://localhost:4321/post/2019-10-10-latest-jolts-survey-shows-labor-market-bifurcation/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/post/2019-10-10-latest-jolts-survey-shows-labor-market-bifurcation/","section":"post","summary":"Four jobs available for every one person who is looking. What wrong with this picture?","tags":["Labor Market","JOLTS","Education","Unemployment"],"title":"How The Beveridge Curve Reveals Current Labor Market Bifurcation","type":"post"},{"authors":["Andre Sealy"],"categories":["Economics"],"content":"Historically, wealthy individuals tend to live longer than poorer individuals, holding other factors constant. They can afford to eat healthier foods from healthier food outlets (such as Whole Foods); they can afford better health care, et cetera.\nFor this reason, it should be obvious that as countries get wealthier that they should be able to afford better health care systems, also that life expectancy should rise. It should also stand to reason htat people living longer, will work longer and that being healthier means fewer people as a propotion drop out of the labor force due to illness.\nHowever, these relationships probably are not linearly. They’re likely to be diminishing returns to one from the other. At least, that is my prior belief.\nMy first hypothesis is that returns to life expectancy at birth from increases in Gross Domesti Product (GDP) per capita are deminishing. This is because there is probably an upper limit on the length of human life and that approaching it is more difficult the closer humans reach.\nAlso, it is easy to concieve nations consuming more unhealthy luxury items as they become richer. Therefore, I expect the coefficient on GDP per capita to be positive and the coefficient of GDP per capital squared to be negative in the regression of life expectancy on the GDP per capita variables.\nMy second hypothesis is that returns to GDP per capita from increases in life expectacy are diminishing. Therefore, I expect the coefficient of life expectancy to be positive and coefficient of life expectancy squared to be negative in the regression of GDP per capita on life expectancy and life expectancy squared.\nTo test these hypothesses, I repeatedly estimate those models over crosssections in my sample period using Ordinary Least Squared (OLS) regression. Finally, I plotted the model coefficients over time to see if the relationships have changed.\nThere are already tons of literature on this question, but I chose not to research it because it did not want bias to approach this problem. Significance in this document is at the alpha = 0.05 level.\nData and Sample Period Selection I used data sets from the World Bank to perform this analysis. I selected my sample period by examining the period of the missing values and choosing long time periods with as few missing observations as possible. This seemed to be the period from 1990 to 2017. Removing observations that weren’t real countries (mainly regional and world-wide averages) and performing an inner join between two variables on the sample period left about three quarters of our countries in our data set, of 163 observations.\nPlotting our Variables of Interest Below is how the life expectancy at birth for the world has changed over the years. It has increased steadily over the years, which is good for all of us.\nThis histogram is representative of the distribution of life expectancy in 2017. The mass of the distribution is on the right with a long left tail. This is good because there are more countries living longer than countries with lower life expectancy levels.\nWorld GDP per capita has grown exponentially over time, but there are some sharp drops from severe negative shocks. This is still a good thing for humanity because it means fewer people as a percentage of the total population are in poverty compared to years prior. Again, this doesn’t tell us about the distribution of GDP per capita in a cross section.\nThe distribution of GDP per capita is very concentrated in the 0 to 10,000ish range, but has a long right tail out to over 100,000 GDP per capita. This resembles a log-normal shape. I performed a natural logarithmic transformation for modeling purposes.\nModeling the Relationships The Jupyter Notebook I’ve created contains all the regression outputs for every year; however, I’ll just summarize them and provide the plots of the coefficients.\nRegressing Life Expectancy on GDP per capita The residuals of this model seem mildly skewed left from 1990 to 2003, and a strong left skew after that with kurtosis in excess of the normal distribution. We won’t really get into what kurtosis is in this blog post, but you should know that in probability and statistics, kurtosis measures the “tailedness” of a probability distribution.\nThe kurtosis of a normal distribution is 3, so any value greater than that will exhibit skewness either to the left or right. The output of the Breusch-Pagan heteroskedasticity test provides strong evidence of heteroskedasticity in the residuals, but I used heteroskedasticity robust standard errors to be sure the model didn’t underestimate the standard errors of the coefficients.\nSo the residuals are not normally or identically distributed. The Durbin-Watson statistic remains close to 2, so it doesn’t appear to be auto-correlation in our residuals. The R-squared, $r^{2}$, of these regressions are absurdly high, ranging from 0.992 to 0.997; meaning, between 99.2% to 99.7% of the variation in life expectancy is explained by …","date":1568764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568764800,"objectID":"e11651b0c046ffb90746f9c91cc5510d","permalink":"http://localhost:4321/post/2019-09-18-modeling-gdp-per-capita-and-life-expectancy/","publishdate":"2019-09-18T00:00:00Z","relpermalink":"/post/2019-09-18-modeling-gdp-per-capita-and-life-expectancy/","section":"post","summary":"Using quadatric terms for non-linear relationships and repeated OLS regression to study model consistency","tags":["Health care","Regression","Statistics","Python"],"title":"Modeling GDP per Capita and Life Expectancy","type":"post"},{"authors":["Andre Sealy"],"categories":["Finance"],"content":"Most people are very conscious shoppers. Some people are attracted to the brands; others may be attracted to the bargains. Then others are motivated by the social aspects of a firm and how certain businesses strive to make a positive impact on our society. To a certain extent, we all like Corporate Brands to make positive contributions towards environmental, social, and corporate governance (ESG). For the brands that fall short of their “Corporate Responsibility,” there’s always #CancelCulture.\n#CancelCulture is an environment that propagates a social media campaign to either harass or shame a person/institution into changing their behavior, to correct something they did (or change an opinion they hold). The campaign usually goes viral, which ends up ruining a business or someone’s career.\nAt first glance, forcing institutions to change their behavior sounds like mob rule, but #CancelCulture has legitimate uses. Examples involve television/online personalities Charlie Rose and Roseanne Barr has had their careers ended from due to public pressure due to either insensitive comments or inappropriate conduct [1][2]. Billion-dollar corporations like Facebook have had their reputations tarnished due to evidence of seeing the information/betraying the trust of billions of users [3].\nOthers involve the campaign #DeleteUber, for choosing not to participate in the taxi strike during President Donald Trump’s Travel Ban saga that occurs in early 2017 [4]. I also can’t forget about #BurnNike, a viral initiative to burn your Nike athletic gear in response to the company’s decision to provide a multi-million dollar endorsement deal to former San Francisco 49ers quarterback Colin Kaepernick [5]. These examples (in my personal opinion) are illegitimate, nonsensical, and a complete waste of time.\nIt doesn’t stop here. Unfortunately, #CancelCulture becomes progressively dumber; the more people choose to engage in these nonsensical campaigns. These days, people no longer need to be upset about any actions businesses decide to take. Just the mere association of a person, place, or thing is enough to drive people irrational.\n#cancelsoulcycle and #boycottequinox If you were fortunate enough to be away from social media on August 8th, you might have missed that a bunch of angry people has decided to cancel their luxurious gym memberships to Soul Cycle and Equinox. This is all because one of their owners has decided to organize an extravagant fundraiser for Trump’s 2020 reelection campaign. You may have noticed that I have used the phrase one of their owners because people tend to use the words owner and stakeholder synonymously.\nConsidering Equinox Holdings is a private company, it’s unclear what the company is currently valued at, let alone what stake Stephen Ross has in the company. Nevertheless, Equinox was forced to state the obvious on its twitter page: Stephen Ross isn’t part of the management; is he does not have a majority stake in the company. He is merely a passive investor.\nAs a passive investor, Ross is most likely not interested in selling his stake in Equinox, so I hope there are plenty of good alternatives to high price gym memberships. I can’t tell if the boycott is simply about raising money for Trump or about being associated with Trump. Nevertheless, Trump/Ross proponents may come to realize that become more ESG conscious may come with some inconvenient realities.\nIf you’re not careful, you may discover that the company that processes CitiBike accessories also produces peripherals firearms [6]. People may want to start a #boycottlyft campaign on twitter and start using Uber until they realize that Peter Thiel (Donald Trump mega-donor) is still making money from the company [7]. Sure, you can always choose not to ride CitiBike, Uber, or Lyft; that’s a personal choice. However, what do you do when you discover that Koch Industries invests in companies that manufacturer toilet paper [8]?\nThis finally brings us to the sad state of affairs involving hashtag activism, when our political and social consciousness dictates how we invest. I’m no stranger to different types of investment philosophies. After all, having a philosophy is better than having no philosophy at all. However, an investment philosophy that roots itself in platitudes rather than coherent analysis is doomed for failure.\nThis is especially true for ESG portfolios.\nAn Analysis of ESG Funds Socially responsible investing (SRI) looks broadly at a company’s impact on a range of environmental, social, and governance issues, which makes up the bulk of the ESG market. While impact funds often evaluate companies on a single issue, others may only invest in companies in which women make up a meaningful number of the senior management team and the board of directors. Some funds hold companies with direct involvement in renewable energy sources, such as wind and solar, such as AllanzGI.\nThe question is of whether ESG funds have outperformed an appropriate …","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"f324d469d9e52e287cf3c9739104a01f","permalink":"http://localhost:4321/post/2019-08-15-the-sad-reality-of-esg-and-cancel-culture/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/post/2019-08-15-the-sad-reality-of-esg-and-cancel-culture/","section":"post","summary":"Most people are very conscious shoppers. Some people are attracted to the brands; others may be attracted to the bargains. Then others are motivated by the social aspects of a firm and how certain businesses strive to make a positive impact on our society.","tags":["Investing","Portfolio Management","Research","Soul Cycle","Equinox"],"title":"The sad reality of ESG and Hashtag Activism","type":"post"},{"authors":["Andre Sealy"],"categories":null,"content":"","date":1555718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555718400,"objectID":"7ff448022a41332c3482517631295e53","permalink":"http://localhost:4321/publication/mind-the-gap/","publishdate":"2019-07-28T00:00:00Z","relpermalink":"/publication/mind-the-gap/","section":"publication","summary":"Can data science and machine learning help close the performance gap between retail and institutional investors?","tags":["Investing"],"title":"Mind The Gap","type":"publication"},{"authors":["Andre Sealy"],"categories":null,"content":"Pairs Trading Strategies Using Python When it comes to making money in the stock market, there are a myriad of different ways to make money. And it seems that in the finance community, everywhere you go, people are telling you that you should learn Python. After all, Python is a popular programming language which can be used in all types of fields, including data science. There are a large number of packages that can help you meet your goals, and many companies use Python for development of data-centric applications and scientific computation, which is associated with the financial world.\nMost of all Python can help us utilize many different trading strategies that (without it) would by very difficult to analyze by hand or with spreadsheets. One of the trading strategies we will talk about is referred to as Pairs Trading.\nPairs Trading Pairs trading is a form of mean-reversion that has a distinct advantage of always being hedged against market movements. It is generally a high alpha strategy when backed up by some rigorous statistics. The stratey is based on mathematical analysis.\nThe prinicple is as follows. Let’s say you have a pair of securities X and Y that have some underlying economic link. An example might be two companies that manufacture the same product, or two companies in one supply chain. If we can model this economic link with a mathematical model, we can make trades on it.\nIn order to understand pairs trading, we need to understand three mathematical concepts: Stationarity, Integration, and Cointegration.\nNote: This will assume everyone knows the basics of hypothesis testing.\nimport numpy as np import pandas as pd import statsmodels import statsmodels.api as sm from statsmodels.tsa.stattools import coint, adfuller import matplotlib.pyplot as plt import seaborn as sns; sns.set(style=\u0026#34;whitegrid\u0026#34;) Stationarity/Non-Stationarity Stationarity is the most commonly untested assumption in time series analysis. We generally assume that data is stationary when the parameters of the data generating process do not change over time. Else consider two series: A and B. Series A will generate a stationary time series with fixed parameters, while B will change over time.\nWe will create a function that creates a z-score for probability density function. The probability density for a Gaussian distribution is:\n$$ p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}$$\nwhere $\\mu$ is the mean and $\\sigma$ the standard deviation. The square of the standard deviation, $\\sigma^{2}$, is the variance. The empircal rule dictates that 66% of the data should be somewhere between $x+\\sigma$ and $x-\\sigma$, which implies that the function numpy.random.normal is more likely to return samples lying close to the mean, rather than those far away.\ndef generate_data(params): mu = params[0] sigma = params[1] return np.random.normal(mu, sigma) From there, we can create two plots that exhibit a stationary and non-stationary time series. The left time series will be stationary, whereas the right will be non-stationary.\nWhy Stationarity is Important Many statistical tests require that the data being tested are stationary. Using certain statistics on a non-stationary data set may lead to garbage results. As an example, let’s take an average through our non-stationary $B$.\nThe computed mean will show that the mean of all data points, but won’t be useful for any forecasting of the future state. It’s meaningless when compared with any specific time, as it’s a collection of different states at different times mashed together. This is just a simple and clear example of why non-stationarity can distort the analysis, much more subtle problems can arise in practice.\nAugmented Dickey Fuller In order to test for stationarity, we need to test for something called a unit root. Autoregressive unit root test is based on the following hypothesis test:\n$$ \\begin{aligned} H_{0} \u0026amp; : \\phi =\\ 1\\ \\implies y_{t} \\sim I(0) \\ | \\ (unit \\ root) \\ H_{1} \u0026amp; : |\\phi| \u0026lt;\\ 1\\ \\implies y_{t} \\sim I(0) \\ | \\ (stationary) \\ \\end{aligned} $$\nIt’s referred to as a unit root tet because under the null hypothesis, the autoregressive polynominal of $\\mathcal{z}_{t},\\ \\phi (\\mathcal{z})=\\ (1-\\phi \\mathcal{z}) \\ = 0$, has a root equal to unity.\n$y_{t}$ is trend stationary under the null hypothesis. If $y_{t}$is then first differenced, it becomes:\n$$ \\begin{aligned} \\Delta y_{t} \u0026amp; = \\delta\\ + \\Delta\\mathcal{z}{t} \\ \\Delta \\mathcal{z} \u0026amp; = \\phi\\Delta\\mathcal{z}{t-1}\\ +\\ \\varepsilon_{t}\\ -\\ \\varepsilon_{t-1} \\ \\end{aligned} .$$\nThe test statistic is\n$$ t_{\\phi=1}=\\frac{\\hat{\\phi}-1}{SE(\\hat{\\phi})}$$\n$\\hat{\\phi}$ is the least square estimate, and SE($\\hat{\\phi}$) is the usual standard error estimate. The test is a one-sided left tail test. If {$y_{t}$} is stationary, then it can be shown that\n$$\\sqrt{T}(\\hat{\\phi}-\\phi)\\xrightarrow[\\text{}]{\\text{d}}N(0,(1-\\phi^{2}))$$\nor\n$$\\hat{\\phi}\\overset{\\text{A}}{\\sim}N\\bigg(\\phi,\\frac{1}{T}(1-\\phi^{2}) \\bigg)$$\nand it …","date":1551657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551657600,"objectID":"58553125f911246621629ae756535ad2","permalink":"http://localhost:4321/project/pairs-trading-strategies-in-python/","publishdate":"2019-03-04T00:00:00Z","relpermalink":"/project/pairs-trading-strategies-in-python/","section":"project","summary":"Using statistical and quantitiative strategies to gain an edge in the financial markets.","tags":["Machine Learning","Data Science","Finance","Stocks","Trading","Quantitiative Finance"],"title":"Pairs Trading Strategies in Python","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://localhost:4321/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Andre Sealy"],"categories":["Finance","Mathematics"],"content":"The following is an general overview of vanilla options partial sensitivities (option greeks). This series will be a continuation of the project Option Payoffs, Black-Scholes, and the Greeks. We’ve already outlined the necessary terms for understanding Options and the Greeks here. In this post, I will be covering the first greek: Detla ($\\Delta$).\nThe delta of an option, $\\Delta$, is defined as the rate of change of the option price respected to the rate of change of the underlying asset price:\n$$\\Delta = \\frac{\\partial V}{\\partial S}$$\nAs you know, $V$ is the value of the option while $S$ is the underlying asset price.\nWe’ve already covered the the Black-Scholes option pricing model for a call option on a non-dividend stock:\n$$ C(S_{t},t) = N(d_{1})S_{t}-N(d_{2})Ke^{-r(T-t)} $$\nWith:\n$d_{1} = \\frac{1}{\\sigma \\sqrt{T-t}} \\Big[ln\\bigg(\\frac{S_{t}}{k}\\bigg)+\\bigg(r+\\frac{\\sigma^{2}}{2}\\bigg)(T-t) \\Big]$\n$d_{2} = d_{1} - \\sigma \\sqrt{T-t}$\nHowever, we haven’t covered the Black-Scholes model for a put option, which is as follows:\n$$P(S_{t},t)=-SN(-d_{1})+Ee^{-r(T-t)}N(-d_{2})$$\nThe call option Delta will be:\n$$\\Delta_{c}=\\frac{dc}{dS}=e^{-qT}N(d_{1})$$\nProof $$ \\begin{aligned} \\Delta_{c} = \\frac{dc}{dS} \u0026amp; = \\frac{d(e^{-qT}SN(d_{1})-Xe^{-rT}N(d_{2}))}{dS}\\ \u0026amp; = e^{-qT}N(d_{1})+Se^{-qT}\\frac{\\partial N(d_{1})}{\\partial S}-Xe^{-rT}\\frac{\\partial N(d_{2})}{\\partial S} \\ \u0026amp; = e^{-qT}N(d_{1})+Se^{-qT}\\frac{\\partial Nd_{1}}{\\partial d_{1}}-Xe^{-rT}\\frac{\\partial d_{2}}{\\partial S}\\frac{\\partial N(d_{2})}{\\partial d_{2}} \\ \u0026amp; = e^{-qT}N(d_{1})+Se^{-qT}\\frac{\\partial Nd_{1}}{\\partial d_{1}}N^{\\prime}(d_{1})-Xe^{-rT}\\frac{\\partial d_{2}}{\\partial S}N^{\\prime}(d_{2})\\ \u0026amp; = e^{-qT}N(d_{1})+\\underbrace{\\frac{Se^{-qT}N^{\\prime}(d_{1})}{S\\sigma\\sqrt{T}}-\\frac{Xe^{-rT}N^{\\prime}(d_{1})}{S\\sigma\\sqrt{T}}}_{I=0} \\end{aligned} $$\nAbove, $I=0$. Referencing the equation for risk-adjusted lognormal probabilities, $d_{1}$, we have:\n$$ \\begin{aligned} ln(S/X\u0026amp;)+(r-q+\\frac{\\sigma^{2}}{2})T=d_{1}\\sigma\\sqrt{T} \\ \\implies \u0026amp; ln(S)-ln(X)+(r-q)T=d_{1}\\sigma\\sqrt{T}-\\frac{\\sigma^{2}}{2}T=\\frac{1}{2}\\big[d^{2}{1}-(d{1}-\\sigma\\sqrt{T})^{2}]\\big] \\ \\implies \u0026amp; ln(S)+ln(\\frac{1}{\\sqrt{2\\pi}})-\\frac{d_{1}^{2}}{2}=ln(X)-(r-q)T+ln(\\frac{1}{\\sqrt{2\\pi}})-\\frac{d_{2}^{2}}{2} \\ \\implies \u0026amp; S\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{d_{1}^{2}}{2}}=Xe^{-(r-q)T}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{d_{2}^{2}}{2}}\\ \\implies\u0026amp;SN^{\\prime}(d_{1})=Xe^{-(r-q)T}N^{\\prime}(d_{2})\\ \\implies\u0026amp;Se^{-qT}N^{\\prime}(d_{1})=Xe^{-rT}N^{\\prime}(d_{2}) \\end{aligned} $$\nFrom the derivation of $\\Delta=\\frac{dc}{dS}$, we obtain:\n$$\\Delta_{c}=e^{-qT}N(d_{1})$$\nAs we should have what is known as Call/Put Parity, we should have the following equation:\n$$ \\begin{aligned} p+S\u0026amp;e^{-qT}=c+Xe^{-rT}\\ \\implies\u0026amp;\\frac{dp}{dS}=\\frac{dc}{dS}-e^{-qT}\\ \\implies\u0026amp;\\Delta_{p}=N(d_{1})-1 \\end{aligned} $$\nHere is the python script.\nimport numpy as np from math import sqrt, pi, log, e from enum import Enum import scipy.stats as stat from scipy.stats import norm import time class BSMerton: def __init__(self, args): # 1 for a Call, -1 for a put self.Type = int(args[0]) self.S = float(args[1]) # Underlying asset price self.K = float(args[2]) # Option strike K self.r = float(args[3]) # Continuous risk free rate self.q = float(args[4]) # Dividend countinous rate self.T = float(args[5]) / 365.0 # Compute time to expiry self.sigma = float(args[6]) # Compute time to expiry self.sigmaT = self.sigma * self.T ** 0.5 # sigma*T for reusability self.d1 = (log(self.S / self.K) + (self.r - self.q + 0.5 * (self.sigma ** 2)) * self.T) / self.sigmaT self.d2 = self.d1 - self.sigmaT [self.Delta] = self.delta() def delta(self): dfq = e ** (-self.q * self.T) if self.Type == 1: return [dfq * norm.cdf(self.d1)] else: return [dfq * (norm.cdf(self.d1) - 1)] Code that you can use to calculate and chart Delta sensitivities.\nimport numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D import math from matplotlib import cm import OptionsAnalytics from OptionsAnalytics import BSMerton sigma = 0.12 # Flat volatility strike = 105.0 # Fixed strike epsilon = 0.4 # The % on the left/right of Strike. # Asset prices are centered around Spot (\u0026#34;ATM Spot\u0026#34;) shortexpiry = 30 # Shortest expiry in days longexpiry = 720 # Longest expiry in days riskfree = 0.00 # Continous risk free rate divrate = 0.00 # Continous div rate # Grid deinition dx, dy = 40, 40 # Step throughout asset price and expiries axis # xx: Asset price axis, yy: expiry axis, zz: greek axis xx, yy = np.meshgrid(np.linspace(strike*(1-epsilon), (1+epsilon)*strike, dx), np.linspace(shortexpiry, longexpiry, dy)) print(\u0026#34;Calculating greeks...\u0026#34;) zz = np.array([BSMerton([1, x, strike, riskfree, divrate, y, sigma]).Delta for x, y in zip(np.ravel(xx), np.ravel(yy))]) zz = zz.reshape(xx.shape) Plot greek surface print(\u0026#34;Plotting surface...\u0026#34;) fig.subtitle(\u0026#39;Call Delta\u0026#39;, fontsize=20) ax = fig.gca(projection=\u0026#39;3d\u0026#39;) surf = ax.plot_surface(xx, yy, rstride=1, cstride=1, alpha=0.75, …","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541203200,"objectID":"88d158122478293b87384c5bfa0af476","permalink":"http://localhost:4321/post/2018-11-03-option-sensitivities-proofs/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/post/2018-11-03-option-sensitivities-proofs/","section":"post","summary":"The following is an general overview of vanilla options partial sensitivities (option greeks). This series will be a continuation of the project Option Payoffs, Black-Scholes, and the Greeks. We’ve already outlined the necessary terms for understanding Options and the Greeks here.","tags":null,"title":"Option Sensitivities, Formula Proofs, and Python Scripts: Part 1 (Delta)","type":"post"},{"authors":["Andre Sealy"],"categories":null,"content":"401K Optimization Using Modern Portfolio Theory I’ve often talked about the benefits of computer programming and using technology in Finance to develop more sophisticated trading strategies. Such strategies usually require a unique understanding of mathematics, data science, and statistics; paired along with a rudimentary knowledge of machine learning and programming.\nBecause of these significant learning curves, maybe algorithmic trading isn’t the way to go. As such, we’re simply going to purchase 100,000 dollars worth of an asset class and not worry about what investment strategy we’re going to adopt. We’ve got the easy part figured out. The hard part is figuring out how much to purchase of one asset over another. How do you figure this out? Do we need to conduct lots of research? Maybe. Or maybe not. With a little help with technology, the answers may be more simple than we think.\nModern Portfolio Theory (MPT) The Modern Portfolio Theory conceptualizes how investors (who are risk-averse) construct portfolios that maximize their expected returns for given levels of risk. This provides us with the insight of how risk and return characteristics of various investments need not be isolated but analyzed of how individual investments affect the performance of a portfolio. The assumptions of MPT, thus, emphasize that investors only assume the additional risk when there is a possibility of higher expected returns.\nThe pioneer of the Modern Portfolio Theory (MPT) was developed by Financial Economist Harry Markowitz’s, with his paper Portfolio Selection (1952). He eventually won a Nobel Memorial Price in 1990 in Economic Sciences for his contribution to the field. To this day, MPT is taught around the world in practically every finance curriculum. The implications for MPT has laid the groundwork for many of the assumptions investors have when constructing portfolios.\nThe assumptions of MPT are as follows:\nInvestors are rational and avoid risk whenever possible Investors aim for the maxmium returns for their investment All investors share the aim of maximizing their expected returns Commissions and taxes on the market are left out of the consideration All investors have access to the same sources and level of all necessary information about investment decisions Investors have unlimited access to borrow and lend money at the risk-free rate The fundamental aspect of this theory is the possibility for investors to construct an “efficient set of portfolios,” which is also known as an “Efficient Frontier.” These efficient portfolios offer the maximum expected returns for a given level of risk. An investor’s tolerance for risk determines the type of “efficient portfolio” constructed by the investor.\nAn investor which the lowest tolerance would opt for a portfolio that offers the maximum expected return, given the lowest possible risk, and vice versa.\nThe diagram below gives an example of the concept of Efficient Frontier:\nDifferent securities post different expected returns.\nDiversification One of the most essential topics utilized after Markowitz’s Modern Portfolio Theory was the concept of diversification. By merely constructing portfolios with different combinations of securities, investors could achieve a maximum expected return, given their risk preferences. This is due to the fact that the returns of a portfolio are greatly affected by the nature of the relationship between assets and their weights in the portfolio.\nAn investor can reduce portfolio risk simply by holding combinations of instruments which are not perfectly positively correlate (correlation coefficient $-1\\leq\\rho_{ij}\u0026lt;1$). In other words, investors can reduce their exposure to individual asset risk by holding a diversified portfolio of assets.\nDiversification may allow for the same portfolio expected return with reduced risk. These ideas have been started with Markowitz and then reinforced by other economists and mathematicians such as Andrew Brennan who have expressed ideas in the limitation of variance through portfolio theory.\nIf all the asset pairs have correlations of 0 – they are perfectly uncorrelated – the portfolio’s return variance is the sum over all assets of the square of the fraction held in the asset times the asset’s return variance.\nModern Portfolio Theory with Python Most people are familiar with what 401(k)s are. They’re what are known as defined-contribution plans, which the employee and employer can contributions to an investment account. This is not to be confused with pensions, or a defined-benefit plan, which only the employer is responsible for making contributions.\nNot only are employees responsible for contributing to their investment accounts, but they’re also responsible for choosing the specific investments within their 401(k). This selection usually includes an assortment of stock and bond mutual funds.\nIt’s consider the following mutual funds: T. Rowe Price Tax-Efficient Equity Fund (PREFX), Janus Henderson Forty Fund …","date":1540857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540857600,"objectID":"8653027e0c9da0de3f8b282c6b40e055","permalink":"http://localhost:4321/project/401k-optimization-using-modern-portfolio-theory/","publishdate":"2018-10-30T00:00:00Z","relpermalink":"/project/401k-optimization-using-modern-portfolio-theory/","section":"project","summary":"Using financial theory and modern programming to construct the best retirement account.","tags":["Data Science","Finance"],"title":"401K Optimization Using Modern Portfolio Theory","type":"project"},{"authors":["Andre Sealy"],"categories":null,"content":"I wanted to get a better understanding of using the Python programming language to play around with options. We’ll have a look at creating some option payoff functions, implementation of Black-Scholes pricing, and then finish up with some sensitivity analysis (Greeks).\nI’ll provide a reasonably high-level overview of what we’re doing, but I’m by no means an expert in options! However, I found that by implementing them in the Python programming language is good practice in some fundamental skills like list manipulations, maps, plotting, and taking it one step further into object-oriented programming.\nBlack-Scholes Equation Perhaps the most famous and possibly infamous equation in quantitative finance is the Black-Scholes equation. A partial differential equation, which provides the time-evolving price of a vanilla option, specifically European put and call option here (there are all sorts of extensions which extend the usability of this formula).\n$$ \\frac{\\partial V}{\\partial t}+\\frac{1}{2}\\sigma^{2}S^{2}\\frac{\\partial V^{2}}{\\partial S^{2}}+rS\\frac{\\partial V}{\\partial S}-rV = 0$$\nThe equation can be solved to yield a fairly simple closed-form solution for an option price for a non-dividend underlying (and a whole bunch of other assumptions such as efficient markets, no transaction cost, etc.).\n$$ C(S_{t},t) = N(d_{1})S_{t}-N(d_{2})Ke^{-r(T-t)}$$\nThe equation can be solved to yield a fairly simple closed-form solution for an option price for a non-dividened underlying (and a whole bunch of other assumptions such as efficient markets, no transaction cost, etc.).\nWith:\n$d_{1} = \\frac{1}{\\sigma \\sqrt{T-t}} \\Big[ln\\bigg(\\frac{S_{t}}{k}\\bigg)+\\bigg(r+\\frac{\\sigma^{2}}{2}\\bigg)(T-t) \\Big]$\n$d_{2} = d_{1} - \\sigma \\sqrt{T-t}$\nThe above equation can be interpreted as the call option premium, which is the difference between the expected benefit of purchasing the underlying outright at time $t$ against the present value of purchasing the underlying at the strike price expiration time $T$.\nAnother way of interpreting the call option premium/option value is through intrinsic value and time value, where the option value is comprised of:\nThe intrinsic value of exercising the option immediately, i.e., what would the payoff be if we exercised\nThe time value of the option derived from the random behavior of the underlying. As the underlying stock is typically modeled as a stochastic process, there is a probabilistic component, which basically means there is some chance that the stock price could move significantly in our favor over time. Thus we would typically expect to pay higher premiums for options with longer maturities\nThe Partial Differential Equation The original derivation of the Black-Scholes partial differential equation was via stochastic calculus, Ito’s lemma, and a simple hedging argument. Assume that the underlying follows a lognormal random walk:\n$$dS=\\mu S dt + \\sigma S dX $$\nUse $\\Pi$ to denote the value of a portfolio of one long option position and a short position in some quantity $\\Delta$ of the underlying:\n$$\\Pi = V(S,t) - \\Delta S. $$\nThe first term on the right is the option and the second term is the short asset position.\nWe then ask how the value of the portfolio changes from time $t$ to $t + dt$. The change in the portfolio value is due partly to the change in the option value and partlyto the change in the underlying:\n$$d\\Pi = dV - \\Delta dS. $$\nFrom Ito’s lemma, we have\n$$ d \\Pi = \\frac{\\partial V}{\\partial t}dt + \\frac{\\partial V}{\\partial S}dS + \\frac{1}{2}\\sigma^{2}S^{2}\\frac{\\partial^{2} V}{\\partial S^{2}}dt - \\Delta dS. $$\nThe right-hand side of this contains two types of terms, the deterministic and the random. The deterministic terms are those with the $dt$, and the random terms are those with the $dS$.\nPretending for moment that we know $V$ and its derivatives, then we know everything about the right-hand side except for the value of $dS$, because this is random.\nThese random terms can be eliminated by choosing\n$$\\Delta = \\frac{\\partial V}{\\partial S}. $$\nAfter choosing the quantity $\\Delta$, we hold a portfolio whose value changes by the amount\n$$d \\Pi = \\bigg(\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^{2}S^{2}\\frac{\\partial^{2} V}{\\partial S^{2}}\\bigg)dt.$$\nThis change is completely riskless. If we have a completely risk-free change $d\\Pi$ in the portfolio value $\\Pi$, then it must be the same as the growth we would get if we put the equivalent amount of cash in a risk-free interest-bearing account:\n$$d\\Pi = r\\Pi dt. $$\nThis is an example of the no arbitrage prinicple. Putting all of the above together to eliminate $\\Pi$ and $\\Delta$ in favor of partial derivatives of $V$ gives us the Black-Scholes Equation:\n$$ \\frac{\\partial V}{\\partial t}+\\frac{1}{2}\\sigma^{2}S^{2}\\frac{\\partial V^{2}}{\\partial S^{2}}+rS\\frac{\\partial V}{\\partial S}-rV = 0$$\nSolving this simple linear diffusion equation with the final condition and you’ll get the following Black-Scholes call option formula. …","date":1538524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538524800,"objectID":"c7dca033a64bc4b892c444946f33ab43","permalink":"http://localhost:4321/project/option-payoffs-black-scholes-and-the-greeks/","publishdate":"2018-10-03T00:00:00Z","relpermalink":"/project/option-payoffs-black-scholes-and-the-greeks/","section":"project","summary":"An exploration of the Black-Scholes framework and examining payoff functions and option Greeks.","tags":["Programming","Finance","Options","Mathematics"],"title":"Option Payoffs, Black-Scholes, and the Greeks","type":"project"},{"authors":["Andre Sealy"],"categories":["Economics"],"content":"Inflation is probably one of the most difficult phenomena to understand. It’s such a difficult topic to understand, most people have difficulty defining it. At the very least, it can be defined as a broad increase in the average level of prices throughout the entire economy. Some people also define inflation as simply an increase in the money supply. Even more difficult than simply defining the term, most probably aren’t aware of how inflation actually occurs, which can originate through a number of different factors. I’m not going to go through them all, but economists can probably agree that inflation is not an observable phenomenon.\nI’m not going to go through them all, but economists can probably agree that inflation is not an observable phenomenon. Well, the only exception might be if the economy was experiencing what we would refer to as ‘hyperinflation‘. In that case, inflation would be similar to pornography, you’ll know it when you see it.\nHowever, since individuals can’t really observe inflation, the best that we can do is try to measure it. There are many different ways to measure inflation, which means there are many different ways to measure things in real terms. This makes measuring information such as ‘real wages’ more of an art rather than a science.\nYou would probably enter “real wages are falling” into a Google search bar and find many different articles relating to wages falling in real terms. For example, this article from the Washington Post outlines how inflation has eroded the purchasing power of their wages [1].\nWithout adjusting for inflation, these “nonsupervisory” workers saw their average hourly earnings jump 2.8 percent from last year. But that was not enough to keep pace with the 2.9 percent increase in inflation, which economists attributed to rising gas prices.\n“This is very likely because of the spike in oil prices eating into inflation-adjusted earnings,” said Allen Sinai, chief global economist and strategist at Decision Economics. “We pay for energy-related costs out of our wages, out of our compensation. And it’s making a real impact.”\nBased on the way the authors of this article is measuring inflation, it would appear that wages are barely keeping up with inflation.\nThere is just one thing wrong with only looking at headline inflation; the authors of the article already pointed out one of the flaws. Inflation appears to be increasing due to rising energy prices. Considering this, we should look at an inflation metric that removes the effects of rapidly increasing energy prices, such as core-CPI.\nUsing this inflation metric, we can see that real wages aren’t falling. So why does it matter whether or not someone is going core-CPI or headline CPI? Core CPI is merely the same as headline CPI, which excludes the price changes in food and energy. This is because food and energy tend to be very volatile, which may unfairly distort how inflation is measured.\nThe this imply that the writers of the Washington Post article, Jeff Stein and Andrew Van Dam, are lying about inflation? It doesn’t imply that they were intentionally trying to mislead. After all, using headline CPI, it is true that inflation is outpacing wage growth. However, it is possible for someone to use inflation statistics for the purposes of being misleading.\nThe following image shows Annualized Average Hourly Earnings compared with 8 different inflation indicies: Headline and Core CPI, Headline and Core PCE, and the GDP Deflator. I’ve also included 3 alternative measures of the CPI: the Chained, Trimmed-mean and the Median CPI.\nAs you can see, some of these metrics (PCE, GDP Deflator, and the Trimmed-Mean CPI) actually show that real wages are rising, not falling. So which inflation metrics should we be using? That all depends on what people are measuring.\nMedian and Trimmed-Mean CPI As mentioned before, Core CPI (as well as PCE) throws out the most volatile price changes: food and energy. Some people may not prefer to do that, as food and energy are good and people actually purchase on a daily basis. Instead, one could use a more mathematically sophisticated price index like the median and Trimmed-Mean CPI. They achieve the same goal; however, instead of throwing out food and energy, the median and Trimmed-Mean CPI removes the items with the largest price fluctuations within a given month.\nYou could think of it as removing the outliers from a particular dataset. Instead of calculating the weighted-average basket of goods, the median CPI price changes of all goods and services. The Trimmed-Mean CPI works in a similar fashion, by removing 8% of items with the highest and lowest one-month price change, resulting in a 16% Trimmed-Mean Inflation Estimate. When looking at both of these inflation metrics, in terms of the Median CPI, wages currently in-line with inflation. In terms of Trimmed-Mean, wages are outpacing inflation.\nGDP Deflator The GDP Deflator is another lesser-known inflation metric, but still just as …","date":1532304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532304000,"objectID":"e047533fd91caedcfc0fe27cdff0f5b5","permalink":"http://localhost:4321/post/how-to-lie-with-inflation/","publishdate":"2018-07-23T00:00:00Z","relpermalink":"/post/how-to-lie-with-inflation/","section":"post","summary":"Inflation is probably one of the most difficult phenomena to understand. It’s such a difficult topic to understand, most people have difficulty defining it. At the very least, it can be defined as a broad increase in the average level of prices throughout the entire economy.","tags":["Inflation","CPI","PCE","GDP Deflator"],"title":"How To Lie With Inflation Statistics","type":"post"},{"authors":["Andre Sealy"],"categories":null,"content":"With the start of the 2018 FIFA World Cup approaching, soccer fans around the world is trying to figure out who will win the tournament? If you’re a techie and a soccer fan, you’ll probably want to figure out more quantitiative methods of figuring out the answer to that question.\nGoal The goal of this project is established as follows:\nUse Machine Learning to predict who will win the 2018 FIFA World Cup. Predict the outcome of individual matches for the entire competition. Run simulation of the next matches i.e. quarter final, semi finals and finals. These goals present a unique real-world Machine Learning prediction problem and involve solving various Machine Learning task: data integration, feature modeling, and outcome prediction.\nData I used two data sets from Kaggle. You can find them here. We will use results of historical matches since the beginning of the 1930s FIFA Championship for all participating teams.\nWe have opted not to use FIFA national team rankings, due to the limitations of FIFA rankings being created in the 90’s. Considering this limitation, we will stick to the historical match records.\nFirst, I’m going to conduct some exploratory analysis on the two datasets. Next, I will use a feature engine to select the most relevant feature for my prediction. Afterwards, I will attempt to manipulate the data. Finally, I will choose a Machine Learning model and deploy it on the dataset.\nFirst Thing’s First We need to import the necessary libaries and load the datasets into a Dataframe. We will be using the following python libraries.\nPandas: Provides in-memory 2d table objectd referred to as ‘Dataframes.’ Numpy: Allows use to conduct fast mathematical computation on arrays and matrices. Matplotlib: A Python SD plotting library. Seaborn: A data visualization library based on matplotlib. Scikit-Learn: A machine learning library. Ensure the information is imported by calling world_cup.head() and results.head() for both datasets. The Dataframes should look something like the following:\nExploratory Analysis Exploratory analysis and feature engineering is the most time consuming part of any Data Science project.\nAfter analyzing both datasets, the resulting dataset has information on previous matches. The new (resulting) dataset will be useful for analyzing and predicting future mathces.\nWe begin our exporatory anaylsis by added goal differentials and match outcomes to the results Dataframe.\nThe outcome of the new Dataframe has the new results with the goal differential visible.\nNow I’ll work on a subset of the data; one that includes games played by a participating team. I wanted to analyze the US Mens National Team, but unforunately, they were eliminated by Trinidad \u0026amp; Tobago in the qualifying rounds.\nThe Danish Mens National Team (NKVD) was also eliminated by The Swedish Mens National Team. Because of this, I have chosen to analyze the German Mens National Team.\nAnalyzing the subset of the data will help us determine which features are more relevant for our analysis.\nAs you can see, the dataset has matches dating back as early as 1908! However, we’re only interested in looking at matches from the World Cup era, which started in 1930. We then create a column for year and pick all the games played 1930.\nWe can now visualize the most common match outcome for Germany throughout the years.\nFIFA has recorded 863 matches from the German Mens National Team since 1930, of which Germany has won over 500 matches with ~175 losses. Slightly more matches has ended in a draw. Getting the win-rate for every country is a useful metric. We could use it to predict the most likely outcome of each match in the tournament.\nAnalyzing Teams Participating in the World Cup We can start off by creating a list of teams participating in the 2018 World Cup. The following teams participating are as follows:\nGroup A: Uruguay, Russia, Saudi Arabia, Egypt Group B: Spain, Portugal, Iran, Morocco Group C: France, Denmark, Peru, Australia Group D: Croatia, Argentina, Nigeria, Iceland Group E: Brazil, Switzerland, Serbia, Costa Rica Group F: Sweden, Mexico, South Korea, Germany Group G: Belgium, England, Tunisia, Panama Group H: Columbia, Japan, Senegal, Poland The following code will show only a dataframe with team wordl cup results starting from 1930, while dropping the duplicates (dropping games before 1930). We will also drop the date, home_score, away_score, tournament, city, country, goal_difference, and match_year columns from the df_teams dataframe and assign it to a variable named df_teams_1930. This will help us create a prediction label to simplify and process our model.\nFor our prediction labels, we need to create a system to help our algorithm to determine a “winning” outcome versus a “losing” outcome. During group play, FIFA rewards 2 points to the winner of the match; 0 points to the loser of the match; and 1 point to both teams if the match results in a draw. We will adopt the same system to our labels dataset.\nFrom there, …","date":1527984000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527984000,"objectID":"4e08d42d3ce3da699c91e431016af586","permalink":"http://localhost:4321/project/predicting-the-2018-fifa-world-cup-winner-using-machine-learning/","publishdate":"2018-06-03T00:00:00Z","relpermalink":"/project/predicting-the-2018-fifa-world-cup-winner-using-machine-learning/","section":"project","summary":"Using historical World Cup and Fifa data to forecast the winner.","tags":["Machine Learning","Sports","Fifa","Soccer","Statistics"],"title":"Predicting the 2018 World Cup Winner Using Machine Learning","type":"project"},{"authors":["Andre Sealy"],"categories":["Finance"],"content":"Flame on…\nThis, for the most part, is true. The Knight Foundation, along with Gallup, has found that Americans believe that the media plays an important role in our society. However, people don’t believe that role is being fulfilled, hence, trust in the media is at an all-time low. The reasons for the increasing distrust is up for interpretation, but that facts are clear: very few people place their complete trust in the media.\nPoint out this caveat and… you’re somehow aligned with the rhetoric of the President?\nThis, for the most part, is also true. We can find statistics going as far back as 1997 (Gallup has started asking this question since 1972) when Gallup first started to conduct these polls.\nThe mainstream media has eroded their trust among the public; this distrust pre-dates the Trump Presidency. There is a very popular narrative that Press Freedom is under attack. There is not much in the way of evidence to suggest that this is true.\nI can’t speak to whether fossil fuel and gas/diesel car companies are among the world’s largest advertisers. It is well-known that Tesla marketing campaigns do not generate ad revenue for news outlets. It is also true that weekly circulation and Sunday circulation newspapers have seen their revenues decline for the past couple of decades. Sure, if you look at the 10-ks’ for both the New York Times and the Wall Street Journal, you’ll see that both publications have added more than 500,000 and 150,000 digital subscriptions respectively. However, these gains failed to translate into circulation growth for the overall industry.\nAs the traditional media loses its influence and market share to the new media (media outlets and companies that are largely digital and independent), you’ll find traditional media outlets such as the New York Times, Wall Street Journal, Washington Post, et al, will continue to publish online articles with more click-bait headlines.\nHow do we solve this? Elon appears to have a solution\nSome key points on these statements. A journalist is merely a person who writes for newspapers, magazines, or news websites or prepares news to be broadcast via television, radio or podcast. As long as society keeps blurring the lines between “objective journalism” (e.g. Maggie Haberman) and “journalism” for the purposes of providing an opinionative analysis (e.g. Chris Hayes), we will probably need a system that rates the credibility of a journalist.\n“Don’t we have fact-checkers for that?” Well, fact-checkers are an entirely different animal.\nObviously, fact-checking a news article, story, or statements by an individual (pundit, politician or otherwise) is a worthwhile endeavor. However, there doesn’t seem to be much consensus on what a “fact” is anymore. As such, whether unintentional or by default, society as assigned these “fact-checkers” as the sole arbiters of truth in our political discourse. Most journalists in the fact-checking business don’t realize how much of their ideology is baked into their fact-checking work.\nIt’s the only reason why you’ll have two ideologues such as Donald Trump and Bernie Sanders saying almost identical statements and receive different rating accuracies.\nA system that allows users to rate the accuracy of a journalist can work, at least in theory. Viewers who are more partisan will more than likely downvote articles they disagree with, so the aggregate rating of journalist will be about average. However, if journalist considers themselves beholden to “the truth” as opposed to an ideology, then they should work hard to improve their overall rating.\nOf course, that’s just my opinion. I wonder what other people think of this idea…\nThere you go; out of nearly 700,000 of Musk’s fans, 88% of them believe there are obvious problems with the media and a third-party platform should be created to deal with it. In fairness, this obviously isn’t the “every twitter users in America” poll; however, Elon Musk has a sizable audience who are more politicial engaged.\nBut to no avail, it didn’t help the end results of the poll…\nThere is some supporting evidence…\nHe also has some support from one of the most well-known celebrities.\nMost people agree that the media should be held accountable mistruths and falsehoods they publish. Most people are unable to distinguish between an attack on the person and an attack on the work the person is reporting. Although, when someone is purposely being misleading, it is merely a distinction without a difference.\nElon Musk — the person who resigned from Trump’s Science Advisory Board for being too anti-science — is attacking science. Is there anyone out there who seriously believes this? Of course not; it’s a ridiculous assertion and Musk was correct to call her out on it. I’m guessing that Erin Biba didn’t count on Elon Musk replying directly to her tweet. Did this episode convince her to re-evaluate her preconceived notions? Was she persuaded enough to look at things from a different point of view?\nNope. It’s …","date":1527033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527033600,"objectID":"2bc0acbe3d6a20d5569d56f86cda4048","permalink":"http://localhost:4321/post/2018-05-23-elon-musk-has-had-enough/","publishdate":"2018-05-23T00:00:00Z","relpermalink":"/post/2018-05-23-elon-musk-has-had-enough/","section":"post","summary":"Elon Musk's battle against the media sheds new light on the growing distrust on the defenders of democracy.","tags":null,"title":"Elon Musk Has Had Enough Of Your Shit...","type":"post"},{"authors":["Andre Sealy"],"categories":["Finance"],"content":"Tesla’s 5.3% bonds due in 2025 would be five or six percentage points wider if they traded along regression of B3 and Caa1 rated automotive and industrial bonds, which is worse than the base-case scenario of two or three points wider using a 33% chance the bonds get downgraded by S\u0026amp;P. Technicals have been supported by active buying from T Rowe Price in 1Q, which combined with Franklin Resources owns about 28% of the total issue.\nBlueLine rental’s 9.25% bonds due in 2024 trade about 70 bps wider than the Tesla notes, and its risk profile likely will benefit further from strong demand, and the potential exit from Platinum Equity. An IPO, based off its recent S-1 filing, or sale to a strategic buyer, could drive favorable event risk.\n","date":1500768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500768000,"objectID":"f9cfffdcc8a315f13f5d1d843f63e2a5","permalink":"http://localhost:4321/post/2018-07-05-chart-of-the-day/","publishdate":"2017-07-23T00:00:00Z","relpermalink":"/post/2018-07-05-chart-of-the-day/","section":"post","summary":"Tesla’s 5.3% bonds due in 2025 would be five or six percentage points wider if they traded along regression of B3 and Caa1 rated automotive and industrial bonds, which is worse than the base-case scenario of two or three points wider using a 33% chance the bonds get downgraded by S\u0026P.","tags":null,"title":"Chart of the Day: Valuing Telsa Through Credit","type":"post"},{"authors":["Andre Sealy"],"categories":["Finance","Economics"],"content":"While demand for US Treasuries remains brisk at primary auctions, the same can hardly be said for the short-end of the market, where moments ago we saw what happens to auction demand in a time of rapidly rising rates.\nAs shown in the chart above, while the yield on the 3 month Bills auctioned off today came in largely as expected at 1.94%, the demand did not, and after an already depressed Bid-to-Cover ratio of 2.89 last week, today’s 3-month auction suffered from one of the lowest demands on record, tumbling to just 2.62. At $125.88 billion in bids tendered for $48 billion in paper, down sharply from $138.87 billion on June 25. In context, the last time the markets have witnessed a Bid-To-Cover ratio this low was post-Lehman crisis.\nWith both T-Bill issuance continuing to surge, and ratings rising, two things are certain: not only will the Libor-OIS spread resume blowing out amid the continued surge in short-term supply and increasingly tighter financial conditions, but demand will continue to slide. However, the good news is that we are still well off from the record lows, in which auctions were only 2.0x covered at the start of the century. That said, who knows: perhaps the break in the bond market will begin with a failed Bill action as the US Treasury finds it increasingly difficult to roll over short-term debt.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"66e2f4541e641044351a78c2da717d05","permalink":"http://localhost:4321/post/2018-07-28-chart-of-the-day/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/2018-07-28-chart-of-the-day/","section":"post","summary":"While demand for US Treasuries remains brisk at primary auctions, the same can hardly be said for the short-end of the market, where moments ago we saw what happens to auction demand in a time of rapidly rising rates.","tags":null,"title":"Chart of the Day: 3-Month Bill Bid-To-Cover Ratio Plunges","type":"post"},{"authors":null,"categories":null,"content":"\u0026lt;!DOCTYPE html\u0026gt;\rpolice_v_education\rIn [1]:\r# import the necessary modules\rimport quandl\rimport pandas as pd\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rimport matplotlib.ticker as mtick\rquandl.ApiConfig.api_key = \u0026#39;USE YOUR OWN QUANDL API\u0026#39;\rsns.set_style(\u0026#39;darkgrid\u0026#39;)\rIn [2]:\r# Extracting the data from FRED, starting from 1959; changing the column name\rdf = quandl.get(\u0026#39;FRED/G160851A027NBEA\u0026#39;, start_date=\u0026#39;1959-01-01\u0026#39;)\rdf.rename(columns={\u0026#39;Value\u0026#39;: \u0026#39;Police\u0026#39;}, inplace=True)\r# formatting the tick marks of the y-axis\rfmt = \u0026#39;${x:,.0f}\u0026#39;\rtick = mtick.StrMethodFormatter(fmt)\r# plotting the police expenditures\rax = df.plot(color=\u0026#39;blue\u0026#39;, xlim=(\u0026#39;1959-01-01\u0026#39;, \u0026#39;2018-01-01\u0026#39;), figsize=(12, 6))\rax.set_xlabel(\u0026#39;Source: U.S. Buearu of Economic Analysis\u0026#39;,\rstyle=\u0026#39;italic\u0026#39;, fontsize=11, x=.85)\rax.set_ylabel(\u0026#39;In Billions\u0026#39;, fontsize=11)\rax.set_title(\r\u0026#39;Government Current Expenditures (State \u0026amp; Local): Police\u0026#39;, fontsize=18, x=.3, y=1.02)\rax.legend(\u0026#39;\u0026#39;)\rax.tick_params(axis=\u0026#39;x\u0026#39;, labelsize=12)\rax.tick_params(axis=\u0026#39;y\u0026#39;, labelsize=12)\rax.yaxis.set_major_formatter(tick)\rplt.savefig(\u0026#39;police_expenditures.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;)\r\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt;\r\u0026lt;!DOCTYPE svg PUBLIC \u0026#34;-//W3C//DTD SVG 1.1//EN\u0026#34;\r\u0026#34;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\u0026#34;\u0026gt;\rIn [3]:\r# extracting the data from teachers expenditures, starting from 1959.\rdf2 = quandl.get(\u0026#39;FRED/G161071A027NBEA\u0026#39;, start_date=\u0026#39;1959-01-01\u0026#39;)\r# plotting the expenditures from teachers\rax2 = df2.plot(color=\u0026#39;blue\u0026#39;, xlim=(\r\u0026#39;1959-01-01\u0026#39;, \u0026#39;2018-01-01\u0026#39;), figsize=(12, 6))\rax2.set_xlabel(\u0026#39;Source: U.S. Buearu of Economic Analysis\u0026#39;,\rstyle=\u0026#39;italic\u0026#39;, fontsize=11, x=.85)\rax2.set_ylabel(\u0026#39;In Billions\u0026#39;, fontsize=11)\rax2.set_title(\r\u0026#39;Government Current Expenditures (State \u0026amp; Local): Education\u0026#39;, fontsize=18, x=.32, y=1.02)\rax2.legend(\u0026#39;\u0026#39;)\rax2.tick_params(axis=\u0026#39;x\u0026#39;, labelsize=12)\rax2.tick_params(axis=\u0026#39;y\u0026#39;, labelsize=12)\rax2.yaxis.set_major_formatter(tick)\rplt.savefig(\u0026#39;teachers_expenditures.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;)\r\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt;\r\u0026lt;!DOCTYPE svg PUBLIC \u0026#34;-//W3C//DTD SVG 1.1//EN\u0026#34;\r\u0026#34;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\u0026#34;\u0026gt;\rIn [4]:\r# merging the two plots\rax3 = df2.plot(color=\u0026#39;blue\u0026#39;, xlim=(\r\u0026#39;1959-01-01\u0026#39;, \u0026#39;2018-01-01\u0026#39;), figsize=(12, 6))\rdf.plot(color=\u0026#39;red\u0026#39;, label=\u0026#39;Police\u0026#39;, ax=ax3)\rax3.set_xlabel(\u0026#39;Source: U.S. Buearu of Economic Analysis\u0026#39;,\rstyle=\u0026#39;italic\u0026#39;, fontsize=11, x=.85)\rax3.set_ylabel(\u0026#39;In Billions\u0026#39;, fontsize=11)\rax3.set_title(\u0026#39;Education vs. Police Expenditures (State \u0026amp; Local)\u0026#39;,\rfontsize=18, x=.25, y=1.02)\rax3.tick_params(axis=\u0026#39;x\u0026#39;, labelsize=12)\rax3.tick_params(axis=\u0026#39;y\u0026#39;, labelsize=12)\rax3.legend([\u0026#39;Education\u0026#39;, \u0026#39;Police\u0026#39;])\rax3.yaxis.set_major_formatter(tick)\rplt.savefig(\u0026#39;teachers_police.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;)\r\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt;\r\u0026lt;!DOCTYPE svg PUBLIC \u0026#34;-//W3C//DTD SVG 1.1//EN\u0026#34;\r\u0026#34;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\u0026#34;\u0026gt;\rIn [5]:\r# introducting total expenditures from state and local\rdf3 = quandl.get(\r\u0026#39;FRED/SLEXPND\u0026#39;, start_date=\u0026#39;1959-01-01\u0026#39;, end_date=\u0026#39;2018-01-01\u0026#39;)\rax4 = df3.plot(color=\u0026#39;black\u0026#39;, xlim=(\r\u0026#39;1959-01-01\u0026#39;, \u0026#39;2018-01-01\u0026#39;), label=\u0026#39;Other Spending\u0026#39;, figsize=(12, 6))\rdf2.plot(color=\u0026#39;blue\u0026#39;, label=\u0026#39;Education\u0026#39;, ax=ax4)\rdf.plot(color=\u0026#39;red\u0026#39;, label=\u0026#39;Police\u0026#39;, ax=ax4)\rax4.legend([\u0026#39;Other Spending\u0026#39;, \u0026#39;Education\u0026#39;, \u0026#39;Police\u0026#39;])\rax4.set_title(\r\u0026#39;Social Spending vs. Police Expenditures (State and Local)\u0026#39;, fontsize=18, x=.30, y=1.01)\rax4.set_xlabel(\u0026#39;Source: U.S. Buearu of Economic Analysis\u0026#39;,\rstyle=\u0026#39;italic\u0026#39;, fontsize=11, x=.85)\rax4.set_ylabel(\u0026#39;In Billions\u0026#39;, fontsize=11)\rax4.tick_params(axis=\u0026#39;x\u0026#39;, labelsize=12)\rax4.tick_params(axis=\u0026#39;y\u0026#39;, labelsize=12)\rax4.yaxis.set_major_formatter(tick)\rplt.savefig(\u0026#39;social_spending_police.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;)\r\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt;\r\u0026lt;!DOCTYPE svg PUBLIC \u0026#34;-//W3C//DTD SVG 1.1//EN\u0026#34;\r\u0026#34;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\u0026#34;\u0026gt;\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"825a32db40310e50947980f5a51259b2","permalink":"http://localhost:4321/post/images/notebooks/police_v_education/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/images/notebooks/police_v_education/","section":"post","summary":"\u003c!DOCTYPE html\u003e\rpolice_v_education\rIn [1]:\r# import the necessary modules\rimport quandl\rimport pandas as pd\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rimport matplotlib.ticker as mtick\rquandl.ApiConfig.api_key = 'USE YOUR OWN QUANDL API'\rsns.","tags":null,"title":"","type":"post"}]